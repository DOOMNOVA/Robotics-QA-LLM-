{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the content -- helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to extract text and store metadata about the book and page number\n",
    "#Note : the page number is number visible in the pdf viewing app not the embedded page in the \n",
    "def extract_text_from_pdf(pdf_path,book_name):\n",
    "    text_with_metadata=[]\n",
    "    with open(pdf_path,\"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            page_text=page.extract_text()\n",
    "            text_with_metadata.append({'book_name':book_name,\n",
    "                                           'page_number':page_num +1,\n",
    "                                           'text':page_text if page_text else \"\"})\n",
    "    return text_with_metadata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## textbook extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path_1 = \"Textbooks/Introduction to Autonomous Mobile Robots book.pdf\"\n",
    "text_path_2 = \"Textbooks/Introduction-to-Robotics-3rd-edition.pdf\"\n",
    "text_path_3 = \"Textbooks/mataric-primer.pdf\"\n",
    "textbook_1 = extract_text_from_pdf(text_path_1,\"Introduction to Autonomous Mobile Robots book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336\n"
     ]
    }
   ],
   "source": [
    "print(len(textbook_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n"
     ]
    }
   ],
   "source": [
    "textbook_2 = extract_text_from_pdf(text_path_2,\"Introduction-to-Robotics-3rd-edition\")\n",
    "print(len(textbook_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323\n"
     ]
    }
   ],
   "source": [
    "textbook_3 = extract_text_from_pdf(text_path_3,\"mataric-primer\")\n",
    "print(len(textbook_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/haridevaraj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#helper function to clean text\n",
    "def clean_testbook(text):\n",
    "    \n",
    "    #normalize white space\n",
    "    text = re.sub(r'\\s+',' ',text).strip()\n",
    "    \n",
    "    #replace multiple spaces with single space\n",
    "    text = re.sub(r' {2,}',' ',text)\n",
    "    \n",
    "    #remove unwanted characters, keeping the punctuation\n",
    "    #text = re.sub(r'[^\\w\\s\\.,!?;]','',text)\n",
    "    return text\n",
    "\n",
    "#function to chunck text with metadata\n",
    "def textbook_chunking(textbook,chunk_size=100):\n",
    "    chunks_with_metadata = []\n",
    "    for entry in textbook:\n",
    "        book_name = entry['book_name']\n",
    "        page_number = entry['page_number']\n",
    "        text = entry['text']\n",
    "        \n",
    "        #clean the text\n",
    "        text = clean_testbook(text)\n",
    "        \n",
    "        #tokenize text into sentences\n",
    "        sentences = sent_tokenize(text)\n",
    "        \n",
    "        #chunk each sentence into short, contiguous texts of approx 100 tokens\n",
    "        current_chunk = \"\"\n",
    "        current_chunk_tokens = 0\n",
    "        for sentence in sentences:\n",
    "            tokenized_sentence = word_tokenize(sentence)\n",
    "            if current_chunk_tokens + len(tokenized_sentence) > chunk_size:\n",
    "                # if adding this sentence would exceed the chunk_size, start a new chunk\n",
    "                if current_chunk:\n",
    "                    chunks_with_metadata.append({\n",
    "                        'book_name':book_name,\n",
    "                        'page_number': page_number,\n",
    "                        'text_chunk' : current_chunk.strip()\n",
    "                    })\n",
    "                current_chunk = sentence + \" \"\n",
    "                current_chunk_tokens = len(tokenized_sentence)\n",
    "            else:\n",
    "                current_chunk += sentence + \" \"\n",
    "                current_chunk_tokens += len(tokenized_sentence)\n",
    "            \n",
    "        #if any last chunk\n",
    "        if current_chunk:\n",
    "            chunks_with_metadata.append({\n",
    "                'book_name' : book_name,\n",
    "                'page_number' : page_number,\n",
    "                'text_chunk' : current_chunk.strip()\n",
    "            })\n",
    "            \n",
    "    return chunks_with_metadata\n",
    "                \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "textbook_chunk_1 = textbook_chunking(textbook_1)\n",
    "textbook_chunk_2 = textbook_chunking(textbook_2)\n",
    "textbook_chunk_3 = textbook_chunking(textbook_3)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined \n",
    "combined_textbook_chunk = textbook_chunk_1 + textbook_chunk_2 + textbook_chunk_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4497"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_textbook_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to embed text chunks with metadata\n",
    "def embed_text_chunks(textbook_chunk_metadata,model):\n",
    "    for chunk in textbook_chunk_metadata:\n",
    "        text_chunk = chunk['text_chunk']\n",
    "        embedding = model.encode(text_chunk)\n",
    "        chunk['embedding'] = embedding\n",
    "    return textbook_chunk_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the textbook chunks as pickle files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle file helper functions\n",
    "def save_chunk_to_pickle(embedding_with_metadata,file_path):\n",
    "    with open(file_path,'wb') as f:\n",
    "        pickle.dump(embedding_with_metadata,f)\n",
    "\n",
    "def load_chunk_to_pickle(file_path):\n",
    "    with open(file_path,'rb') as f:\n",
    "        embedding_with_metadata = pickle.load(f)\n",
    "    return embedding_with_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 1,\n",
       "  'text_chunk': 'Autonomous Mobile RobotsIntroduction toRoland Illah R.SIEGWART NOURBAKHSH Autonomous Mobile Robots SIEGWART and NOURBAKHSHIntroduction to Introduction to Autonomous Mobile Robots Roland Siegwart and Illah R. Nourbakhsh Mobile robots range from the teleoperated Sojourner on the Mars Pathfinder mission to cleaning robots in the Paris Metro. Introduction to Autonomous Mobile Robots offers students and other interested readers an overview of the technology of mobility—the mechanisms that allow a mobile robot to movethrough a real world environment to perform its tasks—including locomotion,sensing, localization, and motion planning.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 1,\n",
       "  'text_chunk': 'It discusses all facets of mobile robotics,including hardware design, wheel design, kinematics analysis, sensors and per-ception, localization, mapping, and robot control architectures. The design of any successful robot involves the integration of many different disciplines, among them kinematics, signal analysis, information theory, artificialintelligence, and probability theory. Reflecting this, the book presents the tech-niques and technology that enable mobility in a series of interacting modules.Each chapter covers a different aspect of mobility, as the book moves from low-level to high-level details.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 1,\n",
       "  'text_chunk': 'The first two chapters explore low-level locomotoryability, examining robots’ wheels and legs and the principles of kinematics. This isfollowed by an in-depth view of perception, including descriptions of many “off-the-shelf” sensors and an analysis of the interpretation of sensed data. The finaltwo chapters consider the higher-level challenges of localization and cognition,discussing successful localization strategies, autonomous mapping, and navigationcompetence.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 1,\n",
       "  'text_chunk': 'Bringing together all aspects of mobile robotics into one volume,Introduction to Autonomous Mobile Robots can serve as a textbook for course- work or a working tool for beginners in the field. Roland Siegwart is Professor and Head of the Autonomous Systems Lab at the Swiss Federal Institute of Technology, Lausanne. Illah R. Nourbakhsh is AssociateProfessor of Robotics in the Robotics Institute, School of Computer Science, atCarnegie Mellon University. “This book is easy to read and well organized.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 1,\n",
       "  'text_chunk': 'The idea of providing a robot functional architecture as an outline of the book, and then explaining each component in a chapter, is excellent. I think the authors have achieved theirgoals, and that both the beginner and the advanced student will have a clearidea of how a robot can be endowed with mobility.” —Raja Chatila, LAAS-CNRS, France Intelligent Robotics and Autonomous Agents seriesA Bradford Book The MIT Press Massachusetts Institute of TechnologyCambridge, Massachusetts 02142http://mitpress.mit.edu ,!7IA2G2-bjfach!'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 1,\n",
       "  'text_chunk': ':t;K;k;K;k0-262-19502-X45695Siegwart 6/10/04 3:17 PM Page 1'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 2,\n",
       "  'text_chunk': 'Introduction to Autonomous Mobile Robots'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 3,\n",
       "  'text_chunk': 'Intelligent Robotics and Autonomous Agents Ronald C. Arkin, editor Robot Shaping: An Experiment in Behavior Engineering , Marco Dorigo and Marco Colombetti, 1997Behavior-Based Robotics , Ronald C. Arkin, 1998Layered Learning in Multiagent Systems: A Winning Approach to Robotic Soccer , Peter Stone, 2000Evolutionary Robotics: The Biology, Intelligence, and Technology of Self-Organizing Machines , Stefano Nolfi and Dario Floreano, 2000 Reasoning about Rational Agents , Michael Wooldridge, 2000Introduction to AI Robotics , Robin R. Murphy, 2000Strategic Negotiation in Multiagent Environments , Sarit Kraus, 2001Mechanics of Robotic Manipulation , Matthew T. Mason, 2001Designing Sociable Robots , Cynthia L. Breazeal, 2002Introduction to Autonomous Mobile Robots , Roland Siegwart and Illah R. Nourbakhsh, 2004'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 4,\n",
       "  'text_chunk': 'Introduction to Autonomous Mobile Robots Roland Siegwart and Illah R. Nourbakhsh A Bradford Book The MIT Press Cambridge, Massachusetts London, England'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 5,\n",
       "  'text_chunk': '© 2004 Massachusetts Institute of Technology All rights reserved. No part of this book may be reproduced in any form by any electronic or mechan- ical means (including photocopying, recording, or information storage and retrieval) without permis- sion in writing from the publisher. This book was set in Times Roman by the authors using Adobe FrameMaker 7.0. Printed and bound in the United States of America. Library of Congress Cataloging-in-Publication Data Siegwart, Roland. Introduction to autonomous mobile robots / Roland Siegwart and Illah Nourbakhsh. p. cm.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 5,\n",
       "  'text_chunk': '— (Intelligent robotics and autonomous agents) “A Bradford book. ” Includes bibliographical references and index. ISBN 0-262-19502-X (hc : alk. paper) 1. Mobile robots. 2. Autonomous robots. I. Nourbakhsh, Illah Reza, 1970– . II. Title. III. Series. TJ211.415.S54 2004 629.8´92—dc22 2003059349'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 6,\n",
       "  'text_chunk': 'To Luzia and my children Janina, Malin and Yanik who give me their support and freedom to grow every day — RSTo my parents Susi and Yvo who opened my eyes — RS To Marti who is my love and my inspiration — IRN To my parents Fatemeh and Mahmoud who let me disassemble and investigate everythingin our home — IRN'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 7,\n",
       "  'text_chunk': 'Slides and exercises that go with this book are available on: http://www.mobilerobots.org'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 8,\n",
       "  'text_chunk': 'Contents Acknowledgments xi Preface xiii 1 Introduction 1 1.1 Introduction 1 1.2 An Overview of the Book 10 2 Locomotion 13 2.1 Introduction 13 2.1.1 Key issues for locomotion 16 2.2 Legged Mobile Robots 17 2.2.1 Leg configurations and stability 182.2.2 Examples of legged robot locomotion 21 2.3 Wheeled Mobile Robots 30 2.3.1 Wheeled locomotion: the design space 312.3.2 Wheeled locomotion: case studies 38 3 Mobile Robot Kinematics 47 3.1 Introduction 47 3.2 Kinematic Models and Constraints 48 3.2.1 Representing robot position 48 3.2.2 Forward kinematic models 51 3.2.3 Wheel kinematic constraints 53 3.2.4 Robot kinematic constraints 613.2.5 Examples: robot kinematic models and constraints 63 3.3 Mobile Robot Maneuverability 67 3.3.1 Degree of mobility 67 3.3.2 Degree of steerability 71 3.3.3 Robot maneuverability 72'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 9,\n",
       "  'text_chunk': 'viii Contents 3.4 Mobile Robot Workspace 74 3.4.1 Degrees of freedom 74 3.4.2 Holonomic robots 75 3.4.3 Path and trajectory considerations 77 3.5 Beyond Basic Kinematics 80 3.6 Motion Control (Kinematic Control) 81 3.6.1 Open loop control (trajectory-following) 81 3.6.2 Feedback control 82 4 Perception 89 4.1 Sensors for Mobile Robots 89 4.1.1 Sensor classification 894.1.2 Characterizing sensor performance 924.1.3 Wheel/motor sensors 97 4.1.4 Heading sensors 98 4.1.5 Ground-based beacons 1014.1.6 Active ranging 104 4.1.7 Motion/speed sensors 115 4.1.8 Vision-based sensors 117 4.2 Representing Uncertainty 145 4.2.1 Statistical representation 1454.2.2 Error propagation: combining uncertain measurements 149 4.3 Feature Extraction 151 4.3.1 Feature extraction based on range data (laser, ultrasonic, vision-based ranging) 154 4.3.2 Visual appearance based feature extraction 163 5 Mobile Robot Localization 181 5.1 Introduction 181 5.2 The Challenge of Localization: Noise and Aliasing 182 5.2.1 Sensor noise 183 5.2.2 Sensor aliasing 184 5.2.3 Effector noise 185 5.2.4 An error model for odometric position estimation 186 5.3 To Localize or Not to Localize: Localization-Based Navigation versus Programmed Solutions 191 5.4 Belief Representation 194 5.4.1 Single-hypothesis belief 194 5.4.2 Multiple-hypothesis belief 196'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 10,\n",
       "  'text_chunk': 'Contents ix 5.5 Map Representation 200 5.5.1 Continuous representations 200 5.5.2 Decomposition strategies 203 5.5.3 State of the art: current challenges in map representation 210 5.6 Probabilistic Map-Based Localization 212 5.6.1 Introduction 2125.6.2 Markov localization 2145.6.3 Kalman filter localization 227 5.7 Other Examples of Localization Systems 244 5.7.1 Landmark-based navigation 2455.7.2 Globally unique localization 246 5.7.3 Positioning beacon systems 248 5.7.4 Route-based localization 249 5.8 Autonomous Map Building 250 5.8.1 The stochastic map technique 2505.8.2 Other mapping techniques 253 6 Planning and Navigation 257 6.1 Introduction 2576.2 Competences for Navigation: Planning and Reacting 258 6.2.1 Path planning 2596.2.2 Obstacle avoidance 272 6.3 Navigation Architectures 291 6.3.1 Modularity for code reuse and sharing 2916.3.2 Control localization 2916.3.3 Techniques for decomposition 292 6.3.4 Case studies: tiered robot architectures 298 Bibliography 305 Books 305 Papers 306Referenced Webpages 314Interesting Internet Links to Mobile Robots 314 Index 317'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 12,\n",
       "  'text_chunk': 'Acknowledgments This book is the result of inspirations and contributions from many researchers and students at the Swiss Federal Institute of Technology Lausanne (EPFL), Carnegie Mellon Univer- sity’s Robotics Institute, Pittsburgh (CMU), and many others around the globe. We would like to thank all the researchers in mobile robotics that make this field so rich and stimulating by sharing their goals and visions with the community. It is their work that enables us to collect the material for this book.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 12,\n",
       "  'text_chunk': 'The most valuable and direct support and contribution for this book came from our past and current collaborators at EPFL and CMU.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 12,\n",
       "  'text_chunk': 'We would like to thank: Kai Arras for his con-tribution to uncertainty representation, feature extraction and Kalman filter localization; Matt Mason for his input on kinematics; Nicola Tomatis and Remy Blank for their supportand assistance for the section on vision-based sensing; Al Rizzi for his guidance on feed- back control; Roland Philippsen and Jan Persson for their contribution to obstacle avoid- ance; Gilles Caprari and Yves Piguet for their input and suggestions on motion control;Agostino Martinelli for his careful checking of some of the equations and Marco Lauria for offering his talent for some of the figures.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 12,\n",
       "  'text_chunk': 'Thanks also to Marti Louw for her efforts on the cover design. This book was also inspired by other courses, especially by the lecture notes on mobile robotics at the Swiss Federal Institute of Technology, Zurich (ETHZ). Sincere thank goesto Gerhard Schweitzer, Martin Adams and Sjur Vestli. At the Robotics Institute specialthanks go to Emily Hamner and Jean Harpley for collecting and organizing photo publica- tion permissions. The material for this book has been used for lectures at EFPL and CMU since 1997.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 12,\n",
       "  'text_chunk': 'Thanks go to all the many hundreds of students that followed the lecture andcontributed thought their corrections and comments. It has been a pleasure to work with MIT Press, publisher of this book. Thanks to Ronald C. Arkin and the editorial board of the Intelligent Robotics and Autonomous Agents seriesfor their careful and valuable review and to Robert Prior, Katherine Almeida, Sharon Deacon Warne, and Valerie Geary from MIT Press for their help in editing and finalizing the book.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 12,\n",
       "  'text_chunk': 'Special thanks also to Marie-Jo Pellaud at EPFL for carefully correcting the text files and to our colleagues at the Swiss Federal Institute of Technology Lausanne and CarnegieMellon University.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 14,\n",
       "  'text_chunk': 'Preface Mobile robotics is a young field. Its roots include many engineering and science disci- plines, from mechanical, electrical and electronics engineering to computer, cognitive and social sciences. Each of these parent fields has its share of introductory textbooks that excite and inform prospective students, preparing them for future advanced courseworkand research. Our objective in writing this textbook is to provide mobile robotics with such a preparatory guide.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 14,\n",
       "  'text_chunk': 'This book presents an introduction to the fundamentals of mobile robotics, spanning the mechanical, motor, sensory, perceptual and cognitive layers that comprise our field ofstudy. A collection of workshop proceedings and journal publications could present the new student with a snapshot of the state of the art in all aspects of mobile robotics. But herewe aim to present a foundation — a formal introduction to the field.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 14,\n",
       "  'text_chunk': \"The formalism and analysis herein will prove useful even as the frontier of the state of the art advances due to the rapid progress in all of mobile robotics' sub-disciplines. We hope that this book will empower both the undergraduate and graduate robotics stu- dent with the background knowledge and analytical tools they will need to evaluate andeven critique mobile robot proposals and artifacts throughout their career. This textbook issuitable as a whole for introductory mobile robotics coursework at both the undergraduate and graduate level.\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 14,\n",
       "  'text_chunk': 'Individual chapters such as those on Perception or Kinematics can be useful as overviews in more focused courses on specific sub-fields of robotics. The origins of the this book bridge the Atlantic Ocean. The authors have taught courses on Mobile Robotics at the undergraduate and graduate level at Stanford University, ETHZurich, Carnegie Mellon University and EPFL (Lausanne). Their combined set of curricu-lum details and lecture notes formed the earliest versions of this text.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 14,\n",
       "  'text_chunk': 'We have combined our individual notes, provided overall structure and then test-taught using this textbook for two additional years before settling on the current, published text. For an overview of the organization of the book and summaries of individual chapters, refer to Section 1.2. Finally, for the teacher and the student: we hope that this textbook proves to be a fruitful launching point for many careers in mobile robotics. That would be the ultimate reward.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 16,\n",
       "  'text_chunk': '1 Introduction 1.1 Introduction Robotics has achieved its greatest success to date in the world of industrial manufacturing. Robot arms, or manipulators , comprise a 2 billion dollar industry. Bolted at its shoulder to a specific position in the assembly line, the robot arm can move with great speed and accu- racy to perform repetitive tasks such as spot welding and painting (figure 1.1). In the elec- tronics industry, manipulators place surface-mounted components with superhumanprecision, making the portable telephone and laptop computer possible.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 16,\n",
       "  'text_chunk': 'Yet, for all of their successes, these commercial robots suffer from a fundamental dis- advantage: lack of mobility. A fixed manipulator has a limited range of motion that depends Figure 1.1 Picture of auto assembly plant-spot welding robot of KUKA and a parallel robot Delta of SIG Demau- rex SA (invented at EPFL [140]) during packaging of chocolates. © KUKA Inc. © SIG Demaurex SA'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 17,\n",
       "  'text_chunk': '2 Chapter 1 on where it is bolted down. In contrast, a mobile robot would be able to travel throughout the manufacturing plant, flexibly applying its talents wherever it is most effective. This book focuses on the technology of mobility: how can a mobile robot move unsu- pervised through real-world environments to fulfill its tasks? The first challenge is locomo-tion itself. How should a mobile robot move, and what is it about a particular locomotion mechanism that makes it superior to alternative locomotion mechanisms?'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 17,\n",
       "  'text_chunk': 'Hostile environments such as Mars trigger even more unusual locomotion mechanisms (figure 1.2). In dangerous and inhospitable environments, even on Earth, such teleoperated systems have gained popularity (figures 1.3, 1.4, 1.5, 1.6). In these cases, the low-level complexities of the robot often make it impossible for a human operator to directly controlits motions. The human performs localization and cognition activities, but relies on the robot’s control scheme to provide motion control.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 17,\n",
       "  'text_chunk': 'For example, Plustech’s walking robot provides automatic leg coordination while the human operator chooses an overall direction of travel (figure 1.3). Figure 1.6 depicts an underwater vehicle that controls six propellers to autonomously stabilize the robot subma- rine in spite of underwater turbulence and water currents while the operator chooses posi-tion goals for the submarine to achieve. Other commercial robots operate not where humans cannot go but rather share space with humans in human environments (figure 1.7).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 17,\n",
       "  'text_chunk': 'These robots are compelling not for rea-sons of mobility but because of their autonomy , and so their ability to maintain a sense of position and to navigate without human intervention is paramount. Figure 1.2 The mobile robot Sojourner was used during the Pathfinder mission to explore Mars in summer 1997. It was almost completely teleoperated from Earth. However, some on-board sensors allowed for obstacle detection. (http://ranier.oact.hq.nasa.gov/telerobotics_page/telerobotics.shtm). © NASA/JPL'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 18,\n",
       "  'text_chunk': 'Introduction 3 Figure 1.3 Plustech developed the first application-driven walking robot. It is designed to move wood out of the forest. The leg coordination is automated, but navigation is still done by the human operator on the robot. (http://www.plustech.fi). © Plustech. Figure 1.4 Airduct inspection robot featuring a pan-tilt camera with zoom and sensors for automatic inclination control, wall following, and intersection detection (http://asl.epfl.ch). © Sedirep / EPFL.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 19,\n",
       "  'text_chunk': '4 Chapter 1 Figure 1.5 Picture of Pioneer, a robot designed to explore the Sarcophagus at Chernobyl. © Wide World Photos. Figure 1.6 Picture of recovering MBARI’s ALTEX AUV (autonomous underwater vehicle) onto the Icebreaker Healy following a dive beneath the Arctic ice. Todd Walsh © 2001 MBARI.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 20,\n",
       "  'text_chunk': 'Introduction 5 Figure 1.7 Tour-guide robots are able to interact and present exhibitions in an educational way [48, 118, 132, 143,]. Ten Roboxes have operated during 5 months at the Swiss exhibition EXPO.02, meeting hun- dreds of thousands of visitors. They were developed by EPFL [132] (http://robotics.epfl.ch) and com-mercialized by BlueBotics (http://www.bluebotics.ch). Figure 1.8 Newest generation of the autonomous guided vehicle (AGV) of SWISSLOG used to transport moto r blocks from one assembly station to another.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 20,\n",
       "  'text_chunk': 'It is guided by an electrical wire installed in the floor. There are thousands of AGVs transporting products in industry, warehouses, and even hospitals. ©Swisslog.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 21,\n",
       "  'text_chunk': '6 Chapter 1 Figure 1.9 HELPMATE is a mobile robot used in hospitals for transportation tasks. It has various on-board sen- sors for autonomous navigation in the corridors. The main sensor for localization is a camera looking to the ceiling. It can detect the lamps on the ceiling as references, or landmarks (http:// www.pyxis.com). © Pyxis Corp.front back Figure 1.10 BR 700 industrial cleaning robot (left) and the RoboCleaner RC 3000 consumer robot developed and sold by Alfred Kärcher GmbH & Co., Germany.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 21,\n",
       "  'text_chunk': 'The navigation system of BR 700 is based on a very sophisticated sonar system and a gyro. The RoboCleaner RC 3000 covers badly soiled areas with a special driving strategy until it is really clean. Optical sensors measure the degree of pollution of the aspirated air (http://www.karcher.de). © Alfred Kärcher GmbH & Co.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 22,\n",
       "  'text_chunk': 'Introduction 7 Figure 1.11 PIONEER is a modular mobile robot offering various options like a gripper or an on-board camera. It is equipped with a sophisticated navigation library developed at SRI, Stanford, CA (Reprinted with permission from ActivMedia Robotics, http://www.MobileRobots.com). Figure 1.12B21 of iRobot is a sophisticated mobile robot with up to three Intel Pentium processors on board. It has a large variety of sensors for high-performance navigation tasks (http://www.irobot.com/rwi/). © iRobot Inc.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 23,\n",
       "  'text_chunk': '8 Chapter 1 For example, AGV (autonomous guided vehicle) robots (figure 1.8) autonomously deliver parts between various assembly stations by following special electrical guidewires using a custom sensor. The Helpmate service robot transports food and medication throughout hospitals by tracking the position of ceiling lights, which are manually specifiedto the robot beforehand (figure 1.9). Several companies have developed autonomous clean- ing robots, mainly for large buildings (figure 1.10). One such cleaning robot is in use at the Paris Metro.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 23,\n",
       "  'text_chunk': 'Other specialized cleaning robots take advantage of the regular geometric pat- tern of aisles in supermarkets to facilitate the localization and navigation tasks. Research into high-level questions of cognition, localization, and navigation can be per- formed using standard research robot platforms that are tuned to the laboratory environ- ment. This is one of the largest current markets for mobile robots. Various mobile robot platforms are available for programming, ranging in terms of size and terrain capability.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 23,\n",
       "  'text_chunk': 'The most popular research robots are those of ActivMedia Robotics, K-Team SA, and I-Robot (figures 1.11, 1.12, 1.13) and also very small robots like the Alice from EPFL (Swiss Federal Institute of Technology at Lausanne) (figure 1.14). Although mobile robots have a broad set of applications and markets as summarized above, there is one fact that is true of virtually every successful mobile robot: its design involves the integration of many different bodies of knowledge.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 23,\n",
       "  'text_chunk': 'No mean feat, this makes mobile robotics as interdisciplinary a field as there can be. To solve locomotion problems,the mobile roboticist must understand mechanism and kinematics; dynamics and control theory. To create robust perceptual systems, the mobile roboticist must leverage the fields of signal analysis and specialized bodies of knowledge such as computer vision to properlyFigure 1.13 KHEPERA is a small mobile robot for research and education. It is only about 60 mm in diameter. Various additional modules such as cameras and grippers are available.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 23,\n",
       "  'text_chunk': 'More then 700 units had already been sold by the end of 1998. KHEPERA is manufactured and distributed by K-Team SA, Switzerland (http://www.k-team.com). © K-Team SA.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 24,\n",
       "  'text_chunk': 'Introduction 9 employ a multitude of sensor technologies. Localization and navigation demand knowl- edge of computer algorithms, information theory, artificial intelligence, and probability theory. Figure 1.15 depicts an abstract control scheme for mobile robot systems that we will use throughout this text. This figure identifies many of the main bodies of knowledge associ-ated with mobile robotics. This book provides an introduction to all aspects of mobile robotics, including software and hardware design considerations, related technologies, and algorithmic techniques.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 24,\n",
       "  'text_chunk': 'The intended audience is broad, including both undergraduate and graduate students in intro- ductory mobile robotics courses, as well as individuals fascinated by the field. While notabsolutely required, a familiarity with matrix algebra, calculus, probability theory, and computer programming will significantly enhance the reader’s experience. Mobile robotics is a large field, and this book focuses not on robotics in general, nor on mobile robot applications, but rather on mobility itself.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 24,\n",
       "  'text_chunk': 'From mechanism and perception to localization and navigation, this book focuses on the techniques and technologies that enable robust mobility . Clearly, a useful, commercially viable mobile robot does more than just move. It pol- ishes the supermarket floor, keeps guard in a factory, mows the golf course, provides toursin a museum, or provides guidance in a supermarket.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 24,\n",
       "  'text_chunk': 'The aspiring mobile roboticist willstart with this book, but quickly graduate to course work and research specific to the desired application, integrating techniques from fields as disparate as human-robot interaction, computer vision, and speech understanding. Figure 1.14 Alice is one of the smallest fully autonomous robots. It is approximately 2x2x2 cm, it has an auton- omy of about 8 hours and uses infrared distance sensors, tactile whiskers, or even a small camera for navigation [54].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 25,\n",
       "  'text_chunk': '10 Chapter 1 1.2 An Overview of the Book This book introduces the different aspects of a robot in modules, much like the modules shown in figure 1.15. Chapters 2 and 3 focus on the robot’s low-level locomotive ability . Chapter 4 presents an in-depth view of perception . Then, Chapters 5 and 6 take us to the higher-level challenges of localization and even higher-level cognition , specifically the ability to navigate robustly .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 25,\n",
       "  'text_chunk': 'Each chapter builds upon previous chapters, and so the reader is encouraged to start at the beginning, even if their interest is primarily at the high level.Robotics is peculiar in that solutions to high-level challenges are most meaningful only inthe context of a solid understanding of the low-level details of the system. Chapter 2, “Locomotion”, begins with a survey of the most popular mechanisms that enable locomotion: wheels and legs.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 25,\n",
       "  'text_chunk': 'Numerous robotic examples demonstrate the particu-Figure 1.15 Reference control scheme for mobile robot systems used throughout this book.Raw dataEnvironment Model Local Map“Position” Global Map Actuator Commands Sensing ActingInformation Extraction and InterpretationPath ExecutionCognition Path PlaningKnowledge, Data BaseMission Commands Path Real World EnvironmentLocalization Map Building Motion ControlPerception'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 26,\n",
       "  'text_chunk': 'Introduction 11 lar talents of each form of locomotion. But designing a robot’s locomotive system properly requires the ability to evaluate its overall motion capabilities quantitatively. Chapter 3, “Mobile Robot Kinematics”, applies principles of kinematics to the whole robot, beginning with the kinematic contribution of each wheel and graduating to an analysis of robotmaneuverability enabled by each mobility mechanism configuration.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 26,\n",
       "  'text_chunk': 'The greatest single shortcoming in conventional mobile robotics is, without doubt, per- ception: mobile robots can travel across much of earth’s man-made surfaces, but theycannot perceive the world nearly as well as humans and other animals. Chapter 4, “Percep- tion”, begins a discussion of this challenge by presenting a clear language for describing the performance envelope of mobile robot sensors.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 26,\n",
       "  'text_chunk': 'With this language in hand, chapter 4goes on to present many of the off-the-shelf sensors available to the mobile roboticist, describing their basic principles of operation as well as their performance limitations. The most promising sensor for the future of mobile robotics is vision, and chapter 4 includes anoverview of the theory of operation and the limitations of both charged coupled device (CCD) and complementary metal oxide semiconductor (CMOS) sensors. But perception is more than sensing. Perception is also the interpretation of sensed data in meaningful ways.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 26,\n",
       "  'text_chunk': 'The second half of chapter 4 describes strategies for feature extractionthat have been most useful in mobile robotics applications, including extraction of geomet- ric shapes from range-based sensing data, as well as landmark and whole-image analysisusing vision-based sensing. Armed with locomotion mechanisms and outfitted with hardware and software for per- ception, the mobile robot can move and perceive the world. The first point at which mobil-ity and sensing must meet is localization: mobile robots often need to maintain a sense of position.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 26,\n",
       "  'text_chunk': 'Chapter 5, “Mobile Robot Localization”, describes approaches that obviate the need for direct localization, then delves into fundamental ingredients of successful local-ization strategies: belief representation and map representation. Case studies demonstrate various localization schemes, including both Markov localization and Kalman filter local- ization. The final part of chapter 5 is devoted to a discussion of the challenges and mostpromising techniques for mobile robots to autonomously map their surroundings. Mobile robotics is so young a discipline that it lacks a standardized architecture.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 26,\n",
       "  'text_chunk': 'There is as yet no established robot operating system. But the question of architecture is of para-mount importance when one chooses to address the higher-level competences of a mobile robot: how does a mobile robot navigate robustly from place to place, interpreting data, localizing and controlling its motion all the while? For this highest level of robot compe-tence, which we term navigation competence , there are numerous mobile robots that show- case particular architectural strategies.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 26,\n",
       "  'text_chunk': 'Chapter 6, “Planning and Navigation”, surveys thestate of the art of robot navigation, showing that today’s various techniques are quite sim-ilar, differing primarily in the manner in which they decompose the problem of robot con-'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 27,\n",
       "  'text_chunk': '12 Chapter 1 trol. But first, chapter 6 addresses two skills that a competent, navigating robot usually must demonstrate: obstacle avoidance and path planning. There is far more to know about the cross-disciplinary field of mobile robotics than can be contained in a single book. We hope, though, that this broad introduction will place thereader in the context of mobile robotics’ collective wisdom.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 27,\n",
       "  'text_chunk': 'This is only the beginning, but, with luck, the first robot you program or build will have only good things to say about you.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 28,\n",
       "  'text_chunk': '2 Locomotion 2.1 Introduction A mobile robot needs locomotion mechanisms that enable it to move unbounded through- out its environment. But there are a large variety of possible ways to move, and so the selec-tion of a robot’s approach to locomotion is an important aspect of mobile robot design. In the laboratory, there are research robots that can walk, jump, run, slide, skate, swim, fly, and, of course, roll.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 28,\n",
       "  'text_chunk': 'Most of these locomotion mechanisms have been inspired by their bio-logical counterparts (see figure 2.1). There is, however, one exception: the actively powered wheel is a human invention that achieves extremely high efficiency on flat ground. This mechanism is not completely for-eign to biological systems. Our bipedal walking system can be approximated by a rolling polygon, with sides equal in length to the span of the step (figure 2.2). As the step size decreases, the polygon approaches a circle or wheel.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 28,\n",
       "  'text_chunk': 'But nature did not develop a fullyrotating, actively powered joint, which is the technology necessary for wheeled locomo- tion. Biological systems succeed in moving through a wide variety of harsh environments. Therefore it can be desirable to copy their selection of locomotion mechanisms. However, replicating nature in this regard is extremely difficult for several reasons. To begin with, mechanical complexity is easily achieved in biological systems through structural replica-tion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 28,\n",
       "  'text_chunk': 'Cell division, in combination with specialization, can readily produce a millipede with several hundred legs and several tens of thousands of individually sensed cilia. In man- made structures, each part must be fabricated individually, and so no such economies ofscale exist. Additionally, the cell is a microscopic building block that enables extreme min- iaturization. With very small size and weight, insects achieve a level of robustness that we have not been able to match with human fabrication techniques.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 28,\n",
       "  'text_chunk': 'Finally, the biologicalenergy storage system and the muscular and hydraulic activation systems used by large ani- mals and insects achieve torque, response time, and conversion efficiencies that far exceed similarly scaled man-made systems. d'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 29,\n",
       "  'text_chunk': '14 Chapter 2 Owing to these limitations, mobile robots generally locomote either using wheeled mechanisms, a well-known human technology for vehicles, or using a small number of articulated legs, the simplest of the biological approaches to locomotion (see figure 2.2). In general, legged locomotion requires higher degrees of freedom and therefore greater mechanical complexity than wheeled locomotion. Wheels, in addition to being simple, are extremely well suited to flat ground.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 29,\n",
       "  'text_chunk': 'As figure 2.3 depicts, on flat surfaces wheeled loco- motion is one to two orders of magnitude more efficient than legged locomotion. The rail- way is ideally engineered for wheeled locomotion because rolling friction is minimized on a hard and flat steel surface. But as the surface becomes soft, wheeled locomotion accumu- lates inefficiencies due to rolling friction whereas legged locomotion suffers much less because it consists only of point contacts with the ground.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 29,\n",
       "  'text_chunk': 'This is demonstrated in figure 2.3 by the dramatic loss of efficiency in the case of a tire on soft ground.Figure 2.1 Locomotion mechanisms used in biological systems.Flow in Crawl Sliding Running Jumping WalkingType of motion Resistance to motion Basic kinematics of motion Hydrodynamic forces Friction forces Friction forces Loss of kinetic energy Loss of kinetic energy Gravitational forcesRolling of a Oscillatory of a multi-linkTransverse vibrationLongitudinal vibrationEddies a Channel polygon (see figure 2.2)movement pendulumOscillatory of a multi-linkmovement pendulum'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 30,\n",
       "  'text_chunk': 'Locomotion 15 Figure 2.2 A biped walking system can be approximated by a rolling polygon, with sides equal in length d to the span of the step. As the step size decreases, the polygon approaches a circle or wheel with the radius l.h lO αα d Figure 2.3Specific power versus attainable speed of various locomotion mechanisms [33].1 10 100100 10 1 0.1unit power (hp/ton) speed (miles/hour)crawling/sliding runningtire on soft ground walking railway wheelflow'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 31,\n",
       "  'text_chunk': '16 Chapter 2 In effect, the efficiency of wheeled locomotion depends greatly on environmental qual- ities, particularly the flatness and hardness of the ground, while the efficiency of legged locomotion depends on the leg mass and body mass, both of which the robot must support at various points in a legged gait. It is understandable therefore that nature favors legged locomotion, since locomotion systems in nature must operate on rough and unstructured terrain.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 31,\n",
       "  'text_chunk': 'For example, in the caseof insects in a forest the vertical variation in ground height is often an order of magnitude greater than the total height of the insect. By the same token, the human environment fre-quently consists of engineered, smooth surfaces, both indoors and outdoors. Therefore, it is also understandable that virtually all industrial applications of mobile robotics utilize some form of wheeled locomotion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 31,\n",
       "  'text_chunk': 'Recently, for more natural outdoor environments, there has been some progress toward hybrid and legged industrial robots such as the forestry robot shown in figure 2.4. In the section 2.1.1, we present general considerations that concern all forms of mobile robot locomotion. Following this, in sections 2.2 and 2.3, we present overviews of legged locomotion and wheeled locomotion techniques for mobile robots. 2.1.1 Key issues for locomotion Locomotion is the complement of manipulation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 31,\n",
       "  'text_chunk': 'In manipulation, the robot arm is fixed but moves objects in the workspace by imparting force to them. In locomotion, the environ-ment is fixed and the robot moves by imparting force to the environment. In both cases, the scientific basis is the study of actuators that generate interaction forces, and mechanismsFigure 2.4 RoboTrac, a hybrid wheel-leg vehicle for rough terrain [130].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 32,\n",
       "  'text_chunk': 'Locomotion 17 that implement desired kinematic and dynamic properties. Locomotion and manipulation thus share the same core issues of stability, contact characteristics, and environmental type: •stability - number and geometry of contact points- center of gravity - static/dynamic stability - inclination of terrain •characteristics of contact - contact point/path size and shape - angle of contact- friction •type of environment - structure- medium, (e.g. water, air, soft or hard ground) A theoretical analysis of locomotion begins with mechanics and physics.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 32,\n",
       "  'text_chunk': 'From this start- ing point, we can formally define and analyze all manner of mobile robot locomotion sys- tems. However, this book focuses on the mobile robot navigation problem, particularly stressing perception, localization, and cognition. Thus we will not delve deeply into thephysical basis of locomotion. Nevertheless, the two remaining sections in this chapterpresent overviews of issues in legged locomotion [33] and wheeled locomotion. Then, chapter 3 presents a more detailed analysis of the kinematics and control of wheeled mobile robots.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 32,\n",
       "  'text_chunk': '2.2 Legged Mobile Robots Legged locomotion is characterized by a series of point contacts between the robot and the ground. The key advantages include adaptability and maneuverability in rough terrain. Because only a set of point contacts is required, the quality of the ground between thosepoints does not matter so long as the robot can maintain adequate ground clearance. In addi- tion, a walking robot is capable of crossing a hole or chasm so long as its reach exceeds the width of the hole.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 32,\n",
       "  'text_chunk': 'A final advantage of legged locomotion is the potential to manipulateobjects in the environment with great skill. An excellent insect example, the dung beetle, is capable of rolling a ball while locomoting by way of its dexterous front legs. The main disadvantages of legged locomotion include power and mechanical complex- ity. The leg, which may include several degrees of freedom, must be capable of sustainingpart of the robot’s total weight, and in many robots must be capable of lifting and lowering the robot.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 32,\n",
       "  'text_chunk': 'Additionally, high maneuverability will only be achieved if the legs have a suf-ficient number of degrees of freedom to impart forces in a number of different directions.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 33,\n",
       "  'text_chunk': '18 Chapter 2 2.2.1 Leg configurations and stability Because legged robots are biologically inspired, it is instructive to examine biologically successful legged systems. A number of different leg configurations have been successfulin a variety of organisms (figure 2.5). Large animals, such as mammals and reptiles, have four legs, whereas insects have six or more legs. In some mammals, the ability to walk on only two legs has been perfected.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 33,\n",
       "  'text_chunk': 'Especially in the case of humans, balance has progressedto the point that we can even jump with one leg 1. This exceptional maneuverability comes at a price: much more complex active control to maintain balance. In contrast, a creature with three legs can exhibit a static, stable pose provided that it can ensure that its center of gravity is within the tripod of ground contact. Static stability, dem- onstrated by a three-legged stool, means that balance is maintained with no need for motion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 33,\n",
       "  'text_chunk': 'A small deviation from stability (e.g., gently pushing the stool) is passively cor- rected toward the stable pose when the upsetting force stops. But a robot must be able to lift its legs in order to walk. In order to achieve static walk- ing, a robot must have at least six legs. In such a configuration, it is possible to design a gait in which a statically stable tripod of legs is in contact with the ground at all times (figure 2.8).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 33,\n",
       "  'text_chunk': 'Insects and spiders are immediately able to walk when born. For them, the problem of balance during walking is relatively simple. Mammals, with four legs, cannot achieve static walking, but are able to stand easily on four legs. Fauns, for example, spend several minutesattempting to stand before they are able to do so, then spend several more minutes learning to walk without falling. Humans, with two legs, cannot even stand in one place with static stability.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 33,\n",
       "  'text_chunk': 'Infants require months to stand and walk, and even longer to learn to jump, run,and stand on one leg. 1.In child development, one of the tests used to determine if the child is acquiring advanced loco- motion skills is the ability to jump on one leg.Figure 2.5 Arrangement of the legs of various animals.mammals reptiles insects two or four legs four legs six legs'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 34,\n",
       "  'text_chunk': 'Locomotion 19 There is also the potential for great variety in the complexity of each individual leg. Once again, the biological world provides ample examples at both extremes. For instance, in the case of the caterpillar, each leg is extended using hydraulic pressure by constrictingthe body cavity and forcing an increase in pressure, and each leg is retracted longitudinally by relaxing the hydraulic pressure, then activating a single tensile muscle that pulls the leg in toward the body.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 34,\n",
       "  'text_chunk': 'Each leg has only a single degree of freedom, which is oriented longi- tudinally along the leg. Forward locomotion depends on the hydraulic pressure in the body, which extends the distance between pairs of legs. The caterpillar leg is therefore mechani- cally very simple, using a minimal number of extrinsic muscles to achieve complex overalllocomotion. At the other extreme, the human leg has more than seven major degrees of freedom, combined with further actuation at the toes. More than fifteen muscle groups actuate eightcomplex joints.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 34,\n",
       "  'text_chunk': 'In the case of legged mobile robots, a minimum of two degrees of freedom is generally required to move a leg forward by lifting the leg and swinging it forward. More common isthe addition of a third degree of freedom for more complex maneuvers, resulting in legs such as those shown in figure 2.6. Recent successes in the creation of bipedal walking robots have added a fourth degree of freedom at the ankle joint. The ankle enables moreconsistent ground contact by actuating the pose of the sole of the foot.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 34,\n",
       "  'text_chunk': 'In general, adding degrees of freedom to a robot leg increases the maneuverability of the robot, both augmenting the range of terrains on which it can travel and the ability of therobot to travel with a variety of gaits. The primary disadvantages of additional joints and actuators are, of course, energy, control, and mass.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 34,\n",
       "  'text_chunk': 'Additional actuators require energy and control, and they also add to leg mass, further increasing power and load requirements onexisting actuators.Figure 2.6 Two examples of legs with three degrees of freedom.θ hip flexion angle ( ψ)hip abduction angle ( θ) knee flexion angle ( ϕ) ϕ ψabduction-adduction upper thigh link lower thigh linkmain drivelift shank link'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 35,\n",
       "  'text_chunk': '20 Chapter 2 In the case of a multilegged mobile robot, there is the issue of leg coordination for loco- motion, or gait control. The number of possible gaits depends on the number of legs [33]. The gait is a sequence of lift and release events for the individual legs.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 35,\n",
       "  'text_chunk': 'For a mobile robotwith legs, the total number of possible events for a walking machine is (2.1) For a biped walker legs, the number of possible events is (2.2)k NFigure 2.7 Two gaits with four legs. Because this robot has fewer than six legs, static walking is not generally possible.changeover walking gallopingfree fly N 2k1–() ! = k 2= N N 2k1–() !3 !3 2 1 ⋅⋅ 6 == = ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 36,\n",
       "  'text_chunk': 'Locomotion 21 The six different events are 1.lift right leg; 2.lift left leg; 3.release right leg; 4.release left leg; 5.lift both legs together; 6.release both legs together. Of course, this quickly grows quite large. For example, a robot with six legs has far more gaits theoretically: (2.3) Figures 2.7 and 2.8 depict several four-legged gaits and the static six-legged tripod gait. 2.2.2 Examples of legged robot locomotion Although there are no high-volume industrial applications to date, legged locomotion is animportant area of long-term research.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 36,\n",
       "  'text_chunk': 'Several interesting designs are presented below, beginning with the one-legged robot and finishing with six-legged robots. For a very good overview of climbing and walking robots, see http://www.uwe.ac.uk/clawar/ . 2.2.2.1 One leg The minimum number of legs a legged robot can have is, of course, one. Minimizing the number of legs is beneficial for several reasons. Body mass is particularly important towalking machines, and the single leg minimizes cumulative leg mass.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 36,\n",
       "  'text_chunk': 'Leg coordination is required when a robot has several legs, but with one leg no such coordination is needed. Perhaps most importantly, the one-legged robot maximizes the basic advantage of leggedlocomotion: legs have single points of contact with the ground in lieu of an entire track, as with wheels. A single-legged robot requires only a sequence of single contacts, making it amenable to the roughest terrain.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 36,\n",
       "  'text_chunk': 'Furthermore, a hopping robot can dynamically cross a gapthat is larger than its stride by taking a running start, whereas a multilegged walking robot that cannot run is limited to crossing gaps that are as large as its reach. The major challenge in creating a single-legged robot is balance. For a robot with one leg, static walking is not only impossible but static stability when stationary is also impos-sible. The robot must actively balance itself by either changing its center of gravity or by imparting corrective forces.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 36,\n",
       "  'text_chunk': 'Thus, the successful single-legged robot must be dynamicallystable.N 11! 3991680 0 =='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 37,\n",
       "  'text_chunk': '22 Chapter 2 Figure 2.9 shows the Raibert hopper [28, 124], one of the most well-known single- legged hopping robots created. This robot makes continuous corrections to body attitude and to robot velocity by adjusting the leg angle with respect to the body. The actuation is hydraulic, including high-power longitudinal extension of the leg during stance to hop back into the air. Although powerful, these actuators require a large, off-board hydraulic pumpto be connected to the robot at all times.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 37,\n",
       "  'text_chunk': 'Figure 2.10 shows a more energy-efficient design developed more recently [46]. Instead of supplying power by means of an off-board hydraulic pump, the bow leg hopper is designed to capture the kinetic energy of the robot as it lands, using an efficient bow spring leg. This spring returns approximately 85% of the energy, meaning that stable hopping requires only the addition of 15% of the required energy on each hop.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 37,\n",
       "  'text_chunk': 'This robot, which isconstrained along one axis by a boom, has demonstrated continuous hopping for 20 minutes using a single set of batteries carried on board the robot. As with the Raibert hopper, the bow leg hopper controls velocity by changing the angle of the leg to the body at the hipjoint.Figure 2.8 Static walking with six legs. A tripod formed by three legs always exists.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 38,\n",
       "  'text_chunk': 'Locomotion 23 Figure 2.9 The Raibert hopper [28, 124]. Image courtesy of the LegLab and Marc Raibert. © 1983. Figure 2.10The 2D single bow leg hopper [46]. Image courtesy of H. Benjamin Brown and Garth Zeglin, CMU.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 39,\n",
       "  'text_chunk': '24 Chapter 2 The paper of Ringrose [125] demonstrates the very important duality of mechanics and controls as applied to a single-legged hopping machine. Often clever mechanical design can perform the same operations as complex active control circuitry. In this robot, the phys- ical shape of the foot is exactly the right curve so that when the robot lands without beingperfectly vertical, the proper corrective force is provided from the impact, making the robot vertical by the next landing. This robot is dynamically stable, and is furthermore passive.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 39,\n",
       "  'text_chunk': 'The correction is provided by physical interactions between the robot and its environment,with no computer or any active control in the loop. 2.2.2.2 Two legs (biped) A variety of successful bipedal robots have been demonstrated over the past ten years. Twolegged robots have been shown to run, jump, travel up and down stairways, and even do aerial tricks such as somersaults. In the commercial sector, both Honda and Sony have made significant advances over the past decade that have enabled highly capable bipedalrobots.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 39,\n",
       "  'text_chunk': 'Both companies designed small, powered joints that achieve power-to-weight per- formance unheard of in commercially available servomotors. These new “intelligent” servos provide not only strong actuation but also compliant actuation by means of torquesensing and closed-loop control. Figure 2.11 The Sony SDR-4X II, © 2003 Sony Corporation. Specifications: Weight: 7 kg Height: 58 cmNeck DOF: 4 Body DOF: 2 Arm DOF: 2 x 5 Legs DOF: 2 x 6 Five-finger Hands'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 40,\n",
       "  'text_chunk': 'Locomotion 25 The Sony Dream Robot, model SDR-4X II, is shown in figure 2.11. This current model is the result of research begun in 1997 with the basic objective of motion entertainment and communication entertainment (i.e., dancing and singing). This robot with thirty-eight degrees of freedom has seven microphones for fine localization of sound, image-basedperson recognition, on-board miniature stereo depth-map reconstruction, and limited speech recognition.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 40,\n",
       "  'text_chunk': 'Given the goal of fluid and entertaining motion, Sony spent consider- able effort designing a motion prototyping application system to enable their engineers toscript dances in a straightforward manner. Note that the SDR-4X II is relatively small, standing at 58 cm and weighing only 6.5 kg. The Honda humanoid project has a significant history but, again, has tackled the very important engineering challenge of actuation. Figure 2.12 shows model P2, which is an immediate predecessor to the most recent Asimo model (advanced step in innovative mobility).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 40,\n",
       "  'text_chunk': 'Note from this picture that the Honda humanoid is much larger than the SDR-4X at 120 cm tall and 52 kg. This enables practical mobility in the human world of stairs and ledges while maintaining a nonthreatening size and posture. Perhaps the first robot to famously demonstrate biomimetic bipedal stair climbing and descending, these Hondahumanoid series robots are being designed not for entertainment purposes but as human aids throughout society.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 40,\n",
       "  'text_chunk': 'Honda refers, for instance, to the height of Asimo as the minimum height which enables it to nonetheless manage operation of the human world, for instance,control of light switches. Figure 2.12 The humanoid robot P2 from Honda, Japan. © Honda Motor Corporation. Specifications: Maximum speed: 2 km/h Autonomy: 15 minWeight: 210 kg Height: 1.82 m Leg DOF: 2 x 6Arm DOF: 2 x 7'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 41,\n",
       "  'text_chunk': '26 Chapter 2 An important feature of bipedal robots is their anthropomorphic shape. They can be built to have the same approximate dimensions as humans, and this makes them excellent vehi- cles for research in human-robot interaction. WABIAN is a robot built at Waseda Univer-sities Japan (figure 2.13) for just such research [75]. WABIAN is designed to emulate human motion, and is even designed to dance like a human.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 41,\n",
       "  'text_chunk': 'Bipedal robots can only be statically stable within some limits, and so robots such as P2 and WABIAN generally must perform continuous balance-correcting servoing even whenstanding still. Furthermore, each leg must have sufficient capacity to support the full weight of the robot. In the case of four-legged robots, the balance problem is facilitated along withthe load requirements of each leg. An elegant design of a biped robot is the Spring Fla- mingo of MIT (figure 2.14).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 41,\n",
       "  'text_chunk': 'This robot inserts springs in series with the leg actuators to achieve a more elastic gait. Combined with “kneecaps” that limit knee joint angles, the Fla-mingo achieves surprisingly biomimetic motion. 2.2.2.3 Four legs (quadruped) Although standing still on four legs is passively stable, walking remains challengingbecause to remain stable the robot’s center of gravity must be actively shifted during theFigure 2.13 The humanoid robot WABIAN-RIII at Waseda University in Japan [75].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 41,\n",
       "  'text_chunk': 'Image courtesy of Atsuo Takanishi, Waseda University.Specifications: Weight: 131 [kg] Height: 1.88 [m] DOF in total: 43 Lower Limbs: 2 x 6 Trunk: 3 Arms: 2 x 10 Neck: 4Eyes: 2 x 2'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 42,\n",
       "  'text_chunk': 'Locomotion 27 gait. Sony recently invested several million dollars to develop a four-legged robot called AIBO (figure 2.15). To create this robot, Sony produced both a new robot operating system that is near real-time and new geared servomotors that are of sufficiently high torque to sup-port the robot, yet back drivable for safety. In addition to developing custom motors and software, Sony incorporated a color vision system that enables AIBO to chase a brightly colored ball. The robot is able to function for at most one hour before requiring recharging.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 42,\n",
       "  'text_chunk': 'Early sales of the robot have been very strong, with more than 60,000 units sold in the first year. Nevertheless, the number of motors and the technology investment behind this robot dog resulted in a very high price of approximately $1500. Four-legged robots have the potential to serve as effective artifacts for research in human-robot interaction (figure 2.16). Humans can treat the Sony robot, for example, as apet and might develop an emotional relationship similar to that between man and dog.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 42,\n",
       "  'text_chunk': 'Fur-thermore, Sony has designed AIBO’s walking style and general behavior to emulate learn- ing and maturation, resulting in dynamic behavior over time that is more interesting for the owner who can track the changing behavior. As the challenges of high energy storage andmotor technology are solved, it is likely that quadruped robots much more capable than AIBO will become common throughout the human environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 42,\n",
       "  'text_chunk': '2.2.2.4 Six legs (hexapod) Six-legged configurations have been extremely popular in mobile robotics because of their static stability during walking, thus reducing the control complexity (figures 2.17 and 1.3).Figure 2.14 The Spring Flamingo developed at MIT [123]. Image courtesy of Jerry Pratt, MIT Leg Laboratory.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 43,\n",
       "  'text_chunk': '28 Chapter 2 In most cases, each leg has three degrees of freedom, including hip flexion, knee flexion, and hip abduction (see figure 2.6). Genghis is a commercially available hobby robot that has six legs, each of which has two degrees of freedom provided by hobby servos (figure 2.18). Such a robot, which consists only of hip flexion and hip abduction, has less maneu-verability in rough terrain but performs quite well on flat ground.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 43,\n",
       "  'text_chunk': 'Because it consists of a straightforward arrangement of servomotors and straight legs, such robots can be readily built by a robot hobbyist. Insects, which are arguably the most successful locomoting creatures on earth, excel at traversing all forms of terrain with six legs, even upside down. Currently, the gap betweenthe capabilities of six-legged insects and artificial six-legged robots is still quite large.Interestingly, this is not due to a lack of sufficient numbers of degrees of freedom on the robots.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 43,\n",
       "  'text_chunk': 'Rather, insects combine a small number of active degrees of freedom with passiveFigure 2.15 AIBO, the artificial dog from Sony, Japan. 1 Stereo microphone: Allows AIBO to pick up surrounding sounds. 2 Head sensor: Senses when a person taps or pets AIBO on the head. 3 Mode indicator: Shows AIBO’s operation mode. 4 Eye lights: These light up in blue-green orred to indicate AIBO’s emotional state. 5 Color camera: Allows AIBO to search for objects and recognize them by color andmovement.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 43,\n",
       "  'text_chunk': '6 Speaker: Emits various musical tones and sound effects. 7 Chin sensor: Senses when a person touches AIBO on the chin. 8 Pause button: Press to activate AIBO or to pause AIBO. 9 Chest light: Gives information about thestatus of the robot. 10 Paw sensors: Located on the bottom of eachpaw. 11 Tail light: Lights up blue or orange to show AIBO’s emotional state. 12 Back sensor: Senses when a person touchesAIBO on the back.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 43,\n",
       "  'text_chunk': 'ERS-210 © 2000 Sony CorporationERS- 110 © 1999 Sony Corporation'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 44,\n",
       "  'text_chunk': 'Locomotion 29 Figure 2.16 Titan VIII, a quadruped robot developed at Tokyo Institute of Technology. (http://mozu.mes.titech.ac.jp/research/walk/). © Tokyo Institute of Technology. Specifications: Weight:1 9 kg Height: 0.25 m DOF: 4 x 3 Figure 2.17 Lauron II, a hexapod platform developed at the University of Karlsruhe, Germany. © University of Karlsruhe.Specifications: Maximum speed: 0.5 m/s Weight:1 6 kg Height: 0.3 mLength: 0.7 m No. of legs: 6 DOF in total: 6 x 3Power consumption:10 W'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 45,\n",
       "  'text_chunk': '30 Chapter 2 structures, such as microscopic barbs and textured pads, that increase the gripping strength of each leg significantly. Robotic research into such passive tip structures has only recently begun. For example, a research group is attempting to re-create the complete mechanical function of the cockroach leg [65]. It is clear from the above examples that legged robots have much progress to make before they are competitive with their biological equivalents. Nevertheless, significantgains have been realized recently, primarily due to advances in motor design.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 45,\n",
       "  'text_chunk': 'Creatingactuation systems that approach the efficiency of animal muscles remains far from the reach of robotics, as does energy storage with the energy densities found in organic life forms. 2.3 Wheeled Mobile Robots The wheel has been by far the most popular locomotion mechanism in mobile robotics and in man-made vehicles in general. It can achieve very good efficiencies, as demonstrated in figure 2.3, and does so with a relatively simple mechanical implementation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 45,\n",
       "  'text_chunk': 'In addition, balance is not usually a research problem in wheeled robot designs, because wheeled robots are almost always designed so that all wheels are in ground contact at alltimes. Thus, three wheels are sufficient to guarantee stable balance, although, as we shallsee below, two-wheeled robots can also be stable. When more than three wheels are used, a suspension system is required to allow all wheels to maintain ground contact when the robot encounters uneven terrain.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 45,\n",
       "  'text_chunk': 'Instead of worrying about balance, wheeled robot research tends to focus on the prob- lems of traction and stability, maneuverability, and control: can the robot wheels provideFigure 2.18 Genghis, one of the most famous walking robots from MIT, uses hobby servomotors as its actuators (http://www.ai.mit.edu/projects/genghis). © MIT AI Lab.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 46,\n",
       "  'text_chunk': 'Locomotion 31 sufficient traction and stability for the robot to cover all of the desired terrain, and does the robot’s wheeled configuration enable sufficient control over the velocity of the robot? 2.3.1 Wheeled locomotion: the design space As we shall see, there is a very large space of possible wheel configurations when one con- siders possible techniques for mobile robot locomotion. We begin by discussing the wheel in detail, as there are a number of different wheel types with specific strengths and weak-nesses.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 46,\n",
       "  'text_chunk': 'Then, we examine complete wheel configurations that deliver particular forms of locomotion for a mobile robot. 2.3.1.1 Wheel design There are four major wheel classes, as shown in figure 2.19. They differ widely in their kinematics, and therefore the choice of wheel type has a large effect on the overall kinemat- ics of the mobile robot. The standard wheel and the castor wheel have a primary axis ofrotation and are thus highly directional. To move in a different direction, the wheel must be steered first along a vertical axis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 46,\n",
       "  'text_chunk': 'The key difference between these two wheels is that the standard wheel can accomplish this steering motion with no side effects, as the center ofrotation passes through the contact patch with the ground, whereas the castor wheel rotates around an offset axis, causing a force to be imparted to the robot chassis during steering. Figure 2.19 The four basic wheel types. (a) Standard wheel: two degrees of freedom; rotation around the (motor- ized) wheel axle and the contact point.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 46,\n",
       "  'text_chunk': '(b) castor wheel: two degrees of freedom; rotation around anoffset steering joint. (c) Swedish wheel: three degrees of freedom; rotation around the (motorized) wheel axle, around the rollers, and around the contact point. (d) Ball or spherical wheel: realization technically difficult.a) Swedish 90° Swedish 45° Swedish 45°b) c) d)'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 47,\n",
       "  'text_chunk': '32 Chapter 2 The Swedish wheel and the spherical wheel are both designs that are less constrained by directionality than the conventional standard wheel. The Swedish wheel functions as a normal wheel, but provides low resistance in another direction as well, sometimes perpen- dicular to the conventional direction, as in the Swedish 90, and sometimes at an intermedi-ate angle, as in the Swedish 45. The small rollers attached around the circumference of the wheel are passive and the wheel’s primary axis serves as the only actively powered joint.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 47,\n",
       "  'text_chunk': 'The key advantage of this design is that, although the wheel rotation is powered only alongthe one principal axis (through the axle), the wheel can kinematically move with very little friction along many possible trajectories, not just forward and backward. The spherical wheel is a truly omnidirectional wheel, often designed so that it may be actively powered to spin along any direction. One mechanism for implementing this spher-ical design imitates the computer mouse, providing actively powered rollers that rest against the top surface of the sphere and impart rotational force.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 47,\n",
       "  'text_chunk': 'Regardless of what wheel is used, in robots designed for all-terrain environments and in robots with more than three wheels, a suspension system is normally required to maintain wheel contact with the ground. One of the simplest approaches to suspension is to design flexibility into the wheel itself. For instance, in the case of some four-wheeled indoor robots that use castor wheels, manufacturers have applied a deformable tire of soft rubber to the wheel to create a primitive suspension.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 47,\n",
       "  'text_chunk': 'Of course, this limited solution cannot compete witha sophisticated suspension system in applications where the robot needs a more dynamic suspension for significantly non flat terrain.Figure 2.20 Navlab I, the first autonomous highway vehicle that steers and controls the throttle using vision and radar sensors [61]. Developed at CMU.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 48,\n",
       "  'text_chunk': 'Locomotion 33 2.3.1.2 Wheel geometry The choice of wheel types for a mobile robot is strongly linked to the choice of wheel arrangement, or wheel geometry. The mobile robot designer must consider these two issues simultaneously when designing the locomoting mechanism of a wheeled robot. Why dowheel type and wheel geometry matter? Three fundamental characteristics of a robot are governed by these choices: maneuverability, controllability, and stability.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 48,\n",
       "  'text_chunk': 'Unlike automobiles, which are largely designed for a highly standardized environment (the road network), mobile robots are designed for applications in a wide variety of situa- tions. Automobiles all share similar wheel configurations because there is one region in the design space that maximizes maneuverability, controllability, and stability for their stan-dard environment: the paved roadway. However, there is no single wheel configuration that maximizes these qualities for the variety of environments faced by different mobile robots.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 48,\n",
       "  'text_chunk': 'So you will see great variety in the wheel configurations of mobile robots. In fact, fewrobots use the Ackerman wheel configuration of the automobile because of its poor maneu- verability, with the exception of mobile robots designed for the road system (figure 2.20). Table 2.1 gives an overview of wheel configurations ordered by the number of wheels. This table shows both the selection of particular wheel types and their geometric configu-ration on the robot chassis. Note that some of the configurations shown are of little use in mobile robot applications.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 48,\n",
       "  'text_chunk': 'For instance, the two-wheeled bicycle arrangement has moder-ate maneuverability and poor controllability. Like a single-legged hopping machine, it can never stand still. Nevertheless, this table provides an indication of the large variety of wheel configurations that are possible in mobile robot design. The number of variations in table 2.1 is quite large. However, there are important trends and groupings that can aid in comprehending the advantages and disadvantages of each configuration.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 48,\n",
       "  'text_chunk': 'Below, we identify some of the key trade-offs in terms of the three issues weidentified earlier: stability, maneuverability, and controllability. 2.3.1.3 Stability Surprisingly, the minimum number of wheels required for static stability is two. As shownabove, a two-wheel differential-drive robot can achieve static stability if the center of mass is below the wheel axle. Cye is a commercial mobile robot that uses this wheel configura- tion (figure 2.21). However, under ordinary circumstances such a solution requires wheel diameters that are impractically large.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 48,\n",
       "  'text_chunk': 'Dynamics can also cause a two-wheeled robot to strike the floorwith a third point of contact, for instance, with sufficiently high motor torques from stand-still. Conventionally, static stability requires a minimum of three wheels, with the addi- tional caveat that the center of gravity must be contained within the triangle formed by the ground contact points of the wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 48,\n",
       "  'text_chunk': 'Stability can be further improved by adding more wheels, although once the number of contact points exceeds three, the hyperstatic nature of the geometry will require some form of flexible suspension on uneven terrain.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 49,\n",
       "  'text_chunk': '34 Chapter 2 Table 2.1 Wheel configurations for rolling vehicles # of wheelsArrangement Description Typical examples 2 One steering wheel in the front, one traction wheel in the rearBicycle, motorcycle Two-wheel differential drive with the center of mass (COM) below the axleCye personal robot 3 Two-wheel centered differen- tial drive with a third point of contactNomad Scout, smartRob EPFL Two independently driven wheels in the rear/front, 1 unpowered omnidirectional wheel in the front/rearMany indoor robots, including the EPFL robots Pygmalion and Alice Two connected traction wheels (differential) in rear, 1 steered free wheel in frontPiaggio minitrucks Two free wheels in rear, 1 steered traction wheel in frontNeptune (Carnegie Mellon University), Hero-1 Three motorized Swedish or spherical wheels arranged in a triangle; omnidirectional move- ment is possibleStanford wheel Tribolo EPFL, Palm Pilot Robot Kit (CMU) Three synchronously motorized and steered wheels; the orienta- tion is not controllable“Synchro drive” Denning MRV-2, Geor- gia Institute of Technol- ogy, I-Robot B24, Nomad 200'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 50,\n",
       "  'text_chunk': 'Locomotion 35 4 Two motorized wheels in the rear, 2 steered wheels in the front; steering has to be differ- ent for the 2 wheels to avoid slipping/skidding.Car with rear-wheel drive Two motorized and steered wheels in the front, 2 free wheels in the rear; steering has to be different for the 2 wheels to avoid slipping/skidding.Car with front-wheel drive Four steered and motorized wheelsFour-wheel drive, four-wheel steering Hyperion (CMU) Two traction wheels (differen-tial) in rear/front, 2 omnidirec- tional wheels in the front/rearCharlie (DMT-EPFL) Four omnidirectional wheels Carnegie Mellon Uranus Two-wheel differential drive with 2 additional points of con- tactEPFL Khepera, Hyperbot Chip Four motorized and steered castor wheelsNomad XR4000Table 2.1 Wheel configurations for rolling vehicles # of wheelsArrangement Description Typical examples'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 51,\n",
       "  'text_chunk': '36 Chapter 2 2.3.1.4 Maneuverability Some robots are omnidirectional, meaning that they can move at any time in any direction along the ground plane regardless of the orientation of the robot around its verticalaxis. This level of maneuverability requires wheels that can move in more than just one direction, and so omnidirectional robots usually employ Swedish or spherical wheels that are powered. A good example is Uranus, shown in figure 2.24. This robot uses four Swed-ish wheels to rotate and translate independently and without constraints.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 51,\n",
       "  'text_chunk': '6 Two motorized and steered wheels aligned in center, 1 omnidirectional wheel at each cornerFirst Two traction wheels (differen- tial) in center, 1 omnidirec- tional wheel at each cornerTerregator (Carnegie Mel- lon University) Icons for the each wheel type are as follows: unpowered omnidirectional wheel (spherical, castor, Swedish); motorized Swedish wheel (Stanford wheel); unpowered standard wheel; motorized standard wheel; motorized and steered castor wheel; steered standard wheel; connected wheels.Table 2.1 Wheel configurations for rolling vehicles # of wheelsArrangement Description Typical examples xy,()'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 52,\n",
       "  'text_chunk': 'Locomotion 37 In general, the ground clearance of robots with Swedish and spherical wheels is some- what limited due to the mechanical constraints of constructing omnidirectional wheels. An interesting recent solution to the problem of omnidirectional navigation while solving this ground-clearance problem is the four-castor wheel configuration in which each castorwheel is actively steered and actively translated.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 52,\n",
       "  'text_chunk': 'In this configuration, the robot is truly omnidirectional because, even if the castor wheels are facing a direction perpendicular to the desired direction of travel, the robot can still move in the desired direction by steeringthese wheels. Because the vertical axis is offset from the ground-contact path, the result of this steering motion is robot motion. In the research community, other classes of mobile robots are popular which achieve high maneuverability, only slightly inferior to that of the omnidirectional configurations.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 52,\n",
       "  'text_chunk': 'In such robots, motion in a particular direction may initially require a rotational motion. With a circular chassis and an axis of rotation at the center of the robot, such a robot can spin without changing its ground footprint. The most popular such robot is the two-wheel differential-drive robot where the two wheels rotate around the center point of the robot.One or two additional ground contact points may be used for stability, based on the appli- cation specifics. In contrast to the above configurations, consider the Ackerman steering configuration common in automobiles.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 52,\n",
       "  'text_chunk': 'Such a vehicle typically has a turning diameter that is larger thanthe car. Furthermore, for such a vehicle to move sideways requires a parking maneuver con- sisting of repeated changes in direction forward and backward. Nevertheless, Ackermansteering geometries have been especially popular in the hobby robotics market, where a robot can be built by starting with a remote control racecar kit and adding sensing and autonomy to the existing mechanism.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 52,\n",
       "  'text_chunk': 'In addition, the limited maneuverability of AckermanFigure 2.21 Cye, a commercially available domestic robot that can vacuum and make deliveries in the home, is built by Aethon Inc. (http://www.aethon.com). © Aethon Inc.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 53,\n",
       "  'text_chunk': '38 Chapter 2 steering has an important advantage: its directionality and steering geometry provide it with very good lateral stability in high-speed turns. 2.3.1.5 Controllability There is generally an inverse correlation between controllability and maneuverability. Forexample, the omnidirectional designs such as the four-castor wheel configuration require significant processing to convert desired rotational and translational velocities to individual wheel commands. Furthermore, such omnidirectional designs often have greater degrees offreedom at the wheel. For instance, the Swedish wheel has a set of free rollers along the wheel perimeter.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 53,\n",
       "  'text_chunk': 'These degrees of freedom cause an accumulation of slippage, tend to reduce dead-reckoning accuracy and increase the design complexity. Controlling an omnidirectional robot for a specific direction of travel is also more diffi- cult and often less accurate when compared to less maneuverable designs. For example, an Ackerman steering vehicle can go straight simply by locking the steerable wheels and driv-ing the drive wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 53,\n",
       "  'text_chunk': 'In a differential-drive vehicle, the two motors attached to the two wheels must be driven along exactly the same velocity profile, which can be challenging considering variations between wheels, motors, and environmental differences. With four-wheel omnidrive, such as the Uranus robot, which has four Swedish wheels, the problem is even harder because all four wheels must be driven at exactly the same speed for the robot to travel in a perfectly straight line.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 53,\n",
       "  'text_chunk': 'In summary, there is no “ideal” drive configuration that simultaneously maximizes sta- bility, maneuverability, and controllability. Each mobile robot application places uniqueconstraints on the robot design problem, and the designer’s task is to choose the mostappropriate drive configuration possible from among this space of compromises. 2.3.2 Wheeled locomotion: case studies Below we describe four specific wheel configurations, in order to demonstrate concrete applications of the concepts discussed above to mobile robots built for real-world activities.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 53,\n",
       "  'text_chunk': '2.3.2.1 Synchro drive The synchro drive configuration (figure 2.22) is a popular arrangement of wheels in indoor mobile robot applications. It is an interesting configuration because, although there are three driven and steered wheels, only two motors are used in total. The one translationmotor sets the speed of all three wheels together, and the one steering motor spins all the wheels together about each of their individual vertical steering axes.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 53,\n",
       "  'text_chunk': 'But note that the wheels are being steered with respect to the robot chassis, and therefore there is no directway of reorienting the robot chassis. In fact, the chassis orientation does drift over time due to uneven tire slippage, causing rotational dead-reckoning error.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 54,\n",
       "  'text_chunk': 'Locomotion 39 Synchro drive is particularly advantageous in cases where omnidirectionality is sought. So long as each vertical steering axis is aligned with the contact path of each tire, the robot can always reorient its wheels and move along a new trajectory without changing its foot- print. Of course, if the robot chassis has directionality and the designers intend to reorientthe chassis purposefully, then synchro drive is only appropriate when combined with an independently rotating turret that attaches to the wheel chassis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 54,\n",
       "  'text_chunk': 'Commercial research robots such as the Nomadics 150 or the RWI B21r have been sold with this configuration(figure 1.12). In terms of dead reckoning, synchro drive systems are generally superior to true omni- directional configurations but inferior to differential-drive and Ackerman steering systems. There are two main reasons for this. First and foremost, the translation motor generally drives the three wheels using a single belt.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 54,\n",
       "  'text_chunk': 'Because of to slop and backlash in the drive train, whenever the drive motor engages, the closest wheel begins spinning before the fur-thest wheel, causing a small change in the orientation of the chassis. With additional changes in motor speed, these small angular shifts accumulate to create a large error in ori- entation during dead reckoning. Second, the mobile robot has no direct control over the ori-entation of the chassis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 54,\n",
       "  'text_chunk': 'Depending on the orientation of the chassis, the wheel thrust can be highly asymmetric, with two wheels on one side and the third wheel alone, or symmetric, with one wheel on each side and one wheel straight ahead or behind, as shown in figure2.22.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 54,\n",
       "  'text_chunk': 'The asymmetric cases result in a variety of errors when tire-ground slippage can occur, again causing errors in dead reckoning of robot orientation.Figure 2.22 Synchro drive: The robot can move in any direction; however, the orientation of the chassis is not controllable.wheel steeringbeltdrivebelt drive motorsteeringsteering pulley driving pulley wheel steering axis motor rolling axis'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 55,\n",
       "  'text_chunk': '40 Chapter 2 2.3.2.2 Omnidirectional drive As we will see later in section 3.4.2, omnidirectional movement is of great interest for com- plete maneuverability. Omnidirectional robots that are able to move in any direction() at any time are also holonomic (see section 3.4.2). They can be realized by either using spherical, castor, or Swedish wheels. Three examples of such holonomic robots arepresented below. Omnidirectional locomotion with three spherical wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 55,\n",
       "  'text_chunk': 'The omnidirectional robot depicted in figure 2.23 is based on three spherical wheels, each actuated by one motor. In this design, the spherical wheels are suspended by three contact points, two given by spher- ical bearings and one by a wheel connected to the motor axle. This concept provides excel- lent maneuverability and is simple in design. However, it is limited to flat surfaces andsmall loads, and it is quite difficult to find round wheels with high friction coefficients. Omnidirectional locomotion with four Swedish wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 55,\n",
       "  'text_chunk': 'The omnidirectional arrange- ment depicted in figure 2.24 has been used successfully on several research robots, includ- ing the Carnegie Mellon Uranus. This configuration consists of four Swedish 45-degree wheels, each driven by a separate motor. By varying the direction of rotation and relativespeeds of the four wheels, the robot can be moved along any trajectory in the plane and, even more impressively, can simultaneously spin around its vertical axis.xy θ,,Figure 2.23 The Tribolo designed at EPFL (Swiss Federal Institute of Technology, Lausanne, Switzerland.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 55,\n",
       "  'text_chunk': 'Left: arrangement of spheric bearings and motors (bottom view). Right: Picture of the robot without the spherical wheels (bottom view). spheric bearing motor'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 56,\n",
       "  'text_chunk': 'Locomotion 41 For example, when all four wheels spin “forward” or “backward” the robot as a whole moves in a straight line forward or backward, respectively. However, when one diagonal pair of wheels is spun in the same direction and the other diagonal pair is spun in the oppo-site direction, the robot moves laterally. This four-wheel arrangement of Swedish wheels is not minimal in terms of control motors.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 56,\n",
       "  'text_chunk': 'Because there are only three degrees of freedom in the plane, one can build a three-wheel omnidirectional robot chassis using three Swedish 90-degree wheels as shown in table 2.1. However, existing examples such as Uranus have been designed with four wheels owing to capacity and stability considerations. One application for which such omnidirectional designs are particularly amenable is mobile manipulation. In this case, it is desirable to reduce the degrees of freedom of themanipulator arm to save arm mass by using the mobile robot chassis motion for grossmotion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 56,\n",
       "  'text_chunk': 'As with humans, it would be ideal if the base could move omnidirectionally with- out greatly impacting the position of the manipulator tip, and a base such as Uranus can afford precisely such capabilities. Omnidirectional locomotion with four castor wheels and eight motors. Another solu- tion for omnidirectionality is to use castor wheels. This is done for the Nomad XR4000 from Nomadic Technologies (fig. 2.25), giving it excellent maneuverability. Unfortu- nately, Nomadic has ceased production of mobile robots.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 56,\n",
       "  'text_chunk': 'The above three examples are drawn from table 2.1, but this is not an exhaustive list of all wheeled locomotion techniques. Hybrid approaches that combine legged and wheeledlocomotion, or tracked and wheeled locomotion, can also offer particular advantages. Below are two unique designs created for specialized applications.Figure 2.24 The Carnegie Mellon Uranus robot, an omnidirectional robot with four powered-swedish 45 wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 57,\n",
       "  'text_chunk': '42 Chapter 2 2.3.2.3 Tracked slip/skid locomotion In the wheel configurations discussed above, we have made the assumption that wheels arenot allowed to skid against the surface. An alternative form of steering, termed slip/skid, may be used to reorient the robot by spinning wheels that are facing the same direction at different speeds or in opposite directions. The army tank operates this way, and theNanokhod (figure 2.26) is an example of a mobile robot based on the same concept.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 57,\n",
       "  'text_chunk': 'Robots that make use of tread have much larger ground contact patches, and this can sig- nificantly improve their maneuverability in loose terrain compared to conventional wheeled designs. However, due to this large ground contact patch, changing the orientation of the robot usually requires a skidding turn, wherein a large portion of the track must slide against the terrain. The disadvantage of such configurations is coupled to the slip/skid steering.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 57,\n",
       "  'text_chunk': 'Because of the large amount of skidding during a turn, the exact center of rotation of the robot is hardto predict and the exact change in position and orientation is also subject to variationsdepending on the ground friction. Therefore, dead reckoning on such robots is highly inac- curate. This is the trade-off that is made in return for extremely good maneuverability and traction over rough and loose terrain. Furthermore, a slip/skid approach on a high-frictionsurface can quickly overcome the torque capabilities of the motors being used.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 57,\n",
       "  'text_chunk': 'In terms of power efficiency, this approach is reasonably efficient on loose terrain but extremely inef- ficient otherwise.Figure 2.25 The Nomad XR4000 from Nomadic Technologies had an arrangement of four castor wheels for holo- nomic motion. All the castor wheels are driven and steered, thus requiring a precise synchronization and coordination to obtain a precise movement in and . xy,θ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 58,\n",
       "  'text_chunk': 'Locomotion 43 2.3.2.4 Walking wheels Walking robots might offer the best maneuverability in rough terrain. However, they are inefficient on flat ground and need sophisticated control. Hybrid solutions, combining the adaptability of legs with the efficiency of wheels, offer an interesting compromise. Solu-tions that passively adapt to the terrain are of particular interest for field and space robotics. The Sojourner robot of NASA/JPL (see figure 1.2) represents such a hybrid solution, able to overcome objects up to the size of the wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 58,\n",
       "  'text_chunk': 'A more recent mobile robot design forsimilar applications has recently been produced by EPFL (figure 2.27). This robot, called Shrimp, has six motorized wheels and is capable of climbing objects up to two times its wheel diameter [97, 133]. This enables it to climb regular stairs though the robot is evensmaller than the Sojourner. Using a rhombus configuration, the Shrimp has a steering wheel in the front and the rear, and two wheels arranged on a bogie on each side.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 58,\n",
       "  'text_chunk': 'The front wheel has a spring suspension to guarantee optimal ground contact of all wheels at any time. Thesteering of the rover is realized by synchronizing the steering of the front and rear wheels and the speed difference of the bogie wheels. This allows for high-precision maneuvers and turning on the spot with minimum slip/skid of the four center wheels. The use of parallel articulations for the front wheel and the bogies creates a virtual center of rotation at the level of the wheel axis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 58,\n",
       "  'text_chunk': 'This ensures maximum stability and climbing abilities even for verylow friction coefficients between the wheel and the ground.Figure 2.26 The microrover Nanokhod, developed by von Hoerner & Sulger GmbH and the Max Planck Institute, Mainz, for the European Space Agency (ESA), will probably go to Mars [138, 154].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 59,\n",
       "  'text_chunk': '44 Chapter 2 The climbing ability of the Shrimp is extraordinary in comparison to most robots of sim- ilar mechanical complexity, owing much to the specific geometry and thereby the manner in which the center of mass (COM) of the robot shifts with respect to the wheels over time. In contrast, the Personal Rover demonstrates active COM shifting to climb ledges that arealso several times the diameter of its wheels, as demonstrated in figure 2.28.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 59,\n",
       "  'text_chunk': 'A majority of the weight of the Personal Rover is borne at the upper end of its swinging boom. A dedi- cated motor drives the boom to change the front/rear weight distribution in order to facili-tate step-climbing. Because this COM-shifting scheme is active, a control loop must explicitly decide how to move the boom during a climbing scenario. In this case the Per- sonal Rover accomplished this closed-loop control by inferring terrain based on measure-ments of current flowing to each independently driven wheel [66].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 59,\n",
       "  'text_chunk': 'As mobile robotics research matures we find ourselves able to design more intricate mechanical systems. At the same time, the control problems of inverse kinematics anddynamics are now so readily conquered that these complex mechanics can in general be controlled. So, in the near future, we can expect to see a great number of unique, hybrid mobile robots that draw together advantages from several of the underlying locomotion mechanisms that we have discussed in this chapter.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 59,\n",
       "  'text_chunk': 'They will each be technologically impressive, and each will be designed as the expert robot for its particular environmental niche.Figure 2.27 Shrimp, an all-terrain robot with outstanding passive climbing abilities (EPFL [97, 133]).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 60,\n",
       "  'text_chunk': 'Locomotion 45 Figure 2.28 The Personal Rover, demonstrating ledge climbing using active center-of-mass shifting.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 62,\n",
       "  'text_chunk': '3 Mobile Robot Kinematics 3.1 Introduction Kinematics is the most basic study of how mechanical systems behave. In mobile robotics, we need to understand the mechanical behavior of the robot both in order to design appro-priate mobile robots for tasks and to understand how to create control software for an instance of mobile robot hardware. Of course, mobile robots are not the first complex mechanical systems to require such analysis. Robot manipulators have been the subject of intensive study for more than thirty years.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 62,\n",
       "  'text_chunk': 'In some ways, manipulator robots are much more complex than early mobile robots: a standard welding robot may have five or more joints, whereas early mobile robots weresimple differential-drive machines. In recent years, the robotics community has achieved a fairly complete understanding of the kinematics and even the dynamics (i.e., relating to force and mass) of robot manipulators [11, 32]. The mobile robotics community poses many of the same kinematic questions as the robot manipulator community.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 62,\n",
       "  'text_chunk': 'A manipulator robot’s workspace is crucial because it defines the range of possible positions that can be achieved by its end effector relative toits fixture to the environment. A mobile robot’s workspace is equally important because it defines the range of possible poses that the mobile robot can achieve in its environment. The robot arm’s controllability defines the manner in which active engagement of motorscan be used to move from pose to pose in the workspace.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 62,\n",
       "  'text_chunk': 'Similarly, a mobile robot’s con- trollability defines possible paths and trajectories in its workspace. Robot dynamics places additional constraints on workspace and trajectory due to mass and force considerations.The mobile robot is also limited by dynamics; for instance, a high center of gravity limits the practical turning radius of a fast, car-like robot because of the danger of rolling. But the chief difference between a mobile robot and a manipulator arm also introduces a significant challenge for position estimation . A manipulator has one end fixed to the envi- ronment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 62,\n",
       "  'text_chunk': 'Measuring the position of an arm’s end effector is simply a matter of understand- ing the kinematics of the robot and measuring the position of all intermediate joints. Themanipulator’s position is thus always computable by looking at current sensor data. But a'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 63,\n",
       "  'text_chunk': '48 Chapter 3 mobile robot is a self-contained automaton that can wholly move with respect to its envi- ronment. There is no direct way to measure a mobile robot’s position instantaneously. Instead, one must integrate the motion of the robot over time. Add to this the inaccuracies ofmotion estimation due to slippage and it is clear that measuring a mobile robot’s position precisely is an extremely challenging task.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 63,\n",
       "  'text_chunk': 'The process of understanding the motions of a robot begins with the process of describ- ing the contribution each wheel provides for motion. Each wheel has a role in enabling thewhole robot to move. By the same token, each wheel also imposes constraints on the robot’s motion; for example, refusing to skid laterally. In the following section, we intro- duce notation that allows expression of robot motion in a global reference frame as well asthe robot’s local reference frame.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 63,\n",
       "  'text_chunk': 'Then, using this notation, we demonstrate the construc- tion of simple forward kinematic models of motion, describing how the robot as a whole moves as a function of its geometry and individual wheel behavior. Next, we formallydescribe the kinematic constraints of individual wheels, and then combine these kinematic constraints to express the whole robot’s kinematic constraints. With these tools, one can evaluate the paths and trajectories that define the robot’s maneuverability.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 63,\n",
       "  'text_chunk': '3.2 Kinematic Models and Constraints Deriving a model for the whole robot’s motion is a bottom-up process. Each individual wheel contributes to the robot’s motion and, at the same time, imposes constraints on robot motion. Wheels are tied together based on robot chassis geometry, and therefore their con-straints combine to form constraints on the overall motion of the robot chassis. But the forces and constraints of each wheel must be expressed with respect to a clear and consis- tent reference frame.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 63,\n",
       "  'text_chunk': 'This is particularly important in mobile robotics because of its self-contained and mobile nature; a clear mapping between global and local frames of reference is required. We begin by defining these reference frames formally, then using the resulting formalism to annotate the kinematics of individual wheels and whole robots. Throughoutthis process we draw extensively on the notation and terminology presented in [52]. 3.2.1 Representing robot position Throughout this analysis we model the robot as a rigid body on wheels, operating on a hor- izontal plane.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 63,\n",
       "  'text_chunk': 'The total dimensionality of this robot chassis on the plane is three, two for position in the plane and one for orientation along the vertical axis, which is orthogonal tothe plane. Of course, there are additional degrees of freedom and flexibility due to the wheel axles, wheel steering joints, and wheel castor joints. However by robot chassis we refer only to the rigid body of the robot, ignoring the joints and degrees of freedom internal to the robot and its wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 64,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 49 In order to specify the position of the robot on the plane we establish a relationship between the global reference frame of the plane and the local reference frame of the robot, as in figure 3.1. The axes and define an arbitrary inertial basis on the plane as theglobal reference frame from some origin O: . To specify the position of the robot, choose a point P on the robot chassis as its position reference point.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 64,\n",
       "  'text_chunk': 'The basis defines two axes relative to P on the robot chassis and is thus the robot’s local reference frame. The position of P in the global reference frame is specified by coordinates x and y, and the angular difference between the global and local reference frames is given by . Wecan describe the pose of the robot as a vector with these three elements.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 64,\n",
       "  'text_chunk': 'Note the use of thesubscript I to clarify the basis of this pose as the global reference frame: (3.1) To describe robot motion in terms of component motions, it will be necessary to map motion along the axes of the global reference frame to motion along the axes of the robot’slocal reference frame.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 64,\n",
       "  'text_chunk': 'Of course, the mapping is a function of the current pose of the robot.This mapping is accomplished using the orthogonal rotation matrix :Figure 3.1 The global reference frame and the robot local reference frame.PYR XR θYI XI XI YI XIYI,{} XRYR,{} θ ξIx y θ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 65,\n",
       "  'text_chunk': '50 Chapter 3 (3.2) This matrix can be used to map motion in the global reference frame to motion in terms of the local reference frame . This operation is denoted by because the computation of this operation depends on the value of : (3.3) For example, consider the robot in figure 3.2.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 65,\n",
       "  'text_chunk': 'For this robot, because we can easily compute the instantaneous rotation matrix R: (3.4)Rθ()θcos θsin 0 θsin– θcos 0 00 1= XIYI,{} XRYR,{} Rθ()ξ· I θ ξR·Rπ 2---()ξI·=Figure 3.2 The mobile robot aligned with a global axis.YRXRYI XIθ θπ 2---= Rπ 2---()01 0 1–0 0 00 1='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 66,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 51 Given some velocity ( ) in the global reference frame we can compute the components of motion along this robot’s local axes and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 66,\n",
       "  'text_chunk': 'In this case, due to the spe- cific angle of the robot, motion along is equal to and motion along is : (3.5) 3.2.2 Forward kinematic models In the simplest cases, the mapping described by equation (3.3) is sufficient to generate a formula that captures the forward kinematics of the mobile robot: how does the robot move, given its geometry and the speeds of its wheels? More formally, consider the example shown in figure 3.3. This differential drive robot has two wheels, each with diameter .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 66,\n",
       "  'text_chunk': 'Given a point cen- tered between the two drive wheels, each wheel is a distance from . Given , , , andthe spinning speed of each wheel, and , a forward kinematic model would predict the robot’s overall speed in the global reference frame: (3.6) From equation (3.3) we know that we can compute the robot’s motion in the global ref- erence frame from motion in its local reference frame: .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 66,\n",
       "  'text_chunk': 'Therefore, the strat- egy will be to first compute the contribution of each of the two wheels in the local referencex·y·θ·,, XR YR XR y· YR x·– ξR·Rπ 2---()ξI·01 0 1–0 0 00 1x· y· θ·y· x·– θ·== =Figure 3.3 A differential-drive robot in its global reference frame.v(t) ω(t)θYI XIcastor wheel r P l P rlθ ϕ·1 ϕ·2 ξI·x· y· θ·flr θϕ·1ϕ·2 ,, , ,() == ξI·Rθ()1–ξR·='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 67,\n",
       "  'text_chunk': '52 Chapter 3 frame, . For this example of a differential-drive chassis, this problem is particularly straightforward. Suppose that the robot’s local reference frame is aligned such that the robot moves for- ward along , as shown in figure 3.1. First consider the contribution of each wheel’sspinning speed to the translation speed at Pin the direction of . If one wheel spins while the other wheel contributes nothing and is stationary, since P is halfway between the two wheels, it will move instantaneously with half the speed: and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 67,\n",
       "  'text_chunk': 'In a differential drive robot, these two contributions can simply be added to calculate the component of . Consider, for example, a differential robot in whicheach wheel spins with equal speed but in opposite directions. The result is a stationary,spinning robot. As expected, will be zero in this case. The value of is even simpler to calculate. Neither wheel can contribute to sideways motion in the robot’s reference frame, and so is always zero. Finally, we must compute the rotational component of .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 67,\n",
       "  'text_chunk': 'Once again, the contributions of each wheel can be computed independently and just added. Consider the right wheel (we will call this wheel 1). Forward spin of this wheelresults in counterclockwise rotation at point . Recall that if wheel 1 spins alone, the robot pivots around wheel 2.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 67,\n",
       "  'text_chunk': 'The rotation velocity at can be computed because the wheel is instantaneously moving along the arc of a circle of radius : (3.7) The same calculation applies to the left wheel, with the exception that forward spin results in clockwise rotation at point : (3.8) Combining these individual formulas yields a kinematic model for the differential-drive example robot: (3.9)ξ· R +XR +XR xr1·12⁄() rϕ·1 = xr2·12⁄() rϕ·2 = xR·ξ· R xR·yR· yR·θR· ξ· R P ω1 P 2l ω1rϕ·1 2l--------= P ω2r–ϕ·2 2l-----------= ξI·Rθ()1–rϕ·1 2--------rϕ·2 2--------+ 0 rϕ·1 2l--------r–ϕ·2 2l-----------+='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 68,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 53 We can now use this kinematic model in an example. However, we must first compute . In general, calculating the inverse of a matrix may be challenging. In this case, however, it is easy because it is simply a transform from to rather than vice versa: (3.10) Suppose that the robot is positioned such that , , and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 68,\n",
       "  'text_chunk': 'If the robot engages its wheels unevenly, with speeds and , we can compute its veloc- ity in the global reference frame: (3.11) So this robot will move instantaneously along the y-axis of the global reference frame with speed 3 while rotating with speed 1. This approach to kinematic modeling can provide information about the motion of a robot given its component wheel speeds in straightfor- ward cases. However, we wish to determine the space of possible motions for each robot chassis design.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 68,\n",
       "  'text_chunk': 'To do this, we must go further, describing formally the constraints on robotmotion imposed by each wheel. Section 3.2.3 begins this process by describing constraints for various wheel types; the rest of this chapter provides tools for analyzing the character- istics and workspace of a robot given these constraints. 3.2.3 Wheel kinematic constraints The first step to a kinematic model of the robot is to express constraints on the motions of individual wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 68,\n",
       "  'text_chunk': 'Just as shown in section 3.2.2, the motions of individual wheels can later be combined to compute the motion of the robot as a whole. As discussed in chapter 2, there are four basic wheel types with widely varying kinematic properties. Therefore, we beginby presenting sets of constraints specific to each wheel type. However, several important assumptions will simplify this presentation. We assume that the plane of the wheel always remains vertical and that there is in all cases one single pointof contact between the wheel and the ground plane.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 68,\n",
       "  'text_chunk': 'Furthermore, we assume that there is no sliding at this single point of contact. That is, the wheel undergoes motion only under conditions of pure rolling and rotation about the vertical axis through the contact point. For a more thorough treatment of kinematics, including sliding contact, refer to [25].Rθ() 1– ξ· R ξ· I Rθ()1–θcos θsin–0 θsin θcos 0 00 1= θπ 2⁄= r 1= l1= ϕ·1 4= ϕ·2 2= ξI·x· y· θ·01–0 100 0013 0 10 3 1= =='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 69,\n",
       "  'text_chunk': '54 Chapter 3 Under these assumptions, we present two constraints for every wheel type. The first con- straint enforces the concept of rolling contact – that the wheel must roll when motion takes place in the appropriate direction. The second constraint enforces the concept of no lateralslippage – that the wheel must not slide orthogonal to the wheel plane. 3.2.3.1 Fixed standard wheel The fixed standard wheel has no vertical axis of rotation for steering.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 69,\n",
       "  'text_chunk': 'Its angle to the chassisis thus fixed, and it is limited to motion back and forth along the wheel plane and rotation around its contact point with the ground plane. Figure 3.4 depicts a fixed standard wheel and indicates its position pose relative to the robot’s local reference frame . Theposition of is expressed in polar coordinates by distance and angle . The angle of the wheel plane relative to the chassis is denoted by , which is fixed since the fixed standard wheel is not steerable.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 69,\n",
       "  'text_chunk': 'The wheel, which has radius , can spin over time, and so its rota-tional position around its horizontal axle is a function of time : .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 69,\n",
       "  'text_chunk': 'The rolling constraint for this wheel enforces that all motion along the direction of the wheel plane must be accompanied by the appropriate amount of wheel spin so that there ispure rolling at the contact point: (3.12)Figure 3.4 A fixed standard wheel and its parameters.YR XRAβ αl Pvϕ, rRobot chassis A XRYR,{} Al α β r tϕt() αβ+()sin αβ+()cos– l–() βcos Rθ()ξI·rϕ·–0 ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 70,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 55 The first term of the sum denotes the total motion along the wheel plane. The three ele- ments of the vector on the left represent mappings from each of to their contri- butions for motion along the wheel plane. Note that the term is used to transform the motion parameters that are in the global reference frame into motionparameters in the local reference frame as shown in example equation (3.5).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 70,\n",
       "  'text_chunk': 'This is necessary because all other parameters in the equation, , are in terms of the robot’s local reference frame. This motion along the wheel plane must be equal, according to thisconstraint, to the motion accomplished by spinning the wheel, . The sliding constraint for this wheel enforces that the component of the wheel’s motion orthogonal to the wheel plane must be zero: (3.13) For example, suppose that wheel is in a position such that .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 70,\n",
       "  'text_chunk': 'This would place the contact point of the wheel on with the plane of the wheel oriented par-allel to . If , then the sliding constraint [equation (3.13)] reduces to (3.14) This constrains the component of motion along to be zero and since and are parallel in this example, the wheel is constrained from sliding sideways, as expected.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 70,\n",
       "  'text_chunk': '3.2.3.2 Steered standard wheel The steered standard wheel differs from the fixed standard wheel only in that there is anadditional degree of freedom: the wheel may rotate around a vertical axis passing through the center of the wheel and the ground contact point. The equations of position for the steered standard wheel (figure 3.5) are identical to that of the fixed standard wheel shownin figure 3.4 with one exception.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 70,\n",
       "  'text_chunk': 'The orientation of the wheel to the robot chassis is no longer a single fixed value, , but instead varies as a function of time: .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 70,\n",
       "  'text_chunk': 'The rolling and sliding constraints are (3.15) (3.16)x·y·θ·,, Rθ()ξ I· ξI·XIYI,{} XRYR,{} αβl,, rϕ· αβ+()cos αβ+()sin lβsin Rθ()ξI·0= A α0=() β0=(), {} XI YI θ 0= 100100 010 001x· y· θ·100x· y· θ·0 == XI XI XR β βt() αβ+()sin αβ+()cos– l–() βcos Rθ()ξI·rϕ·–0 = αβ+()cos αβ+()sin lβsin Rθ()ξ· I0='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 71,\n",
       "  'text_chunk': '56 Chapter 3 These constraints are identical to those of the fixed standard wheel because, unlike , does not have a direct impact on the instantaneous motion constraints of a robot. It is only by integrating over time that changes in steering angle can affect the mobility of a vehicle. This may seem subtle, but is a very important distinction between change in steer- ing position, , and change in wheel spin, . 3.2.3.3 Castor wheel Castor wheels are able to steer around a vertical axis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 71,\n",
       "  'text_chunk': 'However, unlike the steered standard wheel, the vertical axis of rotation in a castor wheel does not pass through the ground con-tact point. Figure 3.6 depicts a castor wheel, demonstrating that formal specification of the castor wheel’s position requires an additional parameter.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 71,\n",
       "  'text_chunk': 'The wheel contact point is now at position , which is connected by a rigid rod of fixed length to point fixes the location of the vertical axis about which steers, and this point has a position specified in the robot’s reference frame, as in figure 3.6. We assume that the plane of the wheel is aligned with at all times. Similar to the steered standard wheel, the castor wheel has two parameters that vary as a function of time. represents the wheel spin over time as before.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 71,\n",
       "  'text_chunk': 'denotes the steering angle and orienta-tion of over time. For the castor wheel, the rolling constraint is identical to equation (3.15) because the offset axis plays no role during motion that is aligned with the wheel plane:Figure 3.5 A steered standard wheel and its parameters.YR XRAβ(t) αl Pvϕ, rRobot chassis ϕ· β· β·ϕ· B AB dA B A AB ϕt() βt() AB'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 72,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 57 (3.17) The castor geometry does, however, have significant impact on the sliding constraint. The critical issue is that the lateral force on the wheel occurs at point because this is the attachment point of the wheel to the chassis. Because of the offset ground contact point rel- ative to , the constraint that there be zero lateral movement would be wrong.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 72,\n",
       "  'text_chunk': 'Instead, the constraint is much like a rolling constraint, in that appropriate rotation of the vertical axis must take place: (3.18) In equation (3.18), any motion orthogonal to the wheel plane must be balanced by an equivalent and opposite amount of castor steering motion. This result is critical to the suc-cess of castor wheels because by setting the value of any arbitrary lateral motion can be acceptable. In a steered standard wheel, the steering action does not by itself cause a move- ment of the robot chassis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 72,\n",
       "  'text_chunk': 'But in a castor wheel the steering action itself moves the robotchassis because of the offset between the ground contact point and the vertical axis of rota- tion.Figure 3.6 A castor wheel and its parameters.YR XRAβ(t) αl Pvϕ, r ddB Robot chassis αβ+()sin αβ+()cos– l–() βcos Rθ()ξI·rϕ·–0 = A A αβ+()cos αβ+()sin dl βsin+ Rθ()ξI·dβ·+0 = β·'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 73,\n",
       "  'text_chunk': '58 Chapter 3 More concisely, it can be surmised from equations (3.17) and (3.18) that, given any robot chassis motion , there exists some value for spin speed and steering speed such that the constraints are met. Therefore, a robot with only castor wheels can move with any velocity in the space of possible robot motions. We term such systems omnidirectional . A real-world example of such a system is the five-castor wheel office chair shown in figure 3.7.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 73,\n",
       "  'text_chunk': 'Assuming that all joints are able to move freely, you may select any motionvector on the plane for the chair and push it by hand. Its castor wheels will spin and steeras needed to achieve that motion without contact point sliding. By the same token, if each of the chair’s castor wheels housed two motors, one for spinning and one for steering, then a control system would be able to move the chair along any trajectory in the plane.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 73,\n",
       "  'text_chunk': 'Thus,although the kinematics of castor wheels is somewhat complex, such wheels do not impose any real constraints on the kinematics of a robot chassis. 3.2.3.4 Swedish wheel Swedish wheels have no vertical axis of rotation, yet are able to move omnidirectionally like the castor wheel. This is possible by adding a degree of freedom to the fixed standard wheel. Swedish wheels consist of a fixed standard wheel with rollers attached to the wheel perimeter with axes that are antiparallel to the main axis of the fixed wheel component.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 73,\n",
       "  'text_chunk': 'The exact angle between the roller axes and the main axis can vary, as shown in figure 3.8. For example, given a Swedish 45-degree wheel, the motion vectors of the principal axis and the roller axes can be drawn as in figure 3.8. Since each axis can spin clockwise or counterclockwise, one can combine any vector along one axis with any vector along theother axis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 73,\n",
       "  'text_chunk': 'These two axes are not necessarily independent (except in the case of the Swed- ish 90-degree wheel); however, it is visually clear that any desired direction of motion is achievable by choosing the appropriate two vectors.ξ· I ϕ· β·Figure 3.7 Office chair with five castor wheels. γ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 74,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 59 The pose of a Swedish wheel is expressed exactly as in a fixed standard wheel, with the addition of a term, , representing the angle between the main wheel plane and the axis of rotation of the small circumferential rollers. This is depicted in figure 3.8 within the robot’s reference frame. Formulating the constraint for a Swedish wheel requires some subtlety. The instanta- neous constraint is due to the specific orientation of the small rollers.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 74,\n",
       "  'text_chunk': 'The axis around which these rollers spin is a zero component of velocity at the contact point. That is, moving in that direction without spinning the main axis is not possible without sliding.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 74,\n",
       "  'text_chunk': 'The motionconstraint that is derived looks identical to the rolling constraint for the fixed standard wheel in equation (3.12) except that the formula is modified by adding such that the effective direction along which the rolling constraint holds is along this zero componentrather than along the wheel plane: (3.19) Orthogonal to this direction the motion is not constrained because of the free rotation of the small rollers.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 74,\n",
       "  'text_chunk': '(3.20) Figure 3.8 A Swedish wheel and its parameters.YR XRAβ αl Pϕ, rγ Robot chassis γ γ αβγ++()sin αβγ++()cos– l–() βγ +()cos Rθ()ξI·rϕ· γcos –0 = ϕ·sw αβγ++()cos αβγ++()sin l βγ+()sin Rθ()ξI·rϕ·γsin rswϕ·sw – –0 ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 75,\n",
       "  'text_chunk': '60 Chapter 3 The behavior of this constraint and thereby the Swedish wheel changes dramatically as the value varies. Consider . This represents the swedish 90-degree wheel. In this case, the zero component of velocity is in line with the wheel plane and so equation (3.19)reduces exactly to equation (3.12), the fixed standard wheel rolling constraint. But because of the rollers, there is no sliding constraint orthogonal to the wheel plane [see equation (3.20)].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 75,\n",
       "  'text_chunk': 'By varying the value of , any desired motion vector can be made to satisfy equa-tion (3.19) and therefore the wheel is omnidirectional. In fact, this special case of the Swed- ish design results in fully decoupled motion, in that the rollers and the main wheel provide orthogonal directions of motion. At the other extreme, consider . In this case, the rollers have axes of rotation that are parallel to the main wheel axis of rotation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 75,\n",
       "  'text_chunk': 'Interestingly, if this value is substitutedfor in equation (3.19) the result is the fixed standard wheel sliding constraint, equation(3.13). In other words, the rollers provide no benefit in terms of lateral freedom of motion since they are simply aligned with the main wheel. However, in this case the main wheel never needs to spin and therefore the rolling constraint disappears. This is a degenerate form of the Swedish wheel and therefore we assume in the remainder of this chapter that .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 75,\n",
       "  'text_chunk': '3.2.3.5 Spherical wheelThe final wheel type, a ball or spherical wheel, places no direct constraints on motion (fig- ure 3.9). Such a mechanism has no principal axis of rotation, and therefore no appropriate rolling or sliding constraints exist. As with castor wheels and Swedish wheels, the sphericalγ γ 0= ϕ· γ π2⁄= γ γπ2⁄≠Figure 3.9 A spherical wheel and its parameters.YR XRA αl Pϕ, rβ vARobot chassis'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 76,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 61 wheel is clearly omnidirectional and places no constraints on the robot chassis kinematics. Therefore equation (3.21) simply describes the roll rate of the ball in the direction of motion of point of the robot. (3.21) By definition the wheel rotation orthogonal to this direction is zero. (3.22) As can be seen, the equations for the spherical wheel are exactly the same as for the fixed standard wheel. However, the interpretation of equation (3.22) is different.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 76,\n",
       "  'text_chunk': 'The omnidirec-tional spherical wheel can have any arbitrary direction of movement, where the motion direction given by is a free variable deduced from equation (3.22). Consider the case that the robot is in pure translation in the direction of . Then equation (3.22) reduces to , thus , which makes sense for this special case. 3.2.4 Robot kinematic constraintsGiven a mobile robot with wheels we can now compute the kinematic constraints of the robot chassis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 76,\n",
       "  'text_chunk': 'The key idea is that each wheel imposes zero or more constraints on robot motion, and so the process is simply one of appropriately combining all of the kinematicconstraints arising from all of the wheels based on the placement of those wheels on the robot chassis. We have categorized all wheels into five categories: (1) fixed and (2)steerable standard wheels, (3) castor wheels, (4) Swedish wheels, and (5) spherical wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 76,\n",
       "  'text_chunk': 'But note from the wheel kinematic constraints in equations (3.17), (3.18), and (3.19) that the castor wheel,Swedish wheel, and spherical wheel impose no kinematic constraints on the robot chassis, since can range freely in all of these cases owing to the internal wheel degrees of free-dom. Therefore only fixed standard wheels and steerable standard wheels have impact on robot chassis kinematics and therefore require consideration when computing the robot’skinematic constraints.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 76,\n",
       "  'text_chunk': 'Suppose that the robot has a total of standard wheels, comprising fixed standard wheels and steerable standard wheels. We use to denote the variable steering angles of the steerable standard wheels. In contrast, refers to theorientation of the fixed standard wheels as depicted in figure 3.4. In the case of wheelspin, both the fixed and steerable wheels have rotational positions around the horizontal axle that vary as a function of time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 76,\n",
       "  'text_chunk': 'We denote the fixed and steerable cases separately as and , and use as an aggregate matrix that combines both values:v A A αβ+()sin αβ+()cos– l–() βcos Rθ()ξI·rϕ·–0 = αβ+()cos αβ+()sin lβsin Rθ()ξ· I0= β YR αβ+()sin 0= βα –= M ξI· N Nf Ns βst() Ns βf Nf ϕft() ϕst() ϕt()'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 77,\n",
       "  'text_chunk': '62 Chapter 3 (3.23) The rolling constraints of all wheels can now be collected in a single expression: (3.24) This expression bears a strong resemblance to the rolling constraint of a single wheel, but substitutes matrices in lieu of single values, thus taking into account all wheels. is a constant diagonal matrix whose entries are radii of all standard wheels. denotes a matrix with projections for all wheels to their motions along their individualwheel planes: (3.25) Note that is only a function of and not .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 77,\n",
       "  'text_chunk': 'This is because the orientations of steerable standard wheels vary as a function of time, whereas the orientations of fixed stan-dard wheels are constant. is therefore a constant matrix of projections for all fixed stan-dard wheels. It has size ( ), with each row consisting of the three terms in the three- matrix from equation (3.12) for each fixed standard wheel. is a matrix of size ( ), with each row consisting of the three terms in the three-matrix from equation (3.15) for each steerable standard wheel.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 77,\n",
       "  'text_chunk': 'In summary, equation (3.24) represents the constraint that all standard wheels must spin around their horizontal axis an appropriate amount based on their motions along the wheelplane so that rolling occurs at the ground contact point.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 77,\n",
       "  'text_chunk': 'We use the same technique to collect the sliding constraints of all standard wheels into a single expression with the same structure as equations (3.13) and (3.16): (3.26) (3.27) and are ( ) and ( ) matrices whose rows are the three terms in the three-matrix of equations (3.13) and (3.16) for all fixed and steerable standard wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 77,\n",
       "  'text_chunk': 'Thusϕt()ϕ ft() ϕst()= J1βs()Rθ()ξI·J2ϕ·–0 = J2 NN× r J1βs() J1βs()J1f J1sβs()= J1βs() βs βf J1f Nf3× J1sβs() Ns3× C1βs()Rθ()ξI·0= C1βs()C1f C1sβs()= C1f C1s Nf3× Ns3×'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 78,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 63 equation (3.26) is a constraint over all standard wheels that their components of motion orthogonal to their wheel planes must be zero. This sliding constraint over all standard wheels has the most significant impact on defining the overall maneuverability of the robot chassis, as explained in the next section. 3.2.5 Examples: robot kinematic models and constraints In section 3.2.2 we presented a forward kinematic solution for in the case of a simpledifferential-drive robot by combining each wheel’s contribution to robot motion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 78,\n",
       "  'text_chunk': 'We can now use the tools presented above to construct the same kinematic expression by direct application of the rolling constraints for every wheel type. We proceed with this techniqueapplied again to the differential drive robot, enabling verification of the method as com- pared to the results of section 3.2.2. Then we proceed to the case of the three-wheeled omni- directional robot. 3.2.5.1 A differential-drive robot example First, refer to equations (3.24) and (3.26).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 78,\n",
       "  'text_chunk': 'These equations relate robot motion to the rolling and sliding constraints and , and the wheel spin speed of the robot’s wheels, . Fusing these two equations yields the following expression: (3.28) Once again, consider the differential drive robot in figure 3.3. We will construct and directly from the rolling constraints of each wheel. The castor is unpowered and is free to move in any direction, so we ignore this third point of contact altogether.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 78,\n",
       "  'text_chunk': 'The two remaining drive wheels are not steerable, and therefore and simplify to and respectively. To employ the fixed standard wheel’s rolling constraint formula, equation (3.12), we must first identify each wheel’s values for and . Suppose that the robot’s local reference frame is aligned such that the robot moves forward along , asshown in figure 3.1. In this case, for the right wheel , , and for the left wheel, , .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 78,\n",
       "  'text_chunk': 'Note the value of for the right wheel is necessary to ensure that positive spin causes motion in the direction (figure 3.4). Now we can computethe and matrix using the matrix terms from equations (3.12) and (3.13).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 78,\n",
       "  'text_chunk': 'Because the two fixed standard wheels are parallel, equation (3.13) results in only one independent equation, and equation (3.28) givesξ I· J1βs() C1βs() ϕ· J1βs() C1βs()Rθ() ξI· J2ϕ 0= J1βs() C1βs() J1βs() C1βs() J1f C1f α β +XR απ 2⁄–= βπ= απ 2⁄= β 0= β +XR J1f C1f'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 79,\n",
       "  'text_chunk': '64 Chapter 3 (3.29) Inverting equation (3.29) yields the kinematic equation specific to our differential drive robot: (3.30) This demonstrates that, for the simple differential-drive case, the combination of wheel rolling and sliding constraints describes the kinematic behavior, based on our manual cal- culation in section 3.2.2. 3.2.5.2 An omnidirectional robot example Consider the omniwheel robot shown in figure 3.10. This robot has three Swedish 90- degree wheels, arranged radially symmetrically, with the rollers perpendicular to each main wheel.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 79,\n",
       "  'text_chunk': 'First we must impose a specific local reference frame upon the robot. We do so by choosing point at the center of the robot, then aligning the robot with the local reference10 l 10 l– 010Rθ() ξI· J2ϕ 0= ξI·Rθ()1–10 l 10 l– 01 01– J2ϕ 0Rθ()1–1 2---1 2---0 00 1 1 2l-----1 2l-----–0J2ϕ 0==Figure 3.10 A three-wheel omnidrive robot developed by Carnegie Mellon University (www.cs.cmu.edu/~pprk). v(t) ω(t) θYI XI P'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 80,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 65 frame such that is colinear with the axis of wheel 2. Figure 3.11 shows the robot and its local reference frame arranged in this manner. We assume that the distance between each wheel and is , and that all three wheels have the same radius, . Once again, the value of can be computed as a combination of the rolling constraints of the robot’s three omnidirectional wheels, as in equation (3.28).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 80,\n",
       "  'text_chunk': 'As with the differential- drive robot, since this robot has no steerable wheels, simplifiesto : (3.31) We calculate using the matrix elements of the rolling constraints for the Swedish wheel, given by equation (3.19). But to use these values, we must establish the values for each wheel. Referring to figure (3.8), we can see that for the Swedish 90- degree wheel. Note that this immediately simplifies equation (3.19) to equation (3.12), therolling constraints of a fixed standard wheel.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 80,\n",
       "  'text_chunk': 'Given our particular placement of the local reference frame, the value of for each wheel is easily computed: . Furthermore, for all wheels because the wheels are tangent to the robot’s circular body. Constructing and simplifying usingequation (3.12) yieldsFigure 3.11 The local reference frame plus detailed parameters for wheel 1.YR XR1 2 rϕ⋅1ω1 3vy1 ICRvx1 XR P l r ξI· J1βs() J1f ξI·Rθ()1–J1f1–J2ϕ· = J1f αβγ,, γ0= α α1π3⁄=() α2π=() α3 π3⁄–=() ,, β 0= J1f'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 81,\n",
       "  'text_chunk': '66 Chapter 3 (3.32) Once again, computing the value of requires calculating the inverse, , as needed in equation (3.31). One approach would be to apply rote methods for calculating the inverse of a 3x3 square matrix. A second approach would be to compute the contribution of each Swedish wheel to chassis motion, as shown in section 3.2.2. We leave this process as an exercise for the enthusiast.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 81,\n",
       "  'text_chunk': 'Once the inverse is obtained, can be isolated: (3.33) Consider a specific omnidrive chassis with and for all wheels. The robot’s local reference frame and global reference frame are aligned, so that . If wheels 1, 2, and 3 spin at speeds , what is the resulting motion of the whole robot?'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 81,\n",
       "  'text_chunk': 'Using the equation above, the answer can be calculated readily: (3.34) So this robot will move instantaneously along the -axis with positive speed and along the axis with negative speed while rotating clockwise.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 81,\n",
       "  'text_chunk': 'We can see from the above exam-ples that robot motion can be predicted by combining the rolling constraints of individual wheels.J 1fπ 3---sinπ 3---cos– l– 0 πcos– l– π 3---–sinπ 3---–cos– l–3 2-------1 2---– l– 01 l– 3 2-------–1 2---– l–== ξI·J1f1– ξI· ξI·Rθ()1–1 3-------01 3-------– 1 3---–2 3---1 3---– 1 3l-----–1 3l-----–1 3l-----–J2ϕ· = l1= r 1= θ 0= ϕ14=() ϕ21=() ϕ32=() ,, ξI·x· y· θ·100 0100011 3-------01 3-------– 1 3---–2 3---1 3---– 1 3---–1 3---–1 3---–100 010 00141 22 3------- 4 3---– 7 3---–= == x y'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 82,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 67 The sliding constraints comprising can be used to go even further, enabling us to evaluate the maneuverability and workspace of the robot rather than just its predicted motion. Next, we examine methods for using the sliding constraints, sometimes in conjunc- tion with rolling constraints, to generate powerful analyses of the maneuverability of arobot chassis. 3.3 Mobile Robot ManeuverabilityThe kinematic mobility of a robot chassis is its ability to directly move in the environment. The basic constraint limiting mobility is the rule that every wheel must satisfy its slidingconstraint.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 82,\n",
       "  'text_chunk': 'Therefore, we can formally derive robot mobility by starting from equation (3.26). In addition to instantaneous kinematic motion, a mobile robot is able to further manip- ulate its position, over time, by steering steerable wheels. As we will see in section 3.3.3, the overall maneuverability of a robot is thus a combination of the mobility available based on the kinematic sliding constraints of the standard wheels, plus the additional freedomcontributed by steering and spinning the steerable standard wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 82,\n",
       "  'text_chunk': '3.3.1 Degree of mobility Equation (3.26) imposes the constraint that every wheel must avoid any lateral slip. Of course, this holds separately for each and every wheel, and so it is possible to specify this constraint separately for fixed and for steerable standard wheels: (3.35) (3.36) For both of these constraints to be satisfied, the motion vector must belong to thenull space of the projection matrix , which is simply a combination of and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 82,\n",
       "  'text_chunk': 'Mathematically, the null space of is the space N such that for any vector n in N, . If the kinematic constraints are to be honored, then the motion of the robot must always be within this space . The kinematic constraints [equations (3.35) and (3.36)] can also be demonstrated geometrically using the concept of a robot’s instantaneous center of rotation () . Consider a single standard wheel. It is forced by the sliding constraint to have zero lat- eral motion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 82,\n",
       "  'text_chunk': 'This can be shown geometrically by drawing a zero motion line through its horizontal axis, perpendicular to the wheel plane (figure 3.12). At any given instant, wheel motion along the zero motion line must be zero. In other words, the wheel must be movingC1βs() C1fRθ()ξI·0= C1sβs()Rθ()ξI·0= Rθ()ξI· C1βs() C1f C1s C1βs() C1βs()n 0= N ICR'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 83,\n",
       "  'text_chunk': '68 Chapter 3 instantaneously along some circle of radius such that the center of that circle is located on the zero motion line. This center point, called the instantaneous center of rotation, may lie anywhere along the zero motion line. When R is at infinity, the wheel moves in a straight line. A robot such as the Ackerman vehicle in figure 3.12a can have several wheels, but must always have a single .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 83,\n",
       "  'text_chunk': 'Because all of its zero motion lines meet at a single point, there is a single solution for robot motion, placing the at this meet point. This geometric construction demonstrates how robot mobility is a function of the number of constraints on the robot’s motion, not the number of wheels. In figure 3.12b, the bicycle shown has two wheels, and . Each wheel contributes a constraint, or a zero motion line. Taken together the two constraints result in a single point as the only remaining solution for the .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 83,\n",
       "  'text_chunk': 'This is because the two constraints are independent, and thus eachfurther constrains overall robot motion. But in the case of the differential drive robot in figure 3.13a, the two wheels are aligned along the same horizontal axis. Therefore, the is constrained to lie along a line, not ata specific point. In fact, the second wheel imposes no additional kinematic constraints on robot motion since its zero motion line is identical to that of the first wheel.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 83,\n",
       "  'text_chunk': 'Thus, although the bicycle and differential-drive chassis have the same number of nonomnidirectionalwheels, the former has two independent kinematic constraints while the latter has only one.Figure 3.12 (a) Four-wheel with car-like Ackerman steering. (b) bicycle.ICR ICRw1 a) b)w2 R ICR ICR ICR w1 w2 ICR ICR'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 84,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 69 The Ackerman vehicle of figure 3.12a demonstrates another way in which a wheel may be unable to contribute an independent constraint to the robot kinematics. This vehicle has two steerable standard wheels. Given the instantaneous position of just one of these steer- able wheels and the position of the fixed rear wheels, there is only a single solution for the . The position of the second steerable wheel is absolutely constrained by the . Therefore, it offers no independent constraints to robot motion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 84,\n",
       "  'text_chunk': 'Robot chassis kinematics is therefore a function of the set of independent constraints arising from all standard wheels. The mathematical interpretation of independence is related to the rank of a matrix. Recall that the rank of a matrix is the smallest number of independent rows or columns. Equation (3.26) represents all sliding constraints imposed bythe wheels of the mobile robot. Therefore is the number of independent con-straints. The greater the number of independent constraints, and therefore the greater the rank of , the more constrained is the mobility of the robot.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 84,\n",
       "  'text_chunk': 'For example, consider a robot with a single fixed standard wheel. Remember that we consider only standard wheels. This robot may be a unicycle or it may have several Swedish wheels; however, it has exactly one fixed standard wheel. The wheel is at a position specified by parameters relativeto the robot’s local reference frame. is comprised of and . However, since there are no steerable standard wheels is empty and therefore contains only .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 84,\n",
       "  'text_chunk': 'Because there is one fixed standard wheel, this matrix has a rank of one and therefore this robot has a single independent constrain on mobility: (3.37) Figure 3.13 (a) Differential drive robot with two individually motorized wheels and a castor wheel, e.g., the Pyg- malion robot at EPFL. (b) Tricycle with two fixed standard wheels and one steered standard wheel, e.g. Piaggio minitransporter.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 84,\n",
       "  'text_chunk': 'βst()a) b) ICR ICR rank C1βs() C1βs() αβ l,, C1βs() C1f C1s C1s C1βs() C1f C1βs() C1f αβ+()cos αβ+()sin lβsin =='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 85,\n",
       "  'text_chunk': '70 Chapter 3 Now let us add an additional fixed standard wheel to create a differential-drive robot by constraining the second wheel to be aligned with the same horizontal axis as the original wheel. Without loss of generality, we can place point at the midpoint between the centers of the two wheels. Given for wheel and for wheel , itholds geometrically that .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 85,\n",
       "  'text_chunk': 'Therefore, in this case, the matrix has two constraints but a rank of one: (3.38) Alternatively, consider the case when is placed in the wheel plane of but with the same orientation, as in a bicycle with the steering locked in the forward position. Weagain place point between the two wheel centers, and orient the wheels such that they lie on axis .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 85,\n",
       "  'text_chunk': 'This geometry implies that and, therefore, the matrix retains two independent constraints and has a rank of two: (3.39) In general, if then the vehicle can, at best, only travel along a circle or along a straight line. This configuration means that the robot has two or more independent constraints due to fixed standard wheels that do not share the same horizontal axis of rota- tion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 85,\n",
       "  'text_chunk': 'Because such configurations have only a degenerate form of mobility in the plane, wedo not consider them in the remainder of this chapter. Note, however, that some degenerate configurations such as the four-wheeled slip/skid steering system are useful in certain envi- ronments, such as on loose soil and sand, even though they fail to satisfy sliding constraints.Not surprisingly, the price that must be paid for such violations of the sliding constraints is that dead reckoning based on odometry becomes less accurate and power efficiency is reduced dramatically.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 85,\n",
       "  'text_chunk': 'In general, a robot will have zero or more fixed standard wheels and zero or more steer- able standard wheels. We can therefore identify the possible range of rank values for anyrobot: . Consider the case . This is only possibleif there are zero independent kinematic constraints in . In this case there are neither fixed nor steerable standard wheels attached to the robot frame: . Consider the other extreme, .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 85,\n",
       "  'text_chunk': 'This is the maximum possible rank since the kinematic constraints are specified along three degrees of freedom (i.e., the con-straint matrix is three columns wide).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 85,\n",
       "  'text_chunk': 'Therefore, there cannot be more than three indepen-P α1β1l1 ,, w1 α2β2l2 ,, w2 l1l2=() β1β20 = = () α1π+ α2= () ,, {} C1βs() C1βs() C1fα1()cos α1()sin 0 α1π+()cos α1π+()sin 0== w2 w1 P x1 l1l2=() β1β2π2⁄ = = () α10=() α2π=() ,, , {} C1βs() C1βs() C1fπ2⁄()cos π2⁄()sin l1 π2⁄()sin 3π2⁄()cos 3 π2⁄()sin l1 π2⁄()sin01 l1 01–l1== = rank C1f1> 0r≤ank C1βs() 3≤ rank C1βs() 0= C1βs() NfNs0 == rank C1βs() 3='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 86,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 71 dent constraints. In fact, when , then the robot is completely constrained in all directions and is, therefore, degenerate since motion in the plane is totally impossible. Now we are ready to formally define a robot’s degree of mobility : (3.40) The dimensionality of the null space ( ) of matrix is a measure of the number of degrees of freedom of the robot chassis that can be immediately manipulatedthrough changes in wheel velocity. It is logical therefore that must range between 0 and 3.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 86,\n",
       "  'text_chunk': 'Consider an ordinary differential-drive chassis. On such a robot there are two fixed stan- dard wheels sharing a common horizontal axis. As discussed above, the second wheel adds no independent kinematic constraints to the system. Therefore, and . This fits with intuition: a differential drive robot can control both the rate of its change in orientation and its forward/reverse speed, simply by manipulating wheel veloci- ties. In other words, its is constrained to lie on the infinite line extending from its wheels’ horizontal axles.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 86,\n",
       "  'text_chunk': 'In contrast, consider a bicycle chassis. This configuration consists of one fixed standard wheel and one steerable standard wheel. In this case, each wheel contributes an indepen- dent sliding constraint to . Therefore, . Note that the bicycle has the sametotal number of nonomidirectional wheels as the differential-drive chassis, and indeed one of its wheels is steerable. Yet it has one less degree of mobility. Upon reflection this is appropriate. A bicycle only has control over its forward/reverse speed by direct manipula-tion of wheel velocities.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 86,\n",
       "  'text_chunk': 'Only by steering can the bicycle change its . As expected, based on equation (3.40) any robot consisting only of omnidirectional wheels such as Swedish or spherical wheels will have the maximum mobility, . Such a robot can directly manipulate all three degrees of freedom. 3.3.2 Degree of steerability The degree of mobility defined above quantifies the degrees of controllable freedom based on changes to wheel velocity.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 86,\n",
       "  'text_chunk': 'Steering can also have an eventual impact on a robot chassis pose , although the impact is indirect because after changing the angle of a steerable stan-dard wheel, the robot must move for the change in steering angle to have impact on pose.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 86,\n",
       "  'text_chunk': 'As with mobility, we care about the number of independently controllable steering parameters when defining the degree of steerability : (3.41)rank C 1βs() 3= δm δm dimN C1βs() 3rank C1βs() – == dimN C1βs() δm rank C1βs() 1= δm 2= ICR C1βs() δm 1= ICR δm 3= ξ δs δsrank C1sβs() ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 87,\n",
       "  'text_chunk': '72 Chapter 3 Recall that in the case of mobility, an increase in the rank of implied more kine- matic constraints and thus a less mobile system. In the case of steerability, an increase in the rank of implies more degrees of steering freedom and thus greater eventual maneuverability. Since includes , this means that a steered standard wheelcan both decrease mobility and increase steerability: its particular orientation at any instant imposes a kinematic constraint, but its ability to change that orientation can lead to addi- tional trajectories.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 87,\n",
       "  'text_chunk': 'The range of can be specified: . The case implies that the robot has no steerable standard wheels, . The case is most common when a robotconfiguration includes one or more steerable standard wheels. For example, consider an ordinary automobile. In this case and . But the fixed wheels share a common axle and so . The fixed wheels and anyone of the steerable wheels constrain the to be a point along the line extending fromthe rear axle.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 87,\n",
       "  'text_chunk': 'Therefore, the second steerable wheel cannot impose any independent kine- matic constraint and so . In this case and . The case is only possible in robots with no fixed standard wheels: . Under these circumstances, it is possible to create a chassis with two separate steerablestandard wheels, like a pseudobicycle (or the two-steer) in which both wheels are steerable. Then, orienting one wheel constrains the to a line while the second wheel can con-strain the to any point along that line.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 87,\n",
       "  'text_chunk': 'Interestingly, this means that the implies that the robot can place its anywhere on the ground plane. 3.3.3 Robot maneuverability The overall degrees of freedom that a robot can manipulate, called the degree of maneuver- ability , can be readily defined in terms of mobility and steerability: (3.42) Therefore maneuverability includes both the degrees of freedom that the robot manipu- lates directly through wheel velocity and the degrees of freedom that it indirectly manipu-lates by changing the steering configuration and moving.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 87,\n",
       "  'text_chunk': 'Based on the investigations of the previous sections, one can draw the basic types of wheel configurations. They are depictedin figure 3.14 Note that two robots with the same are not necessarily equivalent. For example, dif- ferential drive and tricycle geometries (figure 3.13) have equal maneuverability .In differential drive all maneuverability is the result of direct mobility because and . In the case of a tricycle the maneuverability results from steering also: and . Neither of these configurations allows the to range anywhere on theplane.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 87,\n",
       "  'text_chunk': 'In both cases, the must lie on a predefined line with respect to the robot refer-C1βs() C1sβs() C1βs() C1sβs() δs 0δs2 ≤≤ δs0= Ns0= δs1= Nf2= Ns2= rank C1f1= ICR rank C1sβs() 1= δm 1= δs1= δs2= Nf0= ICR ICR δs2= ICR δM δM δmδs+ = δM δM 2= δm 2= δs0= δm 1= δs1= ICR ICR'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 88,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 73 ence frame. In the case of differential drive, this line extends from the common axle of the two fixed standard wheels, with the differential wheel velocities setting the point on this line. In a tricycle, this line extends from the shared common axle of the fixed wheels, with the steerable wheel setting the point along this line. More generally, for any robot with the is always constrained to lie on a line and for any robot with the can be set to any point on the plane.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 88,\n",
       "  'text_chunk': 'One final example will demonstrate the use of the tools we have developed above. One common robot configuration for indoor mobile robotics research is the synchro drive con- figuration (figure 2.22). Such a robot has two motors and three wheels that are lockedtogether. One motor provides power for spinning all three wheels while the second motorprovides power for steering all three wheels. In a three-wheeled synchro drive robot and . Therefore can be used to determine both and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 88,\n",
       "  'text_chunk': 'The three wheels do not share a common axle, therefore two of the three contribute independent sliding constraints. The third must be dependent on these two constraints for motion to be possible. Therefore and . This is intuitively cor- rect. A synchro drive robot with the steering frozen manipulates only one degree of free-dom, consisting of traveling back and forth on a straight line. However an interesting complication occurs when considering .Based on equation (3.41) the robot should have .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 88,\n",
       "  'text_chunk': 'Indeed, for a three-wheel-steering robot with the geo-metric configuration of a synchro drive robot this would be correct. However, we have additional information: in a synchro drive configuration a single motor steers all three wheels using a belt drive. Therefore, although ideally, if the wheels were independentlysteerable, then the system would achieve , in the case of synchro drive the driveFigure 3.14 The five basic types of three-wheel configurations. The spherical wheels can be replaced by castor or Swedish wheels without influencing maneuverability.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 88,\n",
       "  'text_chunk': 'More configurations with various numbers o f wheels are found in chapter 2.Omnidirectional δ M =3 δ m =3 δ s =0Differential δ M =2 δ m =2 δ s =0 Omni-Steer δ M =3 δ m =2 δ s =1 Tricycle δ M =2 δ m =1 δ s =1 Two-Steer δ M =3 δ m =1 δ s =2 ICR ICR δM 2= ICR δM 3= ICR Nf0= Ns3= rank C1sβs() δm δs rank C1sβs() 2= δm 1= δs δs2= δs2='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 89,\n",
       "  'text_chunk': '74 Chapter 3 system further constrains the kinematics such that in reality . Finally, we can com- pute maneuverability based on these values: for a synchro drive robot. This result implies that a synchro drive robot can only manipulate, in total, two degrees of freedom. In fact, if the reader reflects on the wheel configuration of a synchro drive robotit will become apparent that there is no way for the chassis orientation to change.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 89,\n",
       "  'text_chunk': 'Only the position of the chassis can be manipulated and so, indeed, a synchro drive robot has only two degrees of freedom, in agreement with our mathematical conclusion. 3.4 Mobile Robot Workspace For a robot, maneuverability is equivalent to its control degrees of freedom. But the robot is situated in some environment, and the next question is to situate our analysis in the envi- ronment. We care about the ways in which the robot can use its control degrees of freedomto position itself in the environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 89,\n",
       "  'text_chunk': 'For instance, consider the Ackerman vehicle, or auto- mobile. The total number of control degrees of freedom for such a vehicle is , one for steering and the second for actuation of the drive wheels. But what is the total degreesof freedom of the vehicle in its environment? In fact it is three: the car can position itself on the plane at any point and with any angle .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 89,\n",
       "  'text_chunk': 'Thus identifying a robot’s space of possible configurations is important because surpris- ingly it can exceed .In addition to workspace , we care about how the robot is able to move between various configurations: what are the types of paths that it can follow and, furthermore, what are its possible trajectories through this configuration space? In theremainder of this discussion, we move away from inner kinematic details such as wheels and focus instead on the robot chassis pose and the chassis degrees of freedom.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 89,\n",
       "  'text_chunk': 'With this in mind, let us place the robot in the context of its workspace now. 3.4.1 Degrees of freedom In defining the workspace of a robot, it is useful to first examine its admissible velocity space . Given the kinematic constraints of the robot, its velocity space describes the inde- pendent components of robot motion that the robot can control.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 89,\n",
       "  'text_chunk': 'For example, the velocityspace of a unicycle can be represented with two axes, one representing the instantaneous forward speed of the unicycle and the second representing the instantaneous change in ori- entation, , of the unicycle. The number of dimensions in the velocity space of a robot is the number of indepen- dently achievable velocities. This is also called the differentiable degrees of freedom ( ). A robot’s is always equal to its degree of mobility . For example, a bicycle has the following degree of maneuverability: .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 89,\n",
       "  'text_chunk': 'The of a bicycle is indeed 1.δ s1= δM 2= xy– δM 2= xy,θ δM θ· DDO F DDO F δm δMδm= δs+1 1 +2== DDO F'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 90,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 75 In contrast to a bicycle, consider an omnibot , a robot with three Swedish wheels. We know that in this case there are zero standard wheels and therefore . So, the omnibot has three differential degrees of freedom. This is appropriate, given that because such a robot has no kinematic motion constraints, it is able to independently set all three pose variables: . Given the difference in DDOF between a bicycle and an omnibot, consider the overall degrees of freedom in the workspace of each configuration.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 90,\n",
       "  'text_chunk': 'The omnibot can achieve anypose in its environment and can do so by directly achieving the goal positions of all three axes simultaneously because . Clearly, it has a workspace with . Can a bicycle achieve any pose in its environment? It can do so, but achieving some goal points may require more time and energy than an equivalent omnibot. For exam- ple, if a bicycle configuration must move laterally 1 m, the simplest successful maneuverwould involve either a spiral or a back-and-forth motion similar to parallel parking of auto- mobiles.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 90,\n",
       "  'text_chunk': 'Nevertheless, a bicycle can achieve any and therefore the workspace ofa bicycle has =3 as well. Clearly, there is an inequality relation at work: . Although the dimensionality of a robot’s workspace is an important attribute, it is clear from the exampleabove that the particular paths available to a robot matter as well. Just as workspace DOF governs the robot’s ability to achieve various poses, so the robot’s governs its abil- ity to achieve various paths.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 90,\n",
       "  'text_chunk': '3.4.2 Holonomic robots In the robotics community, when describing the path space of a mobile robot, often the con- cept of holonomy is used. The term holonomic has broad applicability to several mathemat- ical areas, including differential equations, functions and constraint expressions. In mobile robotics, the term refers specifically to the kinematic constraints of the robot chassis. A holonomic robot is a robot that has zero nonholonomic kinematic constraints. Conversely, anonholonomic robot is a robot with one or more nonholonomic kinematic constraints.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 90,\n",
       "  'text_chunk': 'Aholonomic kinematic constraint can be expressed as an explicit function of position variables only. For example, in the case of a mobile robot with a single fixed standardwheel, a holonomic kinematic constraint would be expressible using only. Such a constraint may not use derivatives of these values, such as or . A nonholonomic kinematic constraint requires a differential relationship, such as the deriva- tive of a position variable. Furthermore, it cannot be integrated to provide a constraint in terms of the position variables only.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 90,\n",
       "  'text_chunk': 'Because of this latter point of view, nonholonomic sys- tems are often called nonintegrable systems.δ Mδm= δs+3 0+3== x·y·θ·,, x(yθ),, DDOF 3= DOF 3= x(yθ),, x(yθ),, DOF DDOF δMD≤OF ≤ DDO F α1β1l1r1ϕ1, ,, , , xy θ,, ϕ · ξ·'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 91,\n",
       "  'text_chunk': '76 Chapter 3 Consider the fixed standard wheel sliding constraint: (3.43) This constraint must use robot motion rather than pose because the point is to con- strain robot motion perpendicular to the wheel plane to be zero. The constraint is noninte- grable, depending explicitly on robot motion. Therefore, the sliding constraint is a nonholonomic constraint. Consider a bicycle configuration, with one fixed standard wheel and one steerable standard wheel.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 91,\n",
       "  'text_chunk': 'Because the fixed wheel sliding constraint will be inforce for such a robot, we can conclude that the bicycle is a nonholonomic robot. But suppose that one locks the bicycle steering system, so that it becomes two fixed stan- dard wheels with separate but parallel axes. We know that for such a configura-tion. Is it nonholonomic? Although it may not appear so because of the sliding and rolling constraints, the locked bicycle is actually holonomic. Consider the workspace of this locked bicycle.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 91,\n",
       "  'text_chunk': 'It consists of a single infinite line along which the bicycle can move (assum-ing the steering was frozen straight ahead). For formulaic simplicity, assume that this infi- nite line is aligned with in the global reference frame and that . In this case the sliding constraints of both wheels can be replaced with an equally complete set of constraints on the robot pose: .This eliminates two nonholonomic constraints, corresponding to the sliding constraints of the two wheels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 91,\n",
       "  'text_chunk': 'The only remaining nonholonomic kinematic constraints are the rolling constraints for each wheel: (3.44) This constraint is required for each wheel to relate the speed of wheel spin to the speed of motion projected along the wheel plane. But in the case of our locked bicycle, given the initial rotational position of a wheel at the origin, , we can replace this constraint with one that directly relates position on the line, x, with wheel rotation angle, : .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 91,\n",
       "  'text_chunk': 'The locked bicycle is an example of the first type of holonomic robot – where constraints do exist but are all holonomic kinematic constraints. This is the case for all holonomic robots with . The second type of holonomic robot exists when there are no kinematic constraints, that is, and . Since there are no kinematic constraints, there arealso no nonholonomic kinematic constraints and so such a robot is always holonomic.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 91,\n",
       "  'text_chunk': 'This is the case for all holonomic robots with .αβ+()cos αβ+()sin lβsin Rθ()ξ I·0= ξ·ξ δM 1= XI β12, π2⁄α1, 0α2, π == = {} y0θ0=,={} αβ+()sin– αβ+()cos lβcos Rθ()ξI·rϕ·+0 = ϕo ϕ ϕ xr⁄() ϕo+ = δM3< Nf0= Ns0= δM 3='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 92,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 77 An alternative way to describe a holonomic robot is based on the relationship between the differential degrees of freedom of a robot and the degrees of freedom of its workspace: a robot is holonomic if and only if = . Intuitively, this is because it is only through nonholonomic constraints (imposed by steerable or fixed standard wheels) that a robot can achieve a workspace with degrees of freedom exceeding its differential degrees of freedom, > . Examples include differential drive and bicycle/tricycle con- figurations.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 92,\n",
       "  'text_chunk': 'In mobile robotics, useful chassis generally must achieve poses in a workspace with dimensionality 3, so in general we require for the chassis. But the “holonomic”abilities to maneuver around obstacles without affecting orientation and to track at a targetwhile following an arbitrary path are important additional considerations. For these rea- sons, the particular form of holonomy most relevant to mobile robotics is that of . We define this class of robot configurations as omnidirectional: an omnidirectional robot is a holonomic robot with .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 92,\n",
       "  'text_chunk': '3.4.3 Path and trajectory considerations In mobile robotics, we care not only about the robot’s ability to reach the required final con- figurations but also about how it gets there. Consider the issue of a robot’s ability to follow paths: in the best case, a robot should be able to trace any path through its workspace of poses. Clearly, any omnidirectional robot can do this because it is holonomic in a three- dimensional workspace.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 92,\n",
       "  'text_chunk': 'Unfortunately, omnidirectional robots must use unconstrained wheels, limiting the choice of wheels to Swedish wheels, castor wheels, and sphericalwheels. These wheels have not yet been incorporated into designs allowing far larger amounts of ground clearance and suspensions. Although powerful from a path space point of view, they are thus much less common than fixed and steerable standard wheels, mainlybecause their design and fabrication are somewhat complex and expensive. Additionally, nonholonomic constraints might drastically improve stability of move- ments.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 92,\n",
       "  'text_chunk': 'Consider an omnidirectional vehicle driving at high speed on a curve with constantdiameter. During such a movement the vehicle will be exposed to a non-negligible centrip- etal force. This lateral force pushing the vehicle out of the curve has to be counteracted by the motor torque of the omnidirectional wheels. In case of motor or control failure, the vehi-cle will be thrown out of the curve. However, for a car-like robot with kinematic con- straints, the lateral forces are passively counteracted through the sliding constraints, mitigating the demands on motor torque.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 92,\n",
       "  'text_chunk': 'But recall an earlier example of high maneuverability using standard wheels: the bicycle on which both wheels are steerable, often called the two-steer . This vehicle achieves a degree of steerability of 2, resulting in a high degree of maneuverability: . Interestingly, this configuration is not holonomic, yet has a high degree of maneuverability in a workspace with . DDO F DOF DOF DDO F DOF 3= DDOF DOF 3 == DDOF 3= δMδm= δs+1 2+3== DOF 3='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 93,\n",
       "  'text_chunk': '78 Chapter 3 The maneuverability result, , means that the two-steer can select any by appropriately steering its two wheels. So, how does this compare to an omnidirectional robot? The ability to manipulate its in the plane means that the two-steer can follow any path in its workspace. More generally, any robot with can follow any path in its workspace from its initial pose to its final pose.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 93,\n",
       "  'text_chunk': 'An omnidirectional robot can also follow any path in its workspace and, not surprisingly, since in an omnidirectional robot, then it must follow that . But there is still a difference between a degree of freedom granted by steering versus by direct control of wheel velocity. This difference is clear in the context of trajectories rather than paths. A trajectory is like a path, except that it occupies an additional dimension: time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 93,\n",
       "  'text_chunk': 'Therefore, for an omnidirectional robot on the ground plane a path generally denotes a trace through a 3D space of pose; for the same robot a trajectory denotes a trace through the 4D space of pose plus time. For example, consider a goal trajectory in which the robot moves along axis at a con- stant speed of 1 m/s for 1 second, then changes orientation counterclockwise 90 degreesalso in 1 second, then moves parallel to axis for 1 final second.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 93,\n",
       "  'text_chunk': 'The desired 3-secondtrajectory is shown in figure 3.15, using plots of and in relation to time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 93,\n",
       "  'text_chunk': 'δ M 3= ICR ICR δM 3= δm 3= δM 3=Figure 3.15 Example of robot trajectory with omnidirectional robot: move for 1 second with constant speed o f 1m/s along axis ; change orientation counterclockwise 90 degree, in 1 second; move for 1 second with constant speed of 1 m/s along axis .XI YIYI XIx, y, θ t / [s]y(t) x(t) θ(t) 12 3 XI YI xy,θ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 94,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 79 Figure 3.16 Example of robot trajectory similar to figure 3.15 with two steered wheels: move for 1 second with constant speed of 1 m/s along axis ; rotate steered wheels -50 / 50 degree respectively; change ori- entation counterclockwise 90 degree in 1 second; rotate steered wheels 50 / -50 degree respectively; move for 1 second with constant speed of 1 m/s along axis .XI YIYI XI x, y, θ t / [s]y(t) x(t) θ(t) 12 3 4 5βs1,βs2 60° -60°βs1 βs2'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 95,\n",
       "  'text_chunk': '80 Chapter 3 Can the omnidirectional robot accomplish this trajectory? We assume that the robot can achieve some arbitrary, finite velocity at each wheel. For simplicity, we further assume that acceleration is infinite; that is, it takes zero time to reach any desired velocity. Under these assumptions, the omnidirectional robot can indeed follow the trajectory of figure 3.15. Thetransition between the motion of second 1 and second 2, for example, involves only changes to the wheel velocities.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 95,\n",
       "  'text_chunk': 'Because the two-steer has , it must be able to follow the path that would result from projecting this trajectory into timeless workspace. However, it cannot follow this 4D trajectory. Even if steering velocity is finite and arbitrary, although the two-steer would be able to change steering speed instantly, it would have to wait for the angle of the steerablewheels to change to the desired position before initiating a change in the robot chassis ori- entation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 95,\n",
       "  'text_chunk': 'In short, the two-steer requires changes to internal degrees of freedom and because these changes take time, arbitrary trajectories are not attainable. Figure 3.16depicts the most similar trajectory that a two-steer can achieve. In contrast to the desired three phases of motion, this trajectory has five phases. 3.5 Beyond Basic Kinematics The above discussion of mobile robot kinematics is only an introduction to a far richer topic.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 95,\n",
       "  'text_chunk': 'When speed and force are also considered, as is particularly necessary in the case of high-speed mobile robots, dynamic constraints must be expressed in addition to kinematic constraints. Furthermore, many mobile robots such as tank-type chassis and four-wheelslip/skid systems violate the kinematic models above. When analyzing such systems, it is often necessary to explicitly model the dynamics of viscous friction between the robot and the ground plane. More significantly, the kinematic analysis of a mobile robot system provides results concerning the theoretical workspace of that mobile robot.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 95,\n",
       "  'text_chunk': 'However to effectively move inthis workspace a mobile robot must have appropriate actuation of its degrees of freedom.This problem, called motorization, requires further analysis of the forces that must be actively supplied to realize the kinematic range of motion available to the robot. In addition to motorization, there is the question of controllability: under what condi- tions can a mobile robot travel from the initial pose to the goal pose in bounded time?'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 95,\n",
       "  'text_chunk': 'Answering this question requires knowledge – both knowledge of the robot kinematics and knowledge of the control systems that can be used to actuate the mobile robot. Mobile robot control is therefore a return to the practical question of designing a real-world control algo- rithm that can drive the robot from pose to pose using the trajectories demanded for the application.δ M 3='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 96,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 81 3.6 Motion Control (Kinematic Control) As seen above, motion control might not be an easy task for nonholonomic systems. How- ever, it has been studied by various research groups, for example, [8, 39, 52, 53, 137] and some adequate solutions for motion control of a mobile robot system are available. 3.6.1 Open loop control (trajectory-following) The objective of a kinematic controller is to follow a trajectory described by its position or velocity profile as a function of time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 96,\n",
       "  'text_chunk': 'This is often done by dividing the trajectory (path) inmotion segments of clearly defined shape, for example, straight lines and segments of a cir- cle. The control problem is thus to precompute a smooth trajectory based on line and circle segments which drives the robot from the initial position to the final position (figure 3.18).This approach can be regarded as open-loop motion control, because the measured robot position is not fed back for velocity or position control.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 96,\n",
       "  'text_chunk': 'It has several disadvantages: •It is not at all an easy task to precompute a feasible trajectory if all limitations and con- straints of the robot’s velocities and accelerations have to be considered. •The robot will not automatically adapt or correct the trajectory if dynamic changes ofthe environment occur.Figure 3.17 Typical situation for feedback control of a mobile robotYR XR goalv(t) ω(t)θ start'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 97,\n",
       "  'text_chunk': '82 Chapter 3 •The resulting trajectories are usually not smooth, because the transitions from one tra- jectory segment to another are, for most of the commonly used segments (e.g., lines andpart of circles), not smooth. This means there is a discontinuity in the robot’s accelera- tion. 3.6.2 Feedback control A more appropriate approach in motion control of a mobile robot is to use a real-state feed- back controller.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 97,\n",
       "  'text_chunk': 'With such a controller the robot’s path-planning task is reduced to settingintermediate positions (subgoals) lying on the requested path. One useful solution for a sta- bilizing feedback control of differential-drive mobile robots is presented in section 3.6.2.1. It is very similar to the controllers presented in [39, 100]. Others can be found in [8, 52, 53,137]. 3.6.2.1 Problem statement Consider the situation shown in figure 3.17, with an arbitrary position and orientation of the robot and a predefined goal position and orientation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 97,\n",
       "  'text_chunk': 'The actual pose error vector given in the robot reference frame is with , and being the goal coordinates of the robot. Figure 3.18 Open-loop control of a mobile robot based on straight lines and circular trajectory segments.YI XIgoal XRYRθ ,,{} ex y θ,,[]RT= xy,θ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 98,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 83 The task of the controller layout is to find a control matrix , if it exists with (3.45) such that the control of and (3.46) drives the error toward zero.2 (3.47) 2.Remember that v(t) is always heading in the XR direction of the robot’s reference frame due to the nonholonomic constraint.KFigure 3.19 Robot kinematics and its frames of interests.YR XRgoal v ωθρ αxˆ ∆xYG=YI XG=XIβ Kk11k12k13 k21k22k23= kij kte,()= vt() ωt() vt() ωt()Ke⋅ Kx y θR == e et() t∞→lim 0='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 99,\n",
       "  'text_chunk': '84 Chapter 3 3.6.2.2 Kinematic model We assume, without loss of generality, that the goal is at the origin of the inertial frame (fig- ure 3.19). In the following the position vector is always represented in the inertial frame. The kinematics of a differential-drive mobile robot described in the inertial frame is given by (3.48) where and are the linear velocities in the direction of the and of the inertial frame.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 99,\n",
       "  'text_chunk': 'Let denote the angle between the xR axis of the robot’s reference frame and the vector connecting the center of the axle of the wheels with the final position. If , where (3.49) then consider the coordinate transformation into polar coordinates with its origin at the goalposition.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 99,\n",
       "  'text_chunk': '(3.50) (3.51) (3.52) This yields a system description, in the new polar coordinates, using a matrix equation (3.53) where is the distance between the center of the robot’s wheel axle and the goal position,xy θ,,[] T XIYIθ,,{} x· y· θ·I θcos 0 θsin 0 01v ω= x· y· XI YI α xˆ αI1∈ I1π 2---–π 2---,\\uf8ed\\uf8eb= ρ∆ x2∆y2+ = α θ–2 ∆y∆x,() atan+ = βθ – α– = ρ· α· β·αcos–0 αsin ρ----------- 1 – αsin ρ-----------–0v ω= ρ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 100,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 85 denotes the angle between the axis of the robot reference frame, and the axis asso- ciated with the final position and are the tangent and the angular velocity respectively. On the other hand, if , where (3.54) redefining the forward direction of the robot by setting , we obtain a system described by a matrix equation of the form (3.55) 3.6.2.3 Remarks on the kinematic model in polar coordinates [eq.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 100,\n",
       "  'text_chunk': '(3.53) and (3.55)] •The coordinate transformation is not defined at ; as in such a point the deter- minant of the Jacobian matrix of the transformation is not defined, that is unbounded. •For the forward direction of the robot points toward the goal, for it is thereverse direction. •By properly defining the forward direction of the robot at its initial configuration, it is always possible to have at . However, this does not mean that remains in for all time .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 100,\n",
       "  'text_chunk': 'Hence, to avoid that the robot changes direction during approachingthe goal, it is necessary to determine, if possible, the controller in such a way that for all , whenever . The same applies for the reverse direction (see stability issues below). 3.6.2.4 The control law The control signals and must now be designed to drive the robot from its actual con- figuration, say , to the goal position.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 100,\n",
       "  'text_chunk': 'It is obvious that equation (3.53) presents a discontinuity at ; thus the theorem of Brockett does not obstruct smooth stabiliz- ability.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 100,\n",
       "  'text_chunk': 'If we consider now the linear control law (3.56) (3.57)θ XR XI v ω αI2∈ I2 π– π2⁄– ]π 2⁄π], (∪ ,(= v v–= ρ· α· β·αcos 0 αsin ρ-----------–1 αsin ρ----------- 0v ω= xy 0 == αI1∈ αI2∈ αI1∈ t0= α II t αI1∈ t α0() I1∈ v ω ρ0α0β0 ,,() ρ 0= v kρρ = ω kααkββ+ ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 101,\n",
       "  'text_chunk': '86 Chapter 3 we get with equation (3.53) a closed-loop system described by (3.58) The system does not have any singularity at and has a unique equilibrium point at . Thus it will drive the robot to this point, which is the goal posi- tion. •In the Cartesian coordinate system the control law [equation (3.57)] leads to equations which are not defined at . •Be aware of the fact that the angles and have always to be expressed in the range .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 101,\n",
       "  'text_chunk': '•Observe that the control signal has always a constant sign, that is, it is positive when- ever and it is always negative otherwise. This implies that the robot performs its parking maneuver always in a single direction and without reversing its motion. In figure 3.20 you find the resulting paths when the robot is initially on a circle in the plane. All movements have smooth trajectories toward the goal in the center. The con- trol parameters for this simulation were set to .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 101,\n",
       "  'text_chunk': '(3.59) 3.6.2.5 Local stability issue It can further be shown, that the closed-loop control system [equation (3.58)] is locally exponentially stable if ; ; (3.60) Proof: Linearized around the equilibrium ( , ) position, equation (3.58) can be written as , (3.61)ρ· α· β·k–ρραcos kραsin kααkββ– – k–ραsin= ρ 0= ραβ,,() 00 0,,()= xy 0 == α β π–π,() v α0() I1∈ xy k kρk,αkβ, () 38 1 .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 101,\n",
       "  'text_chunk': '5–,,() == kρ0> kβ0< kαkρ–0> xcos 1 = xsin x= ρ· α· β·k–ρ 00 0 kαk–ρ ()– kβ– 0 k–ρ 0ρ α β='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 102,\n",
       "  'text_chunk': 'Mobile Robot Kinematics 87 Figure 3.20 Resulting paths when the robot is initially on the unit circle in the x,y plane.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 103,\n",
       "  'text_chunk': '88 Chapter 3 hence it is locally exponentially stable if the eigenvalues of the matrix (3.62) all have a negative real part. The characteristic polynomial of the matrix is (3.63) and all roots have negative real part if ; ; (3.64) which proves the claim.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 103,\n",
       "  'text_chunk': 'For robust position control, it might be advisable to apply the strong stability condition, which ensures that the robot does not change direction during its approach to the goal: ; ; (3.65) This implies that for all t, whenever and for all , whenever respectively.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 103,\n",
       "  'text_chunk': 'This strong stability condition has also been verified in applica- tions.Ak–ρ 00 0 kαk–ρ ()– kβ– 0 k–ρ 0= A λkρ+() λ2λkαkρ–() kρkβ – +() kρ0> k–β0> kαkρ–0> kρ0> kβ0< kα5 3---kβ2 π---kρ– +0 > αI1∈ α0() I1∈ αI2∈ t α0() I2∈'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 104,\n",
       "  'text_chunk': '4 Perception One of the most important tasks of an autonomous system of any kind is to acquire knowl- edge about its environment. This is done by taking measurements using various sensors and then extracting meaningful information from those measurements. In this chapter we present the most common sensors used in mobile robots and then dis- cuss strategies for extracting information from the sensors. For more detailed informationabout many of the sensors used on mobile robots, refer to the comprehensive book Sensors for Mobile Robots by H.R. Everett [15].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 104,\n",
       "  'text_chunk': '4.1 Sensors for Mobile Robots There are a wide variety of sensors used in mobile robots (figure 4.1). Some sensors are used to measure simple values like the internal temperature of a robot’s electronics or the rotational speed of the motors. Other, more sophisticated sensors can be used to acquireinformation about the robot’s environment or even to directly measure a robot’s global position. In this chapter we focus primarily on sensors used to extract information about the robot’s environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 104,\n",
       "  'text_chunk': 'Because a mobile robot moves around, it will frequently encounter unforeseen environmental characteristics, and therefore such sensing is particularly critical. We begin with a functional classification of sensors. Then, after presenting basic tools for describing a sensor’s performance, we proceed to describe selected sensors in detail. 4.1.1 Sensor classification We classify sensors using two important functional axes: proprioceptive/exteroceptive and passive/active . Proprioceptive sensors measure values internal to the system (robot); for example, motor speed, wheel load, robot arm joint angles, battery voltage.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 104,\n",
       "  'text_chunk': 'Exteroceptive sensors acquire information from the robot’s environment; for example, distance measurements, light intensity, sound amplitude. Hence exteroceptive sensor mea-surements are interpreted by the robot in order to extract meaningful environmental fea-tures.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 105,\n",
       "  'text_chunk': '90 Chapter 4 Passive sensors measure ambient environmental energy entering the sensor. Examples of passive sensors include temperature probes, microphones, and CCD or CMOS cameras. Active sensors emit energy into the environment, then measure the environmental reac- tion. Because active sensors can manage more controlled interactions with the environ- ment, they often achieve superior performance. However, active sensing introduces several risks: the outbound energy may affect the very characteristics that the sensor is attempting to measure.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 105,\n",
       "  'text_chunk': 'Furthermore, an active sensor may suffer from interference between its signalFigure 4.1 Examples of robots with multi-sensor systems: (a) HelpMate from Transition Research Corporation; (b) B21 from Real World Interface; (c) BIBA Robot, BlueBotics SA. b) c) Sonar SensorsPan-Tilt Stereo Camera IR Sensors Pan-Tilt CameraOmnidirectional CameraIMU Inertial Measurement Unit Sonar Sensors Laser RangeScanner BumperEmergency Stop Button Wheel Encodersa)'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 106,\n",
       "  'text_chunk': 'Perception 91 and those beyond its control. For example, signals emitted by other nearby robots, or sim- ilar sensors on the same robot, may influence the resulting measurements. Examples of active sensors include wheel quadrature encoders, ultrasonic sensors, and laser rangefind- ers. Table 4.1 provides a classification of the most useful sensors for mobile robot applica- tions. The most interesting sensors are discussed in this chapter.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 106,\n",
       "  'text_chunk': 'Table 4.1 Classification of sensors used in mobile robotics applications General classification (typical use)Sensor Sensor SystemPC or ECA or P Tactile sensors (detection of physical contact or closeness; security switches)Contact switches, bumpersOptical barriersNoncontact proximity sensorsECECECP A A Wheel/motor sensors(wheel/motor speed and position)Brush encodersPotentiometers Synchros, resolvers Optical encodersMagnetic encoders Inductive encoders Capacitive encodersPCPC PC PCPC PC PCPP A AA A A Heading sensors (orientation of the robot in relation to a fixed reference frame)Compass Gyroscopes InclinometersEC PC ECP P A/P Ground-based beacons(localization in a fixed reference frame)GPS Active optical or RF beacons Active ultrasonic beacons Reflective beaconsEC EC EC ECA A A A Active ranging(reflectivity, time-of-flight, and geo- metric triangulation)Reflectivity sensors Ultrasonic sensor Laser rangefinder Optical triangulation (1D) Structured light (2D)EC EC EC EC ECA A A A A Motion/speed sensors(speed relative to fixed or moving objects)Doppler radar Doppler soundECECAA Vision-based sensors (visual ranging, whole-image analy-sis, segmentation, object recognition)CCD/CMOS camera(s)Visual ranging packagesObject tracking packagesEC P A, active; P, passive; P/A, passive/active; PC, proprioceptive; EC, exteroceptive.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 107,\n",
       "  'text_chunk': '92 Chapter 4 The sensor classes in table 4.1 are arranged in ascending order of complexity and descending order of technological maturity. Tactile sensors and proprioceptive sensors are critical to virtually all mobile robots, and are well understood and easily implemented. Commercial quadrature encoders, for example, may be purchased as part of a gear-motorassembly used in a mobile robot. At the other extreme, visual interpretation by means of one or more CCD/CMOS cameras provides a broad array of potential functionalities, from obstacle avoidance and localization to human face recognition.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 107,\n",
       "  'text_chunk': 'However, commerciallyavailable sensor units that provide visual functionalities are only now beginning to emerge [90, 160]. 4.1.2 Characterizing sensor performance The sensors we describe in this chapter vary greatly in their performance characteristics. Some sensors provide extreme accuracy in well-controlled laboratory settings, but areovercome with error when subjected to real-world environmental variations. Other sensors provide narrow, high-precision data in a wide variety of settings. In order to quantify such performance characteristics, first we formally define the sensor performance terminologythat will be valuable throughout the rest of this chapter.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 107,\n",
       "  'text_chunk': '4.1.2.1 Basic sensor response ratings A number of sensor characteristics can be rated quantitatively in a laboratory setting. Suchperformance ratings will necessarily be best-case scenarios when the sensor is placed on a real-world robot, but are nevertheless useful. Dynamic range is used to measure the spread between the lower and upper limits of input values to the sensor while maintaining normal sensor operation. Formally, thedynamic range is the ratio of the maximum input value to the minimum measurable input value.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 107,\n",
       "  'text_chunk': 'Because this raw ratio can be unwieldy, it is usually measured in decibels, which are computed as ten times the common logarithm of the dynamic range. However, there is potential confusion in the calculation of decibels, which are meant to measure the ratio between powers , such as watts or horsepower. Suppose your sensor measures motor current and can register values from a minimum of 1 mA to 20 Amps.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 107,\n",
       "  'text_chunk': 'The dynamic range of this current sensor is defined as (4.1) Now suppose you have a voltage sensor that measures the voltage of your robot’s bat- tery, measuring any value from 1 mV to 20 V. Voltage is not a unit of power, but the squareof voltage is proportional to power. Therefore, we use 20 instead of 10:1020 0.001------------- 43 dB= log⋅'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 108,\n",
       "  'text_chunk': 'Perception 93 (4.2) Range is also an important rating in mobile robot applications because often robot sen- sors operate in environments where they are frequently exposed to input values beyond their working range. In such cases, it is critical to understand how the sensor will respond. For example, an optical rangefinder will have a minimum operating range and can thus pro- vide spurious data when measurements are taken with the object closer than that minimum. Resolution is the minimum difference between two values that can be detected by a sen- sor.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 108,\n",
       "  'text_chunk': 'Usually, the lower limit of the dynamic range of a sensor is equal to its resolution.However, in the case of digital sensors, this is not necessarily so. For example, suppose thatyou have a sensor that measures voltage, performs an analog-to-digital (A/D) conversion, and outputs the converted value as an 8-bit number linearly corresponding to between 0 and 5 V. If this sensor is truly linear, then it has total output values, or a resolution of .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 108,\n",
       "  'text_chunk': 'Linearity is an important measure governing the behavior of the sensor’s output signal as the input signal varies. A linear response indicates that if two inputs x and y result in the two outputs and , then for any values and , . This means that a plot of the sensor’s input/output response is simply a straight line. Bandwidth orfrequency is used to measure the speed with which a sensor can provide a stream of readings.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 108,\n",
       "  'text_chunk': 'Formally, the number of measurements per second is defined as the sen- sor’s frequency in hertz. Because of the dynamics of moving through their environment, mobile robots often are limited in maximum speed by the bandwidth of their obstacle detec- tion sensors. Thus, increasing the bandwidth of ranging and vision-based sensors has been a high-priority goal in the robotics community. 4.1.2.2 In situ sensor performance The above sensor characteristics can be reasonably measured in a laboratory environment with confident extrapolation to performance in real-world deployment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 108,\n",
       "  'text_chunk': 'However, a number of important measures cannot be reliably acquired without deep understanding of the com-plex interaction between all environmental characteristics and the sensors in question. This is most relevant to the most sophisticated sensors, including active ranging sensors and visual interpretation sensors. Sensitivity itself is a desirable trait. This is a measure of the degree to which an incre- mental change in the target input signal changes the output signal. Formally, sensitivity isthe ratio of output change to input change.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 108,\n",
       "  'text_chunk': 'Unfortunately, however, the sensitivity ofexteroceptive sensors is often confounded by undesirable sensitivity and performance cou- pling to other environmental parameters.2020 0.001------------- 86 dB= log⋅ 281– 5 V 255() 20 mV= fx() fy() ab fa x b y +() afx() bf y()+ ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 109,\n",
       "  'text_chunk': '94 Chapter 4 Cross-sensitivity is the technical term for sensitivity to environmental parameters that are orthogonal to the target parameters for the sensor. For example, a flux-gate compass can demonstrate high sensitivity to magnetic north and is therefore of use for mobile robot nav- igation. However, the compass will also demonstrate high sensitivity to ferrous buildingmaterials, so much so that its cross-sensitivity often makes the sensor useless in some indoor environments. High cross-sensitivity of a sensor is generally undesirable, especially when it cannot be modeled.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 109,\n",
       "  'text_chunk': 'Error of a sensor is defined as the difference between the sensor’s output measurements and the true values being measured, within some specific operating context. Given a truevalue v and a measured value m, we can define error as . Accuracy is defined as the degree of conformity between the sensor’s measurement and the true value, and is often expressed as a proportion of the true value (e.g., 97.5% accu-racy).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 109,\n",
       "  'text_chunk': 'Thus small error corresponds to high accuracy and vice versa: (4.3) Of course, obtaining the ground truth, , can be difficult or impossible, and so establish- ing a confident characterization of sensor accuracy can be problematic. Further, it is impor-tant to distinguish between two different sources of error: Systematic errors are caused by factors or processes that can in theory be modeled. These errors are, therefore, deterministic (i.e., predictable).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 109,\n",
       "  'text_chunk': 'Poor calibration of a laserrangefinder, an unmodeled slope of a hallway floor, and a bent stereo camera head due toan earlier collision are all possible causes of systematic sensor errors. Random errors cannot be predicted using a sophisticated model nor can they be miti- gated by more precise sensor machinery. These errors can only be described in probabilisticterms (i.e., stochastically). Hue instability in a color camera, spurious rangefinding errors, and black level noise in a camera are all examples of random errors.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 109,\n",
       "  'text_chunk': 'Precision is often confused with accuracy, and now we have the tools to clearly distin- guish these two terms. Intuitively, high precision relates to reproducibility of the sensor results. For example, one sensor taking multiple readings of the same environmental state has high precision if it produces the same output. In another example, multiple copies ofthis sensor taking readings of the same environmental state have high precision if their out- puts agree.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 109,\n",
       "  'text_chunk': 'Precision does not, however, have any bearing on the accuracy of the sensor’s output with respect to the true value being measured. Suppose that the random error of a sensor is characterized by some mean value and a standard deviation . The formal def- inition of precision is the ratio of the sensor’s output range to the standard deviation:error m v– = accuracy 1error v---------------- - –=\\uf8ed\\uf8f8\\uf8eb\\uf8f6 v µ σ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 110,\n",
       "  'text_chunk': 'Perception 95 (4.4) Note that only and not has impact on precision. In contrast, mean error is directly proportional to overall sensor error and inversely proportional to sensor accuracy. 4.1.2.3 Characterizing error: the challenges in mobile robotics Mobile robots depend heavily on exteroceptive sensors. Many of these sensors concentrateon a central task for the robot: acquiring information on objects in the robot’s immediate vicinity so that it may interpret the state of its surroundings.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 110,\n",
       "  'text_chunk': 'Of course, these “objects” sur- rounding the robot are all detected from the viewpoint of its local reference frame. Sincethe systems we study are mobile, their ever-changing position and their motion have a sig- nificant impact on overall sensor behavior. In this section, empowered with the terminol- ogy of the earlier discussions, we describe how dramatically the sensor error of a mobilerobot disagrees with the ideal picture drawn in the previous section. Blurring of systematic and random errors .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 110,\n",
       "  'text_chunk': 'Active ranging sensors tend to have failure modes that are triggered largely by specific relative positions of the sensor and environmenttargets. For example, a sonar sensor will produce specular reflections, producing grossly inaccurate measurements of range, at specific angles to a smooth sheetrock wall. During motion of the robot, such relative angles occur at stochastic intervals. This is especially truein a mobile robot outfitted with a ring of multiple sonars. The chances of one sonar entering this error mode during robot motion is high.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 110,\n",
       "  'text_chunk': 'From the perspective of the moving robot, the sonar measurement error is a random error in this case. Yet, if the robot were to stop, becoming motionless, then a very different error modality is possible. If the robot’s static position causes a particular sonar to fail in this manner, the sonar will fail consistently and will tend to return precisely the same (and incorrect!) reading time after time. Once therobot is motionless, the error appears to be systematic and of high precision.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 110,\n",
       "  'text_chunk': 'The fundamental mechanism at work here is the cross-sensitivity of mobile robot sen- sors to robot pose and robot-environment dynamics. The models for such cross-sensitivity are not, in an underlying sense, truly random. However, these physical interrelationships are rarely modeled and therefore, from the point of view of an incomplete model, the errorsappear random during motion and systematic when the robot is at rest. Sonar is not the only sensor subject to this blurring of systematic and random error modality.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 110,\n",
       "  'text_chunk': 'Visual interpretation through the use of a CCD camera is also highly susceptible to robot motion and position because of camera dependence on lighting changes, lighting specularity (e.g., glare), and reflections. The important point is to realize that, while sys- tematic error and random error are well-defined in a controlled setting, the mobile robot canexhibit error characteristics that bridge the gap between deterministic and stochastic error mechanisms.precisionrange σ-------------- - = σ µ µ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 111,\n",
       "  'text_chunk': '96 Chapter 4 Multimodal error distributions . It is common to characterize the behavior of a sensor’s random error in terms of a probability distribution over various output values. In general, one knows very little about the causes of random error and therefore several simplifying assumptions are commonly used. For example, we can assume that the error is zero-mean , in that it symmetrically generates both positive and negative measurement error. We can go even further and assume that the probability density curve is Gaussian.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 111,\n",
       "  'text_chunk': 'Although we dis- cuss the mathematics of this in detail in section 4.2, it is important for now to recognize thefact that one frequently assumes symmetry as well as unimodal distribution . This means that measuring the correct value is most probable, and any measurement that is furtheraway from the correct value is less likely than any measurement that is closer to the correctvalue. These are strong assumptions that enable powerful mathematical principles to be applied to mobile robot problems, but it is important to realize how wrong these assump- tions usually are.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 111,\n",
       "  'text_chunk': 'Consider, for example, the sonar sensor once again. When ranging an object that reflects the sound signal well, the sonar will exhibit high accuracy, and will induce random errorbased on noise, for example, in the timing circuitry. This portion of its sensor behavior willexhibit error characteristics that are fairly symmetric and unimodal.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 111,\n",
       "  'text_chunk': 'However, when the sonar sensor is moving through an environment and is sometimes faced with materials that cause coherent reflection rather than returning the sound signal to the sonar sensor, then thesonar will grossly overestimate the distance to the object. In such cases, the error will be biased toward positive measurement error and will be far from the correct value. The error is not strictly systematic, and so we are left modeling it as a probability distribution of random error.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 111,\n",
       "  'text_chunk': 'So the sonar sensor has two separate types of operational modes, one in which the signal does return and some random error is possible, and the second in which the signal returns after a multipath reflection, and gross overestimation error occurs. Theprobability distribution could easily be at least bimodal in this case, and since overestima- tion is more common than underestimation it will also be asymmetric. As a second example, consider ranging via stereo vision. Once again, we can identify two modes of operation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 111,\n",
       "  'text_chunk': 'If the stereo vision system correctly correlates two images, thenthe resulting random error will be caused by camera noise and will limit the measurement accuracy. But the stereo vision system can also correlate two images incorrectly , matching two fence posts, for example, that are not the same post in the real world. In such a case stereo vision will exhibit gross measurement error, and one can easily imagine such behav- ior violating both the unimodal and the symmetric assumptions.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 111,\n",
       "  'text_chunk': 'The thesis of this section is that sensors in a mobile robot may be subject to multiple modes of operation and, when the sensor error is characterized, unimodality and symmetrymay be grossly violated. Nonetheless, as we shall see, many successful mobile robot sys-tems make use of these simplifying assumptions and the resulting mathematical techniques with great empirical success.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 112,\n",
       "  'text_chunk': 'Perception 97 The above sections have presented a terminology with which we can characterize the advantages and disadvantages of various mobile robot sensors. In the following sections, we do the same for a sampling of the most commonly used mobile robot sensors today. 4.1.3 Wheel/motor sensors Wheel/motor sensors are devices used to measure the internal state and dynamics of a mobile robot. These sensors have vast applications outside of mobile robotics and, as aresult, mobile robotics has enjoyed the benefits of high-quality, low-cost wheel and motor sensors that offer excellent resolution.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 112,\n",
       "  'text_chunk': 'In the next section, we sample just one such sensor, the optical incremental encoder. 4.1.3.1 Optical encoders Optical incremental encoders have become the most popular device for measuring angular speed and position within a motor drive or at the shaft of a wheel or steering mechanism.In mobile robotics, encoders are used to control the position or speed of wheels and other motor-driven joints.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 112,\n",
       "  'text_chunk': 'Because these sensors are proprioceptive , their estimate of position is best in the reference frame of the robot and, when applied to the problem of robot localiza- tion, significant corrections are required as, discussed in chapter 5. An optical encoder is basically a mechanical light chopper that produces a certain number of sine or square wave pulses for each shaft revolution.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 112,\n",
       "  'text_chunk': 'It consists of an illumina- tion source, a fixed grating that masks the light, a rotor disc with a fine optical grid that rotates with the shaft, and fixed optical detectors. As the rotor moves, the amount of light striking the optical detectors varies based on the alignment of the fixed and moving grat-ings. In robotics, the resulting sine wave is transformed into a discrete square wave using a threshold to choose between light and dark states. Resolution is measured in cycles per rev- olution (CPR).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 112,\n",
       "  'text_chunk': 'The minimum angular resolution can be readily computed from an encoder’s CPR rating. A typical encoder in mobile robotics may have 2000 CPR, while the optical encoder industry can readily manufacture encoders with 10000 CPR. In terms of required bandwidth, it is of course critical that the encoder be sufficiently fast to count atthe shaft spin speeds that are expected. Industrial optical encoders present no bandwidth limitation to mobile robot applications. Usually in mobile robotics the quadrature encoder is used.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 112,\n",
       "  'text_chunk': 'In this case, a second illumi- nation and detector pair is placed 90 degrees shifted with respect to the original in terms of the rotor disc. The resulting twin square waves, shown in figure 4.2, provide significantly more information. The ordering of which square wave produces a rising edge first identifiesthe direction of rotation. Furthermore, the four detectably different states improve the res- olution by a factor of four with no change to the rotor disc. Thus, a 2000 CPR encoder in quadrature yields 8000 counts.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 112,\n",
       "  'text_chunk': 'Further improvement is possible by retaining the sinusoidal'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 113,\n",
       "  'text_chunk': '98 Chapter 4 wave measured by the optical detectors and performing sophisticated interpolation. Such methods, although rare in mobile robotics, can yield 1000-fold improvements in resolution. As with most proprioceptive sensors, encoders are generally in the controlled environ- ment of a mobile robot’s internal structure, and so systematic error and cross-sensitivity can be engineered away.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 113,\n",
       "  'text_chunk': 'The accuracy of optical encoders is often assumed to be 100% and, although this may not be entirely correct, any errors at the level of an optical encoder aredwarfed by errors downstream of the motor shaft. 4.1.4 Heading sensors Heading sensors can be proprioceptive (gyroscope, inclinometer) or exteroceptive (com- pass). They are used to determine the robot’s orientation and inclination. They allow us,together with appropriate velocity information, to integrate the movement to a position esti-mate.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 113,\n",
       "  'text_chunk': 'This procedure, which has its roots in vessel and ship navigation, is called dead reck- oning . 4.1.4.1 CompassesThe two most common modern sensors for measuring the direction of a magnetic field are the Hall effect and flux gate compasses. Each has advantages and disadvantages, as described below. The Hall effect describes the behavior of electric potential in a semiconductor when in the presence of a magnetic field.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 113,\n",
       "  'text_chunk': 'When a constant current is applied across the length of asemiconductor, there will be a voltage difference in the perpendicular direction, across thesemiconductor’s width, based on the relative orientation of the semiconductor to magnetic flux lines. In addition, the sign of the voltage potential identifies the direction of the mag- netic field. Thus, a single semiconductor provides a measurement of flux and directionalong one dimension.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 113,\n",
       "  'text_chunk': 'Hall effect digital compasses are popular in mobile robotics, and con-Figure 4.2 Quadrature optical wheel encoder: The observed phase relationship between channel A and B pulse trains are used to determine the direction of the rotation. A single slot in the outer track generates a reference (index) pulse per revolution.s1 s2 s3 s4high high highhigh low low lowlowState Ch A Ch B'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 114,\n",
       "  'text_chunk': 'Perception 99 tain two such semiconductors at right angles, providing two axes of magnetic field (thresh- olded) direction, thereby yielding one of eight possible compass directions. The instruments are inexpensive but also suffer from a range of disadvantages. Resolution of adigital Hall effect compass is poor. Internal sources of error include the nonlinearity of the basic sensor and systematic bias errors at the semiconductor level. The resulting circuitry must perform significant filtering, and this lowers the bandwidth of Hall effect compassesto values that are slow in mobile robot terms.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 114,\n",
       "  'text_chunk': 'For example, the Hall effect compass pictured in figure 4.3 needs 2.5 seconds to settle after a 90 degree spin. The flux gate compass operates on a different principle. Two small coils are wound on ferrite cores and are fixed perpendicular to one another. When alternating current is acti-vated in both coils, the magnetic field causes shifts in the phase depending on its relative alignment with each coil. By measuring both phase shifts, the direction of the magneticfield in two dimensions can be computed.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 114,\n",
       "  'text_chunk': 'The flux gate compass can accurately measure the strength of a magnetic field and has improved resolution and accuracy; however, it is both larger and more expensive than a Hall effect compass. Regardless of the type of compass used, a major drawback concerning the use of the Earth’s magnetic field for mobile robot applications involves disturbance of that magneticfield by other magnetic objects and man-made structures, as well as the bandwidth limita- tions of electronic compasses and their susceptibility to vibration.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 114,\n",
       "  'text_chunk': 'Particularly in indoor environments, mobile robotics applications have often avoided the use of compasses,although a compass can conceivably provide useful local orientation information indoors, even in the presence of steel structures. 4.1.4.2 Gyroscopes Gyroscopes are heading sensors which preserve their orientation in relation to a fixed ref- erence frame.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 114,\n",
       "  'text_chunk': 'Thus they provide an absolute measure for the heading of a mobile system.Figure 4.3 Digital compass: Sensors such as the digital/analog Hall effect sensor shown, available from Dins- more (http://dinsmoregroup.com/dico), enable inexpensive (< $ 15) sensing of magnetic fields.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 115,\n",
       "  'text_chunk': '100 Chapter 4 Gyroscopes can be classified in two categories, mechanical gyroscopes and optical gyro- scopes. Mechanical gyroscopes. The concept of a mechanical gyroscope relies on the inertial properties of a fast-spinning rotor. The property of interest is known as the gyroscopic pre- cession. If you try to rotate a fast-spinning wheel around its vertical axis, you will feel aharsh reaction in the horizontal axis. This is due to the angular momentum associated with a spinning wheel and will keep the axis of the gyroscope inertially stable.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 115,\n",
       "  'text_chunk': 'The reactive torque τ and thus the tracking stability with the inertial frame are proportional to the spin- ning speed , the precession speed , and the wheel’s inertia . (4.5) By arranging a spinning wheel, as seen in figure 4.4, no torque can be transmitted from the outer pivot to the wheel axis. The spinning axis will therefore be space-stable (i.e., fixedin an inertial reference frame).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 115,\n",
       "  'text_chunk': 'Nevertheless, the remaining friction in the bearings of the gyro axis introduce small torques, thus limiting the long-term space stability and introduc- ing small errors over time. A high quality mechanical gyroscope can cost up to $100,000and has an angular drift of about 0.1 degrees in 6 hours. For navigation, the spinning axis has to be initially selected. If the spinning axis is aligned with the north-south meridian, the earth’s rotation has no effect on the gyro’s hor- izontal axis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 115,\n",
       "  'text_chunk': 'If it points east-west, the horizontal axis reads the earth rotation.ω Ω I τ IωΩ =Figure 4.4 Two-axis mechanical gyroscope.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 116,\n",
       "  'text_chunk': 'Perception 101 Rate gyros have the same basic arrangement as shown in figure 4.4 but with a slight modification. The gimbals are restrained by a torsional spring with additional viscous damping. This enables the sensor to measure angular speeds instead of absolute orientation. Optical gyroscopes. Optical gyroscopes are a relatively new innovation. Commercial use began in the early 1980s when they were first installed in aircraft. Optical gyroscopes are angular speed sensors that use two monochromatic light beams, or lasers, emitted from thesame source, instead of moving, mechanical parts.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 116,\n",
       "  'text_chunk': 'They work on the principle that the speed of light remains unchanged and, therefore, geometric change can cause light to take a varying amount of time to reach its destination. One laser beam is sent traveling clockwisethrough a fiber while the other travels counterclockwise. Because the laser traveling in the direction of rotation has a slightly shorter path, it will have a higher frequency. The differ- ence in frequency of the two beams is a proportional to the angular velocity of thecylinder.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 116,\n",
       "  'text_chunk': 'New solid-state optical gyroscopes based on the same principle are build using microfabrication technology, thereby providing heading information with resolution and bandwidth far beyond the needs of mobile robotic applications. Bandwidth, for instance,can easily exceed 100 kHz while resolution can be smaller than 0.0001 degrees/hr. 4.1.5 Ground-based beacons One elegant approach to solving the localization problem in mobile robotics is to use active or passive beacons. Using the interaction of on-board sensors and the environmental bea- cons, the robot can identify its position precisely.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 116,\n",
       "  'text_chunk': 'Although the general intuition is identicalto that of early human navigation beacons, such as stars, mountains, and lighthouses, modern technology has enabled sensors to localize an outdoor robot with accuracies of better than 5 cm within areas that are kilometers in size. In the following section, we describe one such beacon system, the global positioning system (GPS), which is extremely effective for outdoor ground-based and flying robots.Indoor beacon systems have been generally less successful for a number of reasons.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 116,\n",
       "  'text_chunk': 'Theexpense of environmental modification in an indoor setting is not amortized over an extremely large useful area, as it is, for example, in the case of the GPS. Furthermore, indoor environments offer significant challenges not seen outdoors, including multipathand environmental dynamics. A laser-based indoor beacon system, for example, must dis- ambiguate the one true laser signal from possibly tens of other powerful signals that have reflected off of walls, smooth floors, and doors.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 116,\n",
       "  'text_chunk': 'Confounding this, humans and other obsta- cles may be constantly changing the environment, for example, occluding the one true path from the beacon to the robot. In commercial applications, such as manufacturing plants, the environment can be carefully controlled to ensure success. In less structured indoor set-tings, beacons have nonetheless been used, and the problems are mitigated by careful beacon placement and the use of passive sensing modalities.∆ f Ω'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 117,\n",
       "  'text_chunk': '102 Chapter 4 4.1.5.1 The global positioning system The global positioning system (GPS) was initially developed for military use but is now freely available for civilian navigation. There are at least twenty-four operational GPS sat- ellites at all times. The satellites orbit every 12 hours at a height of 20.190 km. Four satel- lites are located in each of six planes inclined 55 degrees with respect to the plane of the earth’s equator (figure 4.5). Each satellite continuously transmits data that indicate its location and the current time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 117,\n",
       "  'text_chunk': 'Therefore, GPS receivers are completely passive but exteroceptive sensors. The GPS sat-ellites synchronize their transmissions so that their signals are sent at the same time. Whena GPS receiver reads the transmission of two or more satellites, the arrival time differences inform the receiver as to its relative distance to each satellite. By combining information regarding the arrival time and instantaneous location of four satellites, the receiver can inferits own position. In theory, such triangulation requires only three data points.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 117,\n",
       "  'text_chunk': 'However, timing is extremely critical in the GPS application because the time intervals being mea- sured are in nanoseconds. It is, of course, mandatory that the satellites be well synchro-nized. To this end, they are updated by ground stations regularly and each satellite carries on-board atomic clocks for timing. Figure 4.5 Calculation of position and heading based on GPS. monitor stations master stationsGPS satellites uploading stationusers'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 118,\n",
       "  'text_chunk': 'Perception 103 The GPS receiver clock is also important so that the travel time of each satellite’s trans- mission can be accurately measured. But GPS receivers have a simple quartz clock. So, although three satellites would ideally provide position in three axes, the GPS receiver requires four satellites, using the additional information to solve for four variables: threeposition axes plus a time correction. The fact that the GPS receiver must read the transmission of four satellites simulta- neously is a significant limitation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 118,\n",
       "  'text_chunk': 'GPS satellite transmissions are extremely low-power,and reading them successfully requires direct line-of-sight communication with the satel- lite. Thus, in confined spaces such as city blocks with tall buildings or in dense forests, one is unlikely to receive four satellites reliably. Of course, most indoor spaces will also fail toprovide sufficient visibility of the sky for a GPS receiver to function.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 118,\n",
       "  'text_chunk': 'For these reasons, the GPS has been a popular sensor in mobile robotics, but has been relegated to projects involv- ing mobile robot traversal of wide-open spaces and autonomous flying machines. A number of factors affect the performance of a localization sensor that makes use of the GPS. First, it is important to understand that, because of the specific orbital paths of theGPS satellites, coverage is not geometrically identical in different portions of the Earth andtherefore resolution is not uniform.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 118,\n",
       "  'text_chunk': 'Specifically, at the North and South Poles, the satellites are very close to the horizon and, thus, while resolution in the latitude and longitude direc- tions is good, resolution of altitude is relatively poor as compared to more equatorial loca-tions. The second point is that GPS satellites are merely an information source. They can be employed with various strategies in order to achieve dramatically different levels of local-ization resolution.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 118,\n",
       "  'text_chunk': 'The basic strategy for GPS use, called pseudorange and described above, generally performs at a resolution of 15 m. An extension of this method is differen- tial GPS (DGPS) , which makes use of a second receiver that is static and at a known exact position. A number of errors can be corrected using this reference, and so resolution improves to the order of 1 m or less.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 118,\n",
       "  'text_chunk': 'A disadvantage of this technique is that the stationary receiver must be installed, its location must be measured very carefully, and of course themoving robot must be within kilometers of this static unit in order to benefit from the DGPS technique. A further improved strategy is to take into account the phase of the carrier signals of each received satellite transmission. There are two carriers, at 19 cm and 24 cm, and there-fore significant improvements in precision are possible when the phase difference between multiple satellites is measured successfully.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 118,\n",
       "  'text_chunk': 'Such receivers can achieve 1 cm resolution forpoint positions and, with the use of multiple receivers, as in DGPS, sub-1 cm resolution. A final consideration for mobile robot applications is bandwidth.The GPS will generally offer no better than 200 to 300 ms latency, and so one can expect no better than 5 Hz GPSupdates. On a fast-moving mobile robot or flying robot, this can mean that local motion integration will be required for proper control due to GPS latency limitations.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 119,\n",
       "  'text_chunk': '104 Chapter 4 4.1.6 Active ranging Active ranging sensors continue to be the most popular sensors in mobile robotics. Many ranging sensors have a low price point, and, most importantly, all ranging sensors provide easily interpreted outputs: direct measurements of distance from the robot to objects in itsvicinity. For obstacle detection and avoidance, most mobile robots rely heavily on active ranging sensors. But the local freespace information provided by ranging sensors can also be accumulated into representations beyond the robot’s current local reference frame.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 119,\n",
       "  'text_chunk': 'Thusactive ranging sensors are also commonly found as part of the localization and environmen- tal modeling processes of mobile robots. It is only with the slow advent of successful visual interpretation competence that we can expect the class of active ranging sensors to gradu-ally lose their primacy as the sensor class of choice among mobile roboticists. Below, we present two time-of-flight active ranging sensors: the ultrasonic sensor and the laser rangefinder. Then, we present two geometric active ranging sensors: the opticaltriangulation sensor and the structured light sensor.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 119,\n",
       "  'text_chunk': '4.1.6.1 Time-of-flight active ranging Time-of-flight ranging makes use of the propagation speed of sound or an electromagneticwave. In general, the travel distance of a sound of electromagnetic wave is given by (4.6) where = distance traveled (usually round-trip); = speed of wave propagation; = time of flight. It is important to point out that the propagation speed of sound is approximately 0.3 m/ms whereas the speed of electromagnetic signals is 0.3 m/ns, which is 1 million times faster.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 119,\n",
       "  'text_chunk': 'The time of flight for a typical distance, say 3 m, is 10 ms for an ultrasonic system but only 10 ns for a laser rangefinder. It is thus evident that measuring the time offlight with electromagnetic signals is more technologically challenging. This explains why laser range sensors have only recently become affordable and robust for use on mobile robots.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 119,\n",
       "  'text_chunk': 'The quality of time-of-flight range sensors depends mainly on •uncertainties in determining the exact time of arrival of the reflected signal; •inaccuracies in the time-of-flight measurement (particularly with laser range sensors); •the dispersal cone of the transmitted beam (mainly with ultrasonic range sensors);dc t ⋅= d c t v t'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 120,\n",
       "  'text_chunk': 'Perception 105 •interaction with the target (e.g., surface absorption, specular reflections); •variation of propagation speed; •the speed of the mobile robot and target (in the case of a dynamic target); As discussed below, each type of time-of-flight sensor is sensitive to a particular subset of the above list of factors. The ultrasonic sensor (time-of-flight, sound).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 120,\n",
       "  'text_chunk': 'The basic principle of an ultrasonic sensor is to transmit a packet of (ultrasonic) pressure waves and to measure the time it takes for this wave packet to reflect and return to the receiver. The distance of the object caus- ing the reflection can be calculated based on the propagation speed of sound and the timeof flight . (4.7) The speed of sound c in air is given by (4.8) where γ = ratio of specific heats; R= gas constant; T= temperature in degrees Kelvin.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 120,\n",
       "  'text_chunk': 'In air at standard pressure and 20° C the speed of sound is approximately = 343 m/s. Figure 4.6 shows the different signal output and input of an ultrasonic sensor. First, a series of sound pulses are emitted, comprising the wave packet . An integrator also begins to linearly climb in value, measuring the time from the transmission of these sound wavesto detection of an echo. A threshold value is set for triggering an incoming sound wave as a valid echo.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 120,\n",
       "  'text_chunk': 'This threshold is often decreasing in time, because the amplitude of theexpected echo decreases over time based on dispersal as it travels longer. But during trans- mission of the initial sound pulses and just afterward, the threshold is set very high to sup- press triggering the echo detector with the outgoing sound pulses. A transducer willcontinue to ring for up to several milliseconds after the initial transmission, and this gov- erns the blanking time of the sensor.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 120,\n",
       "  'text_chunk': 'Note that if, during the blanking time, the transmitted sound were to reflect off of an extremely close object and return to the ultrasonic sensor, it may fail to be detected.d c t dct⋅ 2--------= c γRT = c'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 121,\n",
       "  'text_chunk': '106 Chapter 4 However, once the blanking interval has passed, the system will detect any above- threshold reflected sound, triggering a digital signal and producing the distance measure- ment using the integrator value. The ultrasonic wave typically has a frequency between 40 and 180 kHz and is usually generated by a piezo or electrostatic transducer. Often the same unit is used to measure thereflected signal, although the required blanking interval can be reduced through the use of separate output and input devices.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 121,\n",
       "  'text_chunk': 'Frequency can be used to select a useful range when choosing the appropriate ultrasonic sensor for a mobile robot. Lower frequencies corre-spond to a longer range, but with the disadvantage of longer post-transmission ringing and, therefore, the need for longer blanking intervals. Most ultrasonic sensors used by mobile robots have an effective range of roughly 12 cm to 5 m. The published accuracy of com-mercial ultrasonic sensors varies between 98% and 99.1%. In mobile robot applications, specific implementations generally achieve a resolution of approximately 2 cm.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 121,\n",
       "  'text_chunk': 'In most cases one may want a narrow opening angle for the sound beam in order to also obtain precise directional information about objects that are encountered. This is a major limitation since sound propagates in a cone-like manner (figure 4.7) with opening angles around 20 to 40 degrees. Consequently, when using ultrasonic ranging one does not acquiredepth data points but, rather, entire regions of constant depth. This means that the sensor tells us only that there is an object at a certain distance within the area of the measurement cone.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 121,\n",
       "  'text_chunk': 'The sensor readings must be plotted as segments of an arc (sphere for 3D) and not aspoint measurements (figure 4.8). However, recent research developments show significant improvement of the measurement quality in using sophisticated echo processing [76]. Ultrasonic sensors suffer from several additional drawbacks, namely in the areas of error, bandwidth, and cross-sensitivity. The published accuracy values for ultrasonics areFigure 4.6 Signals of an ultrasonic sensor.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 121,\n",
       "  'text_chunk': 'integratorwave packet threshold time of flight (sensor output)analog echo signal digital echo signal integrated time output signaltransmitted sound threshold'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 122,\n",
       "  'text_chunk': 'Perception 107 Figure 4.7 Typical intensity distribution of an ultrasonic sensor.-30° -60°0° 30° 60° Amplitude [dB]measurement cone Figure 4.8 Typical readings of an ultrasonic system: (a) 360 degree scan; (b) results from different geometric primitives [23]. Courtesy of John Leonard, MIT. a) b)'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 123,\n",
       "  'text_chunk': '108 Chapter 4 nominal values based on successful, perpendicular reflections of the sound wave off of an acoustically reflective material. This does not capture the effective error modality seen on a mobile robot moving through its environment. As the ultrasonic transducer’s angle to the object being ranged varies away from perpendicular, the chances become good that thesound waves will coherently reflect away from the sensor, just as light at a shallow angle reflects off of a smooth surface.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 123,\n",
       "  'text_chunk': 'Therefore, the true error behavior of ultrasonic sensors is compound, with a well-understood error distribution near the true value in the case of a suc-cessful retroreflection, and a more poorly understood set of range values that are grossly larger than the true value in the case of coherent reflection. Of course, the acoustic proper- ties of the material being ranged have direct impact on the sensor’s performance.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 123,\n",
       "  'text_chunk': 'Again,the impact is discrete, with one material possibly failing to produce a reflection that is suf- ficiently strong to be sensed by the unit. For example, foam, fur, and cloth can, in various circumstances, acoustically absorb the sound waves. A final limitation of ultrasonic ranging relates to bandwidth. Particularly in moderately open spaces, a single ultrasonic sensor has a relatively slow cycle time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 123,\n",
       "  'text_chunk': 'For example, mea-suring the distance to an object that is 3 m away will take such a sensor 20 ms, limiting itsoperating speed to 50 Hz. But if the robot has a ring of twenty ultrasonic sensors, each firing sequentially and measuring to minimize interference between the sensors, then the ring’s cycle time becomes 0.4 seconds and the overall update frequency of any one sensoris just 2.5 Hz.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 123,\n",
       "  'text_chunk': 'For a robot conducting moderate speed motion while avoiding obstacles using ultrasonics, this update rate can have a measurable impact on the maximum speed possible while still sensing and avoiding obstacles safely. Laser rangefinder (time-of-flight, electromagnetic). The laser rangefinder is a time-of- flight sensor that achieves significant improvements over the ultrasonic range sensor owing to the use of laser light instead of sound.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 123,\n",
       "  'text_chunk': 'This type of sensor consists of a transmitter which illuminates a target with a collimated beam (e.g., laser), and a receiver capable of detecting the component of light which is essentially coaxial with the transmitted beam. Oftenreferred to as optical radar or lidar (light detection and ranging), these devices produce a range estimate based on the time needed for the light to reach the target and return.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 123,\n",
       "  'text_chunk': 'Amechanical mechanism with a mirror sweeps the light beam to cover the required scene ina plane or even in three dimensions, using a rotating, nodding mirror. One way to measure the time of flight for the light beam is to use a pulsed laser and then measure the elapsed time directly, just as in the ultrasonic solution described earlier. Elec-tronics capable of resolving picoseconds are required in such devices and they are therefore very expensive.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 123,\n",
       "  'text_chunk': 'A second method is to measure the beat frequency between a frequency- modulated continuous wave (FMCW) and its received reflection. Another, even easiermethod is to measure the phase shift of the reflected light. We describe this third approach in detail.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 124,\n",
       "  'text_chunk': 'Perception 109 Phase-shift measurement. Near-infrared light (from a light-emitting diode [LED] or laser) is collimated and transmitted from the transmitter in figure 4.9 and hits a point P in the environment. For surfaces having a roughness greater than the wavelength of the inci- dent light, diffuse reflection will occur, meaning that the light is reflected almost isotropi- cally. The wavelength of the infrared light emitted is 824 nm and so most surfaces, with theexception of only highly polished reflecting objects, will be diffuse reflectors.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 124,\n",
       "  'text_chunk': 'The compo- nent of the infrared light which falls within the receiving aperture of the sensor will return almost parallel to the transmitted beam for distant objects. The sensor transmits 100% amplitude modulated light at a known frequency and mea- sures the phase shift between the transmitted and reflected signals. Figure 4.10 shows how this technique can be used to measure range. The wavelength of the modulating signalobeys the equation where is the speed of light and f the modulating frequency.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 124,\n",
       "  'text_chunk': \"For = 5 MHz (as in the AT&T sensor), = 60 m. The total distance covered by the emitted light isFigure 4.9 Schematic of laser rangefinding by phase-shift measurement.Phase MeasurementTargetD LTransmitter Transmitted Beam Reflected BeamP Figure 4.10 Range estimation by measuring the phase shift between transmitted and received signals.Transmitted Beam Reflected Beam0θ λPhase [m]Amplitude [V] cf λ⋅= c f λ D'\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 125,\n",
       "  'text_chunk': '110 Chapter 4 (4.9) where and are the distances defined in figure 4.9. The required distance , between the beam splitter and the target, is therefore given by (4.10) where is the electronically measured phase difference between the transmitted and reflected light beams, and the known modulating wavelength.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 125,\n",
       "  'text_chunk': 'It can be seen that the transmission of a single frequency modulated wave can theoretically result in ambiguous range estimates since, for example, if = 60 m, a target at a range of 5 m would give anindistinguishable phase measurement from a target at 65 m, since each phase angle would be 360 degrees apart. We therefore define an “ambiguity interval” of , but in practice we note that the range of the sensor is much lower than due to the attenuation of the signalin air.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 125,\n",
       "  'text_chunk': 'It can be shown that the confidence in the range (phase estimate) is inversely propor- tional to the square of the received signal amplitude, directly affecting the sensor’s accu-racy.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 125,\n",
       "  'text_chunk': \"Hence dark, distant objects will not produce as good range estimates as close, bright objects.D'L2D+ Lθ 2π------λ + == DL D Dλ 4π------θ = θ λ λ λ λFigure 4.11 (a) Schematic drawing of laser range sensor with rotating mirror; (b) Scanning range sensor from EPS Technologies Inc.; (c) Industrial 180 degree laser range sensor from Sick Inc., Germany a) c) DetectorLED/LaserRotating Mirror Transmitted lightReflected light Reflected lightb)\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 126,\n",
       "  'text_chunk': 'Perception 111 In figure 4.11 the schematic of a typical 360 degrees laser range sensor and two exam- ples are shown. figure 4.12 shows a typical range image of a 360 degrees scan taken with a laser range sensor. As expected, the angular resolution of laser rangefinders far exceeds that of ultrasonic sensors. The Sick laser scanner shown in Figure 4.11 achieves an angular resolution of0.5 degree.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 126,\n",
       "  'text_chunk': 'Depth resolution is approximately 5 cm, over a range from 5 cm up to 20 m or more, depending upon the brightness of the object being ranged. This device performs twenty five 180 degrees scans per second but has no mirror nodding capability for the ver-tical dimension. As with ultrasonic ranging sensors, an important error mode involves coherent reflection of the energy. With light, this will only occur when striking a highly polished surface.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 126,\n",
       "  'text_chunk': 'Prac-tically, a mobile robot may encounter such surfaces in the form of a polished desktop, file cabinet or, of course, a mirror. Unlike ultrasonic sensors, laser rangefinders cannot detect the presence of optically transparent materials such as glass, and this can be a significantobstacle in environments, for example, museums, where glass is commonly used. 4.1.6.2 Triangulation-based active ranging Triangulation-based ranging sensors use geometric properties manifest in their measuringstrategy to establish distance readings to objects.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 126,\n",
       "  'text_chunk': 'The simplest class of triangulation-basedFigure 4.12 Typical range image of a 2D laser range sensor with a rotating mirror. The length of the lines through the measurement points indicate the uncertainties.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 127,\n",
       "  'text_chunk': '112 Chapter 4 rangers are active because they project a known light pattern (e.g., a point, a line, or a tex- ture) onto the environment. The reflection of the known pattern is captured by a receiver and, together with known geometric values, the system can use simple triangulation to establish range measurements. If the receiver measures the position of the reflection along a single axis, we call the sensor an optical triangulation sensor in 1D.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 127,\n",
       "  'text_chunk': 'If the receiver mea-sures the position of the reflection along two orthogonal axes, we call the sensor a struc- tured light sensor. These two sensor types are described in the two sections below. Optical triangulation (1D sensor). The principle of optical triangulation in 1D is straightforward, as depicted in figure 4.13. A collimated beam (e.g., focused infrared LED, laser beam) is transmitted toward the target. The reflected light is collected by a lens and projected onto a position-sensitive device (PSD) or linear camera.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 127,\n",
       "  'text_chunk': 'Given the geometry of figure 4.13, the distance is given by (4.11) The distance is proportional to ; therefore the sensor resolution is best for close objects and becomes poor at a distance. Sensors based on this principle are used in rangesensing up to 1 or 2 m, but also in high-precision industrial measurements with resolutionsfar below 1 µm. Optical triangulation devices can provide relatively high accuracy with very good reso- lution (for close objects). However, the operating range of such a device is normally fairlylimited by geometry.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 127,\n",
       "  'text_chunk': 'For example, the optical triangulation sensor pictured in figure 4.14Figure 4.13 Principle of 1D laser triangulation.TargetD LLaser / Collimated beam Transmitted Beam Reflected BeamP Position-Sensitive Device (PSD) or Linear Camerax Lens DfL x--- = D DfL x--- = 1x⁄'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 128,\n",
       "  'text_chunk': 'Perception 113 operates over a distance range of between 8 and 80 cm. It is inexpensive compared to ultra- sonic and laser rangefinder sensors. Although more limited in range than sonar, the optical triangulation sensor has high bandwidth and does not suffer from cross-sensitivities that aremore common in the sound domain. Structured light (2D sensor).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 128,\n",
       "  'text_chunk': 'If one replaces the linear camera or PSD of an optical tri- angulation sensor with a 2D receiver such as a CCD or CMOS camera, then one can recover distance to a large set of points instead of to only one point. The emitter must project a known pattern, or structured light , onto the environment. Many systems exist which either project light textures (figure 4.15b) or emit collimated light (possibly laser) by means of a rotating mirror.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 128,\n",
       "  'text_chunk': 'Yet another popular alternative is to project a laser stripe (figure 4.15a) by turning a laser beam into a plane using a prism. Regardless of how it is created, the pro-jected light has a known structure, and therefore the image taken by the CCD or CMOS receiver can be filtered to identify the pattern’s reflection. Note that the problem of recovering depth is in this case far simpler than the problem of passive image analysis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 128,\n",
       "  'text_chunk': 'In passive image analysis, as we discuss later, existing features inthe environment must be used to perform correlation , while the present method projects a known pattern upon the environment and thereby avoids the standard correlation problem altogether. Furthermore, the structured light sensor is an active device so it will continue to work in dark environments as well as environments in which the objects are featureless (e.g., uniformly colored and edgeless). In contrast, stereo vision would fail in such texture-free circumstances. Figure 4.15c shows a 1D active triangulation geometry.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 128,\n",
       "  'text_chunk': 'We can examine the trade-off in the design of triangulation systems by examining the geometry in figure 4.15c. The mea-Figure 4.14 A commercially available, low-cost optical triangulation sensor: the Sharp GP series infrared rangefinders provide either analog or digital distance measures and cost only about $ 15.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 129,\n",
       "  'text_chunk': '114 Chapter 4 sured values in the system are α and u, the distance of the illuminated point from the origin in the imaging sensor. (Note the imaging sensor here can be a camera or an array of photo diodes of a position-sensitive device (e.g., a 2D PSD). From figure 4.15c, simple geometry shows that ; (4.12)Figure 4.15 (a) Principle of active two dimensional triangulation. (b) Other possible light structures. (c) 1D sche- matic of the principle.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 129,\n",
       "  'text_chunk': 'Image (a) and (b) courtesy of Albert-Jan Baerveldt, Halmstad University. H=D· tanαa) b)b u TargetbLaser / Collimated beam Transmitted Beam Reflected Beam(x, z)u LensCamera x zαfcotα-u f c) xbu⋅ fαu– cot---------------------- - = zbf⋅ fαu– cot---------------------- - ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 130,\n",
       "  'text_chunk': 'Perception 115 where is the distance of the lens to the imaging plane. In the limit, the ratio of image res- olution to range resolution is defined as the triangulation gain and from equation (4.12) is given by (4.13) This shows that the ranging accuracy, for a given image resolution, is proportional to source/detector separation and focal length , and decreases with the square of the range .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 130,\n",
       "  'text_chunk': 'In a scanning ranging system, there is an additional effect on the ranging accuracy, caused by the measurement of the projection angle . From equation 4.12 we see that (4.14) We can summarize the effects of the parameters on the sensor accuracy as follows: •Baseline length ( ): the smaller is, the more compact the sensor can be. The larger is, the better the range resolution will be. Note also that although these sensors do not suffer from the correspondence problem, the disparity problem still occurs.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 130,\n",
       "  'text_chunk': 'As the base- line length is increased, one introduces the chance that, for close objects, the illumi- nated point(s) may not be in the receiver’s field of view. •Detector length and focal length ( ) : A larger detector length can provide either a larger field of view or an improved range resolution or partial benefits for both. Increasing thedetector length, however, means a larger sensor head and worse electrical characteristics (increase in random error and reduction of bandwidth).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 130,\n",
       "  'text_chunk': 'Also, a short focal length gives a large field of view at the expense of accuracy, and vice versa. At one time, laser stripe-based structured light sensors were common on several mobile robot bases as an inexpensive alternative to laser rangefinding devices. However, with the increasing quality of laser rangefinding sensors in the 1990s, the structured light system has become relegated largely to vision research rather than applied mobile robotics.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 130,\n",
       "  'text_chunk': '4.1.7 Motion/speed sensors Some sensors measure directly the relative motion between the robot and its environment.Since such motion sensors detect relative motion, so long as an object is moving relative to the robot’s reference frame, it will be detected and its speed can be estimated. There are a number of sensors that inherently measure some aspect of motion or change. For example,a pyroelectric sensor detects change in heat.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 130,\n",
       "  'text_chunk': 'When a human walks across the sensor’s fieldf Gp u∂ z∂----- Gpbf⋅ z2--------- == b f z α α∂ z∂------ Gαb αsin2 z2---------------- - == bb b b f'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 131,\n",
       "  'text_chunk': '116 Chapter 4 of view, his or her motion triggers a change in heat in the sensor’s reference frame. In the next section, we describe an important type of motion detector based on the Doppler effect. These sensors represent a well-known technology with decades of general applications behind them. For fast-moving mobile robots such as autonomous highway vehicles andunmanned flying vehicles, Doppler-based motion detectors are the obstacle detection sensor of choice.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 131,\n",
       "  'text_chunk': '4.1.7.1 Doppler effect-based sensing (radar or sound) Anyone who has noticed the change in siren pitch that occurs when an approaching fire engine passes by and recedes is familiar with the Doppler effect. A transmitter emits an electromagnetic or sound wave with a frequency . It is either received by a receiver (figure 4.16a) or reflected from an object (figure 4.16b).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 131,\n",
       "  'text_chunk': 'The mea-sured frequency at the receiver is a function of the relative speed between transmitter and receiver according to (4.15) if the transmitter is moving and (4.16) if the receiver is moving. In the case of a reflected wave (figure 4.16b) there is a factor of 2 introduced, since any change x in relative separation affects the round-trip path length by .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 131,\n",
       "  'text_chunk': 'Furthermore, in such situations it is generally more convenient to consider the change in frequency , known as the Doppler shift , as opposed to the Doppler frequency notation above.ftFigure 4.16 Doppler effect between two moving objects (a) or a moving and a stationary object (b).Transmitter/ v ReceiverTransmitter vObjectReceivera) b) fr v frft1 1vc⁄+------------------ = frft1vc⁄+()= 2x ∆f'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 132,\n",
       "  'text_chunk': 'Perception 117 (4.17) (4.18) where = Doppler frequency shift; = relative angle between direction of motion and beam axis. The Doppler effect applies to sound and electromagnetic waves. It has a wide spectrum of applications: •Sound waves : for example, industrial process control, security, fish finding, measure of ground speed. •Electromagnetic waves : for example, vibration measurement, radar systems, object tracking. A current application area is both autonomous and manned highway vehicles.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 132,\n",
       "  'text_chunk': 'Both microwave and laser radar systems have been designed for this environment. Both systems have equivalent range, but laser can suffer when visual signals are deteriorated by environ- mental conditions such as rain, fog, and so on. Commercial microwave radar systems arealready available for installation on highway trucks. These systems are called VORAD (vehicle on-board radar) and have a total range of approximately 150 m. With an accuracy of approximately 97%, these systems report range rates from 0 to 160 km/hr with a resolu- tion of 1 km/hr.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 132,\n",
       "  'text_chunk': 'The beam is approximately 4 degrees wide and 5 degrees in elevation. One of the key limitations of radar technology is its bandwidth. Existing systems can provide information on multiple targets at approximately 2 Hz. 4.1.8 Vision-based sensors Vision is our most powerful sense. It provides us with an enormous amount of informationabout the environment and enables rich, intelligent interaction in dynamic environments. It is therefore not surprising that a great deal of effort has been devoted to providing machines with sensors that mimic the capabilities of the human vision system.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 132,\n",
       "  'text_chunk': 'The first step in thisprocess is the creation of sensing devices that capture the same raw information light that the human vision system uses. The next section describes the two current technologies for creating vision sensors: CCD and CMOS. These sensors have specific limitations in per- formance when compared to the human eye, and it is important that the reader understand these limitations.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 132,\n",
       "  'text_chunk': 'Afterward, the second and third sections describe vision-based sensors∆ff tfr–2ftv θcos c--------------------- == v∆fc⋅ 2ftθcos------------------= ∆f θ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 133,\n",
       "  'text_chunk': '118 Chapter 4 that are commercially available, like the sensors discussed previously in this chapter, along with their disadvantages and most popular applications. 4.1.8.1 CCD and CMOS sensors CCD technology. The charged coupled device is the most popular basic ingredient of robotic vision systems today. The CCD chip (see figure 4.17) is an array of light-sensitive picture elements, or pixels, usually with between 20,000 and several million pixels total.Each pixel can be thought of as a light-sensitive, discharging capacitor that is 5 to 25 µm in size.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 133,\n",
       "  'text_chunk': 'First, the capacitors of all pixels are charged fully, then the integration period begins. As photons of light strike each pixel, they liberate electrons, which are captured byelectric fields and retained at the pixel. Over time, each pixel accumulates a varying level of charge based on the total number of photons that have struck it. After the integration period is complete, the relative charges of all pixels need to be frozen and read. In a CCD,the reading process is performed at one corner of the CCD chip.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 133,\n",
       "  'text_chunk': 'The bottom row of pixel charges is transported to this corner and read, then the rows above shift down and the pro- cess is repeated. This means that each charge must be transported across the chip, and it iscritical that the value be preserved. This requires specialized control circuitry and custom fabrication techniques to ensure the stability of transported charges. The photodiodes used in CCD chips (and CMOS chips as well) are not equally sensitive to all frequencies of light.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 133,\n",
       "  'text_chunk': 'They are sensitive to light between 400 and 1000 nm wavelength.It is important to remember that photodiodes are less sensitive to the ultraviolet end of the spectrum (e.g., blue) and are overly sensitive to the infrared portion (e.g., heat).Figure 4.17 Commercially available CCD chips and CCD cameras. Because this technology is relatively mature, cameras are available in widely varying forms and costs (http://www.howstuffworks.com/digital- camera2.htm). 2048 x 2048 CCD array Cannon IXUS 300Sony DFW-X700 Orangemicro iBOT Firewire'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 134,\n",
       "  'text_chunk': 'Perception 119 You can see that the basic light-measuring process is colorless: it is just measuring the total number of photons that strike each pixel in the integration period. There are two common approaches for creating color images. If the pixels on the CCD chip are grouped into 2 x 2 sets of four, then red, green, and blue dyes can be applied to a color filter so that each individual pixel receives only light of one color.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 134,\n",
       "  'text_chunk': 'Normally, two pixels measure green while one pixel each measures red and blue light intensity. Of course, this one-chip color CCD has a geometric resolution disadvantage. The number of pixels in the system has beeneffectively cut by a factor of four, and therefore the image resolution output by the CCD camera will be sacrificed. The three-chip color camera avoids these problems by splitting the incoming light into three complete (lower intensity) copies.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 134,\n",
       "  'text_chunk': 'Three separate CCD chips receive the light, withone red, green, or blue filter over each entire chip. Thus, in parallel, each chip measures light intensity for one color, and the camera must combine the CCD chips’ outputs to createa joint color image. Resolution is preserved in this solution, although the three-chip color cameras are, as one would expect, significantly more expensive and therefore more rarely used in mobile robotics.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 134,\n",
       "  'text_chunk': 'Both three-chip and single-chip color CCD cameras suffer from the fact that photo- diodes are much more sensitive to the near-infrared end of the spectrum. This means thatthe overall system detects blue light much more poorly than red and green. To compensate,the gain must be increased on the blue channel, and this introduces greater absolute noise on blue than on red and green. It is not uncommon to assume at least one to two bits of addi- tional noise on the blue channel.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 134,\n",
       "  'text_chunk': 'Although there is no satisfactory solution to this problem today, over time the processes for blue detection have been improved and we expect this positive trend to continue. The CCD camera has several camera parameters that affect its behavior. In some cam- eras, these values are fixed. In others, the values are constantly changing based on built-in feedback loops. In higher-end cameras, the user can modify the values of these parameters via software. The iris position and shutter speed regulate the amount of light being mea- sured by the camera.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 134,\n",
       "  'text_chunk': 'The iris is simply a mechanical aperture that constricts incoming light, just as in standard 35 mm cameras. Shutter speed regulates the integration period of the chip. In higher-end cameras, the effective shutter speed can be as brief at 1/30,000 secondsand as long as 2 seconds. Camera gain controls the overall amplification of the analog sig- nal, prior to A/D conversion. However, it is very important to understand that, even thoughthe image may appear brighter after setting high gain, the shutter speed and iris may nothave changed at all.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 134,\n",
       "  'text_chunk': 'Thus gain merely amplifies the signal, and amplifies along with the signal all of the associated noise and error. Although useful in applications where imaging is done for human consumption (e.g., photography, television), gain is of little value to amobile roboticist.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 135,\n",
       "  'text_chunk': '120 Chapter 4 In color cameras, an additional control exists for white balance . Depending on the source of illumination in a scene (e.g., fluorescent lamps, incandescent lamps, sunlight, underwater filtered light, etc. ), the relative measurements of red, green, and blue light that define pure white light will change dramatically.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 135,\n",
       "  'text_chunk': 'The human eye compensates for all sucheffects in ways that are not fully understood, but, the camera can demonstrate glaring incon- sistencies in which the same table looks blue in one image, taken during the night, and yellow in another image, taken during the day. White balance controls enable the user tochange the relative gains for red, green, and blue in order to maintain more consistent color definitions in varying contexts. The key disadvantages of CCD cameras are primarily in the areas of inconstancy and dynamic range.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 135,\n",
       "  'text_chunk': 'As mentioned above, a number of parameters can change the brightnessand colors with which a camera creates its image. Manipulating these parameters in a way to provide consistency over time and over environments, for example, ensuring that a greenshirt always looks green, and something dark gray is always dark gray, remains an open problem in the vision community. For more details on the fields of color constancy and luminosity constancy, consult [40].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 135,\n",
       "  'text_chunk': 'The second class of disadvantages relates to the behavior of a CCD chip in environments with extreme illumination. In cases of very low illumination, each pixel will receive only asmall number of photons. The longest possible integration period (i.e., shutter speed) andcamera optics (i.e., pixel size, chip size, lens focal length and diameter) will determine the minimum level of light for which the signal is stronger than random error noise.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 135,\n",
       "  'text_chunk': 'In cases of very high illumination, a pixel fills its well with free electrons and, as the well reaches its limit, the probability of trapping additional electrons falls and therefore the linearity between incoming light and electrons in the well degrades. This is termed saturation and can indicate the existence of a further problem related to cross-sensitivity.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 135,\n",
       "  'text_chunk': 'When a well has reached its limit, then additional light within the remainder of the integration period may cause further charge to leak into neighboring pixels, causing them to report incorrect values or even reach secondary saturation. This effect, called blooming , means that individual pixel values are not truly independent. The camera parameters may be adjusted for an environment with a particular light level, but the problem remains that the dynamic range of a camera is limited by the well capacityof the individual pixels.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 135,\n",
       "  'text_chunk': 'For example, a high-quality CCD may have pixels that can hold 40,000 electrons. The noise level for reading the well may be 11 electrons, and therefore the dynamic range will be 40,000:11, or 3600:1, which is 35 dB. CMOS technology. The complementary metal oxide semiconductor chip is a significant departure from the CCD. It too has an array of pixels, but located alongside each pixel are several transistors specific to that pixel. Just as in CCD chips, all of the pixels accumulate charge during the integration period.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 135,\n",
       "  'text_chunk': 'During the data collection step, the CMOS takes a new'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 136,\n",
       "  'text_chunk': 'Perception 121 approach: the pixel-specific circuitry next to every pixel measures and amplifies the pixel’s signal, all in parallel for every pixel in the array. Using more traditional traces from general semiconductor chips, the resulting pixel values are all carried to their destinations. CMOS has a number of advantages over CCD technologies. First and foremost, there is no need for the specialized clock drivers and circuitry required in the CCD to transfer each pixel’s charge down all of the array columns and across all of its rows.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 136,\n",
       "  'text_chunk': 'This also means that specialized semiconductor manufacturing processes are not required to create CMOSchips. Therefore, the same production lines that create microchips can create inexpensive CMOS chips as well (see figure 4.18). The CMOS chip is so much simpler that it consumes significantly less power; incredibly, it operates with a power consumption that is one-hun-dredth the power consumption of a CCD chip. In a mobile robot, power is a scarce resource and therefore this is an important advantage.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 136,\n",
       "  'text_chunk': 'On the other hand, the CMOS chip also faces several disadvantages. Most importantly, the circuitry next to each pixel consumes valuable real estate on the face of the light-detect-ing array. Many photons hit the transistors rather than the photodiode, making the CMOS chip significantly less sensitive than an equivalent CCD chip. Second, the CMOS technol-ogy is younger and, as a result, the best resolution that one can purchase in CMOS format continues to be far inferior to the best CCD chips available.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 136,\n",
       "  'text_chunk': 'Time will doubtless bring the high-end CMOS imagers closer to CCD imaging performance. Given this summary of the mechanism behind CCD and CMOS chips, one can appreci- ate the sensitivity of any vision-based robot sensor to its environment. As compared to the human eye, these chips all have far poorer adaptation, cross-sensitivity, and dynamic range.As a result, vision sensors today continue to be fragile.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 136,\n",
       "  'text_chunk': 'Only over time, as the underlying performance of imaging chips improves, will significantly more robust vision-based sen- sors for mobile robots be available.Figure 4.18 A commercially available, low-cost CMOS camera with lens attached.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 137,\n",
       "  'text_chunk': '122 Chapter 4 Camera output considerations. Although digital cameras have inherently digital output, throughout the 1980s and early 1990s, most affordable vision modules provided analog output signals, such as NTSC (National Television Standards Committee) and PAL (Phase Alternating Line). These camera systems included a D/A converter which, ironically,would be counteracted on the computer using a framegrabber , effectively an A/D converter board situated, for example, on a computer’s bus.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 137,\n",
       "  'text_chunk': 'The D/A and A/D steps are far fromnoisefree, and furthermore the color depth of the analog signal in such cameras was opti-mized for human vision, not computer vision. More recently, both CCD and CMOS technology vision systems provide digital signals that can be directly utilized by the roboticist. At the most basic level, an imaging chip pro-vides parallel digital I/O (input/output) pins that communicate discrete pixel level values.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 137,\n",
       "  'text_chunk': 'Some vision modules make use of these direct digital signals, which must be handled sub- ject to hard-time constraints governed by the imaging chip. To relieve the real-timedemands, researchers often place an image buffer chip between the imager’s digital output and the computer’s digital inputs. Such chips, commonly used in webcams, capture a com-plete image snapshot and enable non real time access to the pixels, usually in a single,ordered pass.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 137,\n",
       "  'text_chunk': 'At the highest level, a roboticist may choose instead to utilize a higher-level digital transport protocol to communicate with an imager. Most common are the IEEE 1394(Firewire ) standard and the USB (and USB 2.0) standards, although some older imaging modules also support serial (RS-232). To use any such high-level protocol, one must locate or create driver code both for that communication layer and for the particular implementa- tion details of the imaging chip.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 137,\n",
       "  'text_chunk': 'Take note, however, of the distinction between lossless digital video and the standard digital video stream designed for human visual consumption. Most digital video cameras provide digital output, but often only in compressed form. Forvision researchers, such compression must be avoided as it not only discards information but even introduces image detail that does not actually exist, such as MPEG (Moving Pic- ture Experts Group) discretization boundaries. 4.1.8.2 Visual ranging sensors Range sensing is extremely important in mobile robotics as it is a basic input for successful obstacle avoidance.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 137,\n",
       "  'text_chunk': 'As we have seen earlier in this chapter, a number of sensors are popularin robotics explicitly for their ability to recover depth estimates: ultrasonic, laser rangefinder, optical rangefinder, and so on. It is natural to attempt to implement ranging functionality using vision chips as well. However, a fundamental problem with visual images makes rangefinding relatively dif- ficult. Any vision chip collapses the 3D world into a 2D image plane, thereby losing depth information.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 137,\n",
       "  'text_chunk': 'If one can make strong assumptions regarding the size of objects in the world, or their particular color and reflectance, then one can directly interpret the appearance of the 2D image to recover depth. But such assumptions are rarely possible in real-world'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 138,\n",
       "  'text_chunk': 'Perception 123 mobile robot applications. Without such assumptions, a single picture does not provide enough information to recover spatial information. The general solution is to recover depth by looking at several images of the scene to gain more information, hopefully enough to at least partially recover depth. The images usedmust be different, so that taken together they provide additional information. They could differ in viewpoint, yielding stereo or motion algorithms.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 138,\n",
       "  'text_chunk': 'An alternative is to create differ- ent images, not by changing the viewpoint, but by changing the camera geometry, such as the focus position or lens iris. This is the fundamental idea behind depth from focus and depth from defocus techniques. In the next section, we outline the general approach to the depth from focus techniques because it presents a straightforward and efficient way to create a vision-based range sen- sor. Subsequently, we present details for the correspondence-based techniques of depth from stereo and motion. Depth from focus.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 138,\n",
       "  'text_chunk': 'The depth from focus class of techniques relies on the fact that image properties not only change as a function of the scene but also as a function of the camera parameters. The relationship between camera parameters and image properties is depicted in figure 4.19.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 138,\n",
       "  'text_chunk': 'The basic formula governing image formation relates the distance of the object from the lens, in the above figure, to the distance from the lens to the focal point, based on the focal length of the lens:Figure 4.19 Depiction of the camera optics and its impact on the image. In order to get a sharp image, the image plane must coincide with the focal plane.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 138,\n",
       "  'text_chunk': 'Otherwise the image of the point ( x,y,z) will be blurred in the image, as can be seen in the drawing above. focal plane f(xl, yl)(x, y, z) imag e plane d eδ de f'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 139,\n",
       "  'text_chunk': '124 Chapter 4 (4.19) If the image plane is located at distance from the lens, then for the specific object voxel depicted, all light will be focused at a single point on the image plane and the object voxel will be focused . However, when the image plane is not at , as is depicted in figure 4.19, then the light from the object voxel will be cast on the image plane as a blur circle .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 139,\n",
       "  'text_chunk': 'To a first approximation, the light is homogeneously distributed throughout this blur circle, and the radius of the circle can be characterized according to the equation (4.20) is the diameter of the lens or aperture and is the displacement of the image plan from the focal point. Given these formulas, several basic optical effects are clear. For example, if the aperture or lens is reduced to a point, as in a pinhole camera, then the radius of the blur circle approaches zero.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 139,\n",
       "  'text_chunk': 'This is consistent with the fact that decreasing the iris aperture opening causes the depth of field to increase until all objects are in focus. Of course, the disadvan- tage of doing so is that we are allowing less light to form the image on the image plane and so this is practical only in bright circumstances. The second property that can be deduced from these optics equations relates to the sen- sitivity of blurring as a function of the distance from the lens to the object.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 139,\n",
       "  'text_chunk': 'Suppose the image plane is at a fixed distance 1.2 from a lens with diameter and focal length . We can see from equation (4.20) that the size of the blur circle changes pro- portionally with the image plane displacement . If the object is at distance , then from equation (4.19) we can compute and therefore = 0.2. Increase the object dis- tance to and as a result = 0.533. Using equation (4.20) in each case we can com- pute and respectively.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 139,\n",
       "  'text_chunk': 'This demonstrates high sensitivity for defocusing when the object is close to the lens. In contrast, suppose the object is at . In this case we compute . But if the object is again moved one unit, to , then we compute . The resultingblur circles are and , far less than the quadrupling in when the obstacle is one-tenth the distance from the lens. This analysis demonstrates the fundamentallimitation of depth from focus techniques: they lose sensitivity as objects move farther away (given a fixed focal length).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 139,\n",
       "  'text_chunk': 'Interestingly, this limitation will turn out to apply to vir- tually all visual ranging techniques, including depth from stereo and depth from motion. Nevertheless, camera optics can be customized for the depth range of the intended appli- cation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 139,\n",
       "  'text_chunk': 'For example, a zoom lens with a very large focal length will enable range resolu-1 f---1 d---1 e---+= e e R RLδ 2e------= L δ L 0.2= f0.5= R δ d 1= e 1= δ d 2= δ R 0.02= R 0.08= d 10= e 0.526 = d 11= e 0.524 = R 0.117 = R 0.129 = R f'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 140,\n",
       "  'text_chunk': 'Perception 125 tion at significant distances, of course at the expense of field of view. Similarly, a large lens diameter, coupled with a very fast shutter speed, will lead to larger, more detectable blur circles. Given the physical effects summarized by the above equations, one can imagine a visual ranging sensor that makes use of multiple images in which camera optics are varied (e.g., image plane displacement ) and the same scene is captured (see figure 4.20). In fact, thisapproach is not a new invention.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 140,\n",
       "  'text_chunk': 'The human visual system uses an abundance of cues and techniques, and one system demonstrated in humans is depth from focus. Humans vary the focal length of their lens continuously at a rate of about 2 Hz. Such approaches, in whichthe lens optics are actively searched in order to maximize focus, are technically called depth from focus . In contrast, depth from defocus means that depth is recovered using a series of images that have been taken with different camera geometries.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 140,\n",
       "  'text_chunk': 'The depth from focus method is one of the simplest visual ranging techniques. To deter- mine the range to an object, the sensor simply moves the image plane (via focusing) until maximizing the sharpness of the object. When the sharpness is maximized, the correspond- ing position of the image plane directly reports range. Some autofocus cameras and virtu- ally all autofocus video cameras use this technique. Of course, a method is required for measuring the sharpness of an image or an object within the image.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 140,\n",
       "  'text_chunk': 'The most common tech-niques are approximate measurements of the subimage intensity () gradient : (4.21) (4.22)δFigure 4.20 Two images of the same scene taken with a camera at two different focusing positions. Note the sig- nificant change in texture sharpness between the near surface and far surface. The scene is an outdoor concrete step. I sharpness1 Ixy,() Ix 1–y, ()– xy,∑= sharpness2 Ixy,() Ix 2–y2–, ()– ()2 xy,∑='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 141,\n",
       "  'text_chunk': '126 Chapter 4 A significant advantage of the horizontal sum of differences technique [equation (4.21)] is that the calculation can be implemented in analog circuitry using just a rectifier, a low- pass filter, and a high-pass filter. This is a common approach in commercial cameras andvideo recorders. Such systems will be sensitive to contrast along one particular axis, although in practical terms this is rarely an issue.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 141,\n",
       "  'text_chunk': 'However depth from focus is an active search method and will be slow because it takes time to change the focusing parameters of the camera, using, for example, a servo-con- trolled focusing ring. For this reason this method has not been applied to mobile robots. A variation of the depth from focus technique has been applied to a mobile robot, dem- onstrating obstacle avoidance in a variety of environments, as well as avoidance of concave obstacles such as steps and ledges [117].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 141,\n",
       "  'text_chunk': 'This robot uses three monochrome cameras placed as close together as possible with different, fixed lens focus positions (figure 4.21). Several times each second, all three frame-synchronized cameras simultaneously cap- ture three images of the same scene. The images are each divided into five columns andthree rows, or fifteen subregions. The approximate sharpness of each region is computedusing a variation of equation (4.22), leading to a total of forty-five sharpness values. Note that equation (4.22) calculates sharpness along diagonals but skips one row.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 141,\n",
       "  'text_chunk': 'This is due to a subtle but important issue. Many cameras produce images in interlaced mode. This meansFigure 4.21 The Cheshm robot uses three monochrome cameras as its only ranging sensor for obstacle avoidance in the context of humans, static obstacles such as bushes, and convex obstacles such as ledges and steps.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 142,\n",
       "  'text_chunk': 'Perception 127 that the odd rows are captured first, then afterward the even rows are captured. When such a camera is used in dynamic environments, for example, on a moving robot, then adjacent rows show the dynamic scene at two different time points, differing by up to one-thirtieth of a second. The result is an artificial blurring due to motion and not optical defocus. Bycomparing only even-numbered rows we avoid this interlacing side effect.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 142,\n",
       "  'text_chunk': 'Recall that the three images are each taken with a camera using a different focus posi- tion. Based on the focusing position, we call each image close ,medium or far. A 5 x 3 coarse depth map of the scene is constructed quickly by simply comparing the sharpness values of each of the three corresponding regions. Thus, the depth map assigns only two bits of depth information to each region using the values close, medium, and far.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 142,\n",
       "  'text_chunk': 'The crit- ical step is to adjust the focus positions of all three cameras so that flat ground in front of the obstacle results in medium readings in one row of the depth map. Then, unexpected readings of either close or far will indicate convex and concave obstacles respectively, enabling basic obstacle avoidance in the vicinity of objects on the ground as well as drop- offs into the ground. Although sufficient for obstacle avoidance, the above depth from focus algorithm pre- sents unsatisfyingly coarse range information.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 142,\n",
       "  'text_chunk': 'The alternative is depth from defocus , the most desirable of the focus-based vision techniques. Depth from defocus methods take as input two or more images of the same scene, taken with different, known camera geometry. Given the images and the camera geometry set- tings, the goal is to recover the depth information of the 3D scene represented by the images.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 142,\n",
       "  'text_chunk': 'We begin by deriving the relationship between the actual scene properties (irradi- ance and depth), camera geometry settings, and the image g that is formed at the image plane. Thefocused image of a scene is defined as follows. Consider a pinhole aperture () in lieu of the lens. For every point at position on the image plane, draw a line through the pinhole aperture to the corresponding, visible point P in the actual scene.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 142,\n",
       "  'text_chunk': 'We define as the irradiance (or light intensity) at due to the light from . Intu-itively, represents the intensity image of the scene perfectly in focus. Thepoint spread function is defined as the amount of irradiance from point in the scene (corresponding to in the focused image that contributesto point in the observed, defocused image . Note that the point spread function depends not only upon the source, , and the target, , but also on , the blur circle radius.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 142,\n",
       "  'text_chunk': ', in turn, depends upon the distance from point to the lens, as can be seenby studying equations (4.19) and (4.20). Given the assumption that the blur circle is homogeneous in intensity, we can define as follows:fxy,() L 0= p xy,() fxy,() p P fxy,() hxgygxfyfRxy, ,,,, () P xfyf,() f xgyg,() g xfyf,() xgyg,() R R P h'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 143,\n",
       "  'text_chunk': '128 Chapter 4 (4.23) Intuitively, point contributes to the image pixel only when the blur circle of point contains the point . Now we can write the general formula that computes the value of each pixel in the image, , as a function of the point spread function and the focused image: (4.24) This equation relates the depth of scene points via to the observed image . Solving for would provide us with the depth map. However, this function has another unknown, and that is , the focused image.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 143,\n",
       "  'text_chunk': 'Therefore, one image alone is insufficient to solve the depth recovery problem, assuming we do not know how the fully focused image would look. Given two images of the same scene, taken with varying camera geometry, in theory it will be possible to solve for as well as because stays constant. There are a number of algorithms for implementing such a solution accurately and quickly. The classic approach is known as inverse filtering because it attempts to directly solve for , then extract depth information from this solution.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 143,\n",
       "  'text_chunk': 'One special case of the inverse filtering solu- tion has been demonstrated with a real sensor. Suppose that the incoming light is split andsent to two cameras, one with a large aperture and the other with a pinhole aperture [121]. The pinhole aperture results in a fully focused image, directly providing the value of . With this approach, there remains a single equation with a single unknown, and so the solu-tion is straightforward.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 143,\n",
       "  'text_chunk': 'Pentland [121] has demonstrated such a sensor, with several meters of range and better than 97% accuracy. Note, however, that the pinhole aperture necessi- tates a large amount of incoming light, and that furthermore the actual image intensitiesmust be normalized so that the pinhole and large-diameter images have equivalent total radiosity. More recent depth from defocus methods use statistical techniques and charac- terization of the problem as a set of linear equations [64]. These matrix-based methods have recently achieved significant improvements in accuracy over all previous work.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 143,\n",
       "  'text_chunk': 'In summary, the basic advantage of the depth from defocus method is its extremely fast speed. The equations above do not require search algorithms to find the solution, as would the correlation problem faced by depth from stereo methods.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 143,\n",
       "  'text_chunk': 'Perhaps more importantly, the depth from defocus methods also need not capture the scene at different perspectives, and are therefore unaffected by occlusions and the disappearance of objects in a second view.hx gygxfyfRxy, ,,,,()1 πR2--------- if xgxf–()2ygyf–()2+ () R2≤ 0if xgxf–()2ygyf–()2+ () R2>= P xgyg,() P xgyg,() fxy,() gxgyg,() hxgygxyRxy, ,,,, () fxy,() xy,∑= R g R f g R f R f'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 144,\n",
       "  'text_chunk': 'Perception 129 As with all visual methods for ranging, accuracy decreases with distance. Indeed, the accuracy can be extreme; these methods have been used in microscopy to demonstrate ranging at the micrometer level. Stereo vision. Stereo vision is one of several techniques in which we recover depth infor- mation from two images that depict the scene from different perspectives. The theory of depth from stereo has been well understood for years, while the engineering challenge of creating a practical stereo sensor has been formidable [16, 29, 30].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 144,\n",
       "  'text_chunk': 'Recent times have seen the first successes on this front, and so after presenting a basic formalism of stereo ranging, we describe the state-of-the-art algorithmic approach and one of the recent, commercially available stereo sensors. First, we consider a simplified case in which two cameras are placed with their optical axes parallel, at a separation (called the baseline ) of b, shown in figure 4.22.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 144,\n",
       "  'text_chunk': 'In this figure, a point on the object is described as being at coordinate with respect to a central origin located between the two camera lenses. The position of thisFigure 4.22 Idealized camera geometry for stereo vision.objects contour focal planeorigin f b/2 b/2(xl, yl) (xr, yr)(x, y, z) lens r lens l x zy xy z,,()'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 145,\n",
       "  'text_chunk': '130 Chapter 4 point’s light rays on each camera’s image is depicted in camera-specific local coordinates. Thus, the origin for the coordinate frame referenced by points of the form ( ) is located at the center of lens . From the figure 4.22, it can be seen that and (4.25) and (out of the plane of the page) (4.26) where is the distance of both lenses to the image plane.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 145,\n",
       "  'text_chunk': 'Note from equation (4.25) that (4.27) where the difference in the image coordinates, is called the disparity. This is an important term in stereo vision, because it is only by measuring disparity that we can recover depth information. Using the disparity and solving all three above equations pro- vides formulas for the three dimensions of the scene point being imaged: ; ; (4.28) Observations from these equations are as follows: •Distance is inversely proportional to disparity.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 145,\n",
       "  'text_chunk': 'The distance to near objects can therefore be measured more accurately than that to distant objects, just as with depth from focus techniques. In general, this is acceptable for mobile robotics, because for navigation and obstacle avoidance closer objects are of greater importance. •Disparity is proportional to . For a given disparity error, the accuracy of the depth esti- mate increases with increasing baseline . •As b is increased, because the physical separation between the cameras is increased, some objects may appear in one camera but not in the other.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 145,\n",
       "  'text_chunk': 'Such objects by definitionwill not have a disparity and therefore will not be ranged successfully.x lyl, l xl f----xb 2⁄+ z------------------ - =xr f----xb 2⁄– z------------------= yl f----yr f----y z-- == f xlxr– f--------------b z---= xlxr– xbxlxr+() 2⁄ xlxr–--------------------------= ybylyr+() 2⁄ xlxr–--------------------------= zbf xlxr–--------------= b b'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 146,\n",
       "  'text_chunk': 'Perception 131 •A point in the scene visible to both cameras produces a pair of image points (one via each lens) known as a conjugate pair . Given one member of the conjugate pair, we know that the other member of the pair lies somewhere along a line known as an epipolar line . In the case depicted by figure 4.22, because the cameras are perfectly aligned with one another, the epipolar lines are horizontal lines (i.e., along the direction).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 146,\n",
       "  'text_chunk': 'However, the assumption of perfectly aligned cameras is normally violated in practice. In order to optimize the range of distances that can be recovered, it is often useful to turn the cameras inward toward one another, for example. Figure 4.22 shows the orientation vectors that are necessary to solve this more general problem. We will express the positionof a scene point in terms of the reference frame of each camera separately.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 146,\n",
       "  'text_chunk': 'The referenceframes of the cameras need not be aligned, and can indeed be at any arbitrary orientation relative to one another. For example the position of point will be described in terms of the left camera frame as . Note that these are the coordinates of point , not the position of its counterpart in the left camera image. can also be described in terms of the right camera frame as .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 146,\n",
       "  'text_chunk': 'If we have a rotation matrix and translation matrix relat-ing the relative positions of cameras l and r, then we can define in terms of : (4.29) where is a 3 x 3 rotation matrix and is an offset translation matrix between the two cameras. Expanding equation (4.29) yields (4.30) The above equations have two uses: 1.We could find if we knew R, and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 146,\n",
       "  'text_chunk': 'Of course, if we knew then we would have complete information regarding the position of relative to the left camera, and so the depth recovery problem would be solved. Note that, for perfectly aligned cameras as in figure 4.22, (the identify matrix).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 146,\n",
       "  'text_chunk': \"2.We could calibrate the system and find r 11, r12 … given a set of conjugate pairs .x P P r'l x'ly'lz'l ,,()= P P r'r x'ry'rz'r ,,()= R r0 r'r r'l r'rRr'lr0+⋅ = R r0 x'r y'r z'rr11r12r13 r21r22r21 r31r32r33x'l y'l z'lr01 r02 r03+ = r'r r'l r0 r'l P RI= x'ly'lz'l ,,() x'ry'rz'r ,,(), {}\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 147,\n",
       "  'text_chunk': '132 Chapter 4 In order to carry out the calibration step of step 2 above, we must find values for twelve unknowns, requiring twelve equations. This means that calibration requires, for a given scene, four conjugate points. The above example supposes that regular translation and rotation are all that are required to effect sufficient calibration for stereo depth recovery using two cameras. In fact, single-camera calibration is itself an active area of research, particularly when the goal includes any 3D recovery aspect.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 147,\n",
       "  'text_chunk': 'When researchers intend to use even a single camera with high pre-cision in 3D, internal errors relating to the exact placement of the imaging chip relative to the lens optical axis, as well as aberrations in the lens system itself, must be calibrated against. Such single-camera calibration involves finding solutions for the values for theexact offset of the imaging chip relative to the optical axis, both in translation and angle, and finding the relationship between distance along the imaging chip surface and external viewed surfaces.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 147,\n",
       "  'text_chunk': 'Furthermore, even without optical aberration in play, the lens is an inher-ently radial instrument, and so the image projected upon a flat imaging surface is radially distorted (i.e., parallel lines in the viewed world converge on the imaging chip). A commonly practiced technique for such single-camera calibration is based upon acquiring multiple views of an easily analyzed planar pattern, such as a grid of blacksquares on a white background.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 147,\n",
       "  'text_chunk': 'The corners of such squares can easily be extracted, and using an interactive refinement algorithm the intrinsic calibration parameters of a cameracan be extracted. Because modern imaging systems are capable of spatial accuracy greatly exceeding the pixel size, the payoff of such refined calibration can be significant. For fur- ther discussion of calibration and to download and use a standard calibration program, see [158]. Assuming that the calibration step is complete, we can now formalize the range recovery problem.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 147,\n",
       "  'text_chunk': 'To begin with, we do not have the position of P available, and therefore and are unknowns. Instead, by virtue of the two cameras we have pixels on the image planes of each camera, and . Given the focallength of the cameras we can relate the position of to the left camera image as follows: and (4.31) Let us concentrate first on recovery of the values and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 147,\n",
       "  'text_chunk': \"From equations (4.30) and (4.31) we can compute these values from any two of the following equations: (4.32)x' ly'lz'l ,,() x'ry'rz'r ,,() xlylzl ,,() xryrzr ,,() f P xl f----x'l z'l-----=yl f----y'l z'l-----= z'l z'r r11xl f----r12yl f----r13 ++\\uf8ed\\uf8f8\\uf8eb\\uf8f6z'lr01+xr f----z'r =\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 148,\n",
       "  'text_chunk': 'Perception 133 (4.33) (4.34) The same process can be used to identify values for and , yielding complete infor- mation about the position of point . However, using the above equations requires us to have identified conjugate pairs in the left and right camera images: image points that orig- inate at the same object point in the scene. This fundamental challenge, identifying theconjugate pairs and thereby recovering disparity, is the correspondence problem .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 148,\n",
       "  'text_chunk': 'Intu- itively, the problem is, given two images of the same scene from different perspectives,how can we identify the same object points in both images? For every such identified objectpoint, we will be able to recover its 3D position in the scene. The correspondence problem, or the problem of matching the same object in two differ- ent inputs, has been one of the most challenging problems in the computer vision field andthe artificial intelligence fields.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 148,\n",
       "  'text_chunk': 'The basic approach in nearly all proposed solutions involves converting each image in order to create more stable and more information-rich data. With more reliable data in hand, stereo algorithms search for the best conjugate pairs representing as many of the images’ pixels as possible. The search process is well understood, but the quality of the resulting depth maps depends heavily upon the way in which images are treated to reduce noise and improve sta-bility.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 148,\n",
       "  'text_chunk': 'This has been the chief technology driver in stereo vision algorithms, and one par- ticular method has become widely used in commercially available systems. The zero crossings of Laplacian of Gaussian (ZLoG). ZLoG is a strategy for identify- ing features in the left and right camera images that are stable and will match well, yielding high-quality stereo depth recovery. This approach has seen tremendous success in the fieldof stereo vision, having been implemented commercially in both software and hardware with good results.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 148,\n",
       "  'text_chunk': 'It has led to several commercial stereo vision systems and yet it is extremely simple. Here we summarize the approach and explain some of its advantages. The core of ZLoG is the Laplacian transformation of an image. Intuitively, this is noth- ing more than the second derivative.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 148,\n",
       "  'text_chunk': \"Formally, the Laplacian of an image withintensities is defined as (4.35)r 21xl f----r22yl f----r23 ++\\uf8ed\\uf8f8\\uf8eb\\uf8f6z'lr02+yr f----z'r = r31xl f----r32yl f----r33 ++\\uf8ed\\uf8f8\\uf8eb\\uf8f6z'lr03+ z'r= x' y' P P Lx y,() Ixy,() Lxy,() x22 ∂∂I y22 ∂∂I+ =\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 149,\n",
       "  'text_chunk': '134 Chapter 4 So the Laplacian represents the second derivative of the image, and is computed along both axes. Such a transformation, called a convolution , must be computed over the discrete space of image pixel values, and therefore an approximation of equation (4.35) is required for application: (4.36) We depict a discrete operator , called a kernel , that approximates the second derivative operation along both axes as a 3 x 3 table: (4.37) Application of the kernel to convolve an image is straightforward.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 149,\n",
       "  'text_chunk': 'The kernel defines the contribution of each pixel in the image to the corresponding pixel in the target as well as its neighbors. For example, if a pixel (5,5) in the image has value , then application of the kernel depicted by equation (4.37) causes pixel to make the fol-lowing contributions to the target image : += -40; += 10; += 10; += 10; += 10.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 149,\n",
       "  'text_chunk': 'Now consider the graphic example of a step function, representing a pixel row in which the intensities are dark, then suddenly there is a jump to very bright intensities. The second derivative will have a sharp positive peak followed by a sharp negative peak, as depictedin figure 4.23. The Laplacian is used because of this extreme sensitivity to changes in the image. But the second derivative is in fact oversensitive.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 149,\n",
       "  'text_chunk': 'We would like the Laplacian to trigger large peaks due to real changes in the scene’s intensities, but we would like to keepsignal noise from triggering false peaks. For the purpose of removing noise due to sensor error, the ZLoG algorithm applies Gaussian smoothing first, then executes the Laplacian convolution.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 149,\n",
       "  'text_chunk': 'Such smoothing can be effected via convolution with a table that approximates Gaussian smoothing:LP I⊗ = P 010 14–1 010 P I I55,() 10= I55,() L L55,() L45,() L65,() L54,() L56,() 33×'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 150,\n",
       "  'text_chunk': 'Perception 135 (4.38) Gaussian smoothing does not really remove error; it merely distributes image variations over larger areas. This should seem familiar. In fact, Gaussian smoothing is almost identical to the blurring caused by defocused optics. It is, nonetheless, very effective at removinghigh-frequency noise, just as blurring removes fine-grained detail. Note that, like defocus- ing, this kernel does not change the total illumination but merely redistributes it (by virtue of the divisor 16).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 150,\n",
       "  'text_chunk': 'The result of Laplacian of Gaussian (LoG) image filtering is a target array with sharp positive and negative spikes identifying boundaries of change in the original image. Forexample, a sharp edge in the image will result in both a positive spike and a negative spike,located on either side of the edge. To solve the correspondence problem, we would like to identify specific features in LoG that are amenable to matching between the left camera and right camera filtered images.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 150,\n",
       "  'text_chunk': 'A very effective feature has been to identify each zero crossing of the LoG as such a feature.Figure 4.23 Step function example of second derivative shape and the impact of noise. 1 16------2 16------1 16------ 2 16------4 16------2 16------ 1 16------2 16------1 16------'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 151,\n",
       "  'text_chunk': '136 Chapter 4 Many zero crossings do lie at edges in images, but their occurrence is somewhat broader than that. An interesting characteristic of zero crossings is that they are very sharply defined, covering just one “pixel” width in the filtered image. The accuracy can even befurther enhanced by using interpolation to establish the position of the zero crossing with subpixel accuracy. All told, the accuracy of the zero crossing features in ZLoG have made them the preferred features in state-of-the-art stereo depth recovery algorithms.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 151,\n",
       "  'text_chunk': 'Figure 4.24 shows on an example the various steps required to extract depth information from a stereo image. Several commercial stereo vision depth recovery sensors have been available for researchers over the past 10 years. A popular unit in mobile robotics today is the digital stereo head (or SVM) from Videre Design shown in figure 4.25. The SVM uses the LoG operator, following it by tessellating the resulting array into sub- regions within which the sum of absolute values is computed.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 151,\n",
       "  'text_chunk': 'The correspondence problemis solved at the level of these subregions, a process called area correlation, and after cor- respondence is solved the results are interpolated to one-fourth pixel precision. An impor- tant feature of the SVM is that it produces not just a depth map but distinct measures ofFigure 4.25 The SVM module mounted on EPFL’s Shrimp robot.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 152,\n",
       "  'text_chunk': 'Perception 137 Figure 4.24 Extracting depth information from a stereo image. (a1 and a2) Left and right image. (b1 and b2) Ver- tical edge filtered left and right image: filter = [1 2 4 -2 -10 -2 4 2 1]. (c) Confidence image: bright = high confidence (good texture); dark = low confidence (no texture). (d) Depth image (dispar- ity): bright = close; dark = far. a1 a2 b1 b2 c d'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 153,\n",
       "  'text_chunk': '138 Chapter 4 match quality for each pixel. This is valuable because such additional information can be used over time to eliminate spurious, incorrect stereo matches that have poor match quality. The performance of SVM provides a good representative of the state of the art in stereo ranging today. The SVM consists of sensor hardware, including two CMOS cameras andDSP (Digital Signal Processor) hardware. In addition, the SVM includes stereo vision soft- ware that makes use of a standard computer (e.g., a Pentium processor).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 153,\n",
       "  'text_chunk': 'On a 320 x 240 pixel image pair, the SVM assigns one of thirty-two discrete levels of disparity (i.e., depth)to every pixel at a rate of twelve frames per second (based on the speed of a 233 MHz Pen- tium II). This compares favorably to both laser rangefinding and ultrasonics, particularly when one appreciates that ranging information with stereo is being computed for not justone target point, but all target points in the image.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 153,\n",
       "  'text_chunk': 'It is important to note that the SVM uses CMOS chips rather than CCD chips, demon- strating that resolution sufficient for stereo vision algorithms is readily available using theless expensive, power efficient CMOS technology. The resolution of a vision-based ranging system will depend upon the range to the object, as we have stated before. It is instructive to observe the published resolution valuesfor the SVM sensor.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 153,\n",
       "  'text_chunk': 'Although highly dependent on the camera optics, using a standard 6mm focal length lens pair, the SVM claims a resolution of 10 mm at 3 m range, and a res- olution of 60 mm at 10 m range. These values are based on ideal circumstances, but never- theless exemplify the rapid loss in resolution that will accompany vision-based ranging. 4.1.8.3 Motion and optical flow A great deal of information can be recovered by recording time-varying images from afixed (or moving) camera.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 153,\n",
       "  'text_chunk': 'First, we distinguish between the motion field and optical flow: •Motion field: this assigns a velocity vector to every point in an image. If a point in the environment moves with velocity , then this induces a velocity in the image plane.It is possible to determine mathematically the relationship between and . •Optical flow: it can also be true that brightness patterns in the image move as the object that causes them moves (light source). Optical flow is the apparent motion of these brightness patterns.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 153,\n",
       "  'text_chunk': 'In our analysis here we assume that the optical flow pattern will correspond to the motion field, although this is not always true in practice. This is illustrated in figure 4.26a where a sphere exhibits spatial variation of brightness, or shading, in the image of the sphere since its surface is curved. If the surface moves, however, this shading pattern will not move hence the optical flow is zero everywhere even though the motion field is notzero. In figure 4.26b, the opposite occurs.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 153,\n",
       "  'text_chunk': 'Here we have a fixed sphere with a moving light source. The shading in the image will change as the source moves. In this case the opticalv 0 vi vi v0'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 154,\n",
       "  'text_chunk': 'Perception 139 flow is nonzero but the motion field is zero. If the only information accessible to us is the optical flow and we depend on this, we will obtain incorrect results in both cases. Optical Flow. There are a number of techniques for attempting to measure optical flow and thereby obtain the scene’s motion field. Most algorithms use local information, attempting to find the motion of a local patch in two consecutive images.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 154,\n",
       "  'text_chunk': 'In some cases,global information regarding smoothness and consistency can help to further disambiguate such matching processes. Below we present details for the optical flow constraint equation method. For more details on this and other methods refer to [41, 77, 146]. Suppose first that the time interval between successive snapshots is so fast that we can assume that the measured intensity of a portion of the same object is effectively constant. Mathematically, let be the image irradiance at time t at the image point .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 154,\n",
       "  'text_chunk': 'If and are the and components of the optical flow vector at that point, we need to search a new image for a point where the irradiance will be the same at time , that is, at point , where and . That is, (4.39) for a small time interval, . This will capture the motion of a constant-intensity patch through time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 154,\n",
       "  'text_chunk': 'If we further assume that the brightness of the image varies smoothly, then we can expand the left hand side of equation (4.39) as a Taylor series to obtain (4.40) where e contains second- and higher-order terms in , and so on. In the limit as tends to zero we obtainFigure 4.26 Motion of the sphere or the light source here demonstrates that optical flow is not always the same as the motion field.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 154,\n",
       "  'text_chunk': 'b) a) Exy t,,() xy,() ux y,() vxy,() xy ttδ+ xtδ+ ytδ+, () xδ utδ= yδ vtδ= Exu tδ+ yv t δ+ ttδ+, (, ) Exyt,,()= tδ Exyt,,() xE∂ x∂------ yE∂ y∂------ tE∂ t∂------ e+ δ+ δ+ δ+ Exyt,,()= xδ tδ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 155,\n",
       "  'text_chunk': '140 Chapter 4 (4.41) from which we can abbreviate ; (4.42) and ; ; (4.43) so that we obtain (4.44) The derivative represents how quickly the intensity changes with time while the derivatives and represent the spatial rates of intensity change (how quickly intensity changes across the image). Altogether, equation (4.44) is known as the optical flow con- straint equation and the three derivatives can be estimated for each pixel given successive images.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 155,\n",
       "  'text_chunk': 'We need to calculate both u and v for each pixel, but the optical flow constraint equation only provides one equation per pixel, and so this is insufficient. The ambiguity is intuitively clear when one considers that a number of equal-intensity pixels can be inherently ambig- uous – it may be unclear which pixel is the resulting location for an equal-intensity origi-nating pixel in the prior image. The solution to this ambiguity requires an additional constraint.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 155,\n",
       "  'text_chunk': 'We assume that in gen- eral the motion of adjacent pixels will be similar, and that therefore the overall optical flowof all pixels will be smooth. This constraint is interesting in that we know it will be violated tosome degree, but we enforce the constraint nonetheless in order to make the optical flow computation tractable. Specifically, this constraint will be violated precisely when different objects in the scene are moving in different directions with respect to the vision system.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 155,\n",
       "  'text_chunk': 'Of course, such situations will tend to include edges, and so this may introduce a useful visual cue.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 155,\n",
       "  'text_chunk': 'Because we know that this smoothness constraint will be somewhat incorrect, we can mathematically define the degree to which we violate this constraint by evaluating the for-mulaE∂ x∂------xd td-----E∂ y∂------yd td-----E∂ t∂------ ++ 0 = uxd td-----= vyd td-----= E xE∂ x∂------= EyE∂ y∂------= EtE∂ t∂------ 0 == ExuEyvEt ++ 0 = Et Ex Ey'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 156,\n",
       "  'text_chunk': 'Perception 141 (4.45) which is the integral of the square of the magnitude of the gradient of the optical flow. We also determine the error in the optical flow constraint equation (which in practice will not quite be zero). (4.46) Both of these equations should be as small as possible so we want to minimize , where is a parameter that weights the error in the image motion equation relative to thedeparture from smoothness.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 156,\n",
       "  'text_chunk': 'A large parameter should be used if the brightness measure- ments are accurate and small if they are noisy. In practice the parameter is adjusted man- ually and interactively to achieve the best performance. The resulting problem then amounts to the calculus of variations, and the Euler equa- tions yield (4.47) (4.48) where (4.49) which is the Laplacian operator. Equations (4.47) and (4.48) form a pair of elliptical second-order partial differential equations which can be solved iteratively.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 156,\n",
       "  'text_chunk': 'Where silhouettes (one object occluding another) occur, discontinuities in the optical flow will occur. This of course violates the smoothness constraint. One possibility is to try and find edges that are indicative of such occlusions, excluding the pixels near such edges from the optical flow computation so that smoothness is a more realistic assumption. Another possibility is to opportunistically make use of these distinctive edges.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 156,\n",
       "  'text_chunk': 'In fact, cor- ners can be especially easy to pattern-match across subsequent images and thus can serve as fiducial markers for optical flow computation in their own right. Optical flow promises to be an important ingredient in future vision algorithms that combine cues across multiple algorithms. However, obstacle avoidance and navigatione s u2v2+() xdyd∫∫= ec ExuEyvEt ++ ()2xdyd∫∫= esλec + λ λ ∇2u λExuEyvEt ++ () Ex = ∇2v λExuEyvEt ++ () Ey = ∇2 ∂2 x2δ-------∂2 y2δ-------+ ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 157,\n",
       "  'text_chunk': '142 Chapter 4 control systems for mobile robots exclusively using optical flow have not yet proved to be broadly effective. 4.1.8.4 Color-tracking sensors Although depth from stereo will doubtless prove to be a popular application of vision-basedmethods to mobile robotics, it mimics the functionality of existing sensors, including ultra- sonic, laser, and optical rangefinders. An important aspect of vision-based sensing is that the vision chip can provide sensing modalities and cues that no other mobile robot sensorprovides. One such novel sensing modality is detecting and tracking color in the environ- ment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 157,\n",
       "  'text_chunk': 'Color represents an environmental characteristic that is orthogonal to range, and it rep- resents both a natural cue and an artificial cue that can provide new information to a mobile robot. For example, the annual robot soccer events make extensive use of color both for environmental marking and for robot localization (see figure 4.27). Color sensing has two important advantages. First, detection of color is a straightfor- ward function of a single image, therefore no correspondence problem need be solved insuch algorithms.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 157,\n",
       "  'text_chunk': 'Second, because color sensing provides a new, independent environmen-tal cue, if it is combined (i.e., sensor fusion ) with existing cues, such as data from stereo vision or laser rangefinding, we can expect significant information gains. Efficient color-tracking sensors are now available commercially.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 157,\n",
       "  'text_chunk': 'Below, we briefly describe two commercial, hardware-based color-tracking sensors, as well as a publicly available software-based solution.Figure 4.27 Color markers on the top of EPFL’s STeam Engine soccer robots enable a color-tracking sensor to locate the robots and the ball in the soccer field.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 158,\n",
       "  'text_chunk': 'Perception 143 Cognachrome color-tracking system. The Cognachrome Vision System form Newton Research Labs is a color-tracking hardware-based sensor capable of extremely fast color tracking on a dedicated processor [162]. The system will detect color blobs based on three user-defined colors at a rate of 60 Hz. The Cognachrome system can detect and report on amaximum of twenty-five objects per frame, providing centroid, bounding box, area, aspect ratio, and principal axis orientation information for each object independently. This sensor uses a technique called constant thresholding to identify each color.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 158,\n",
       "  'text_chunk': 'In (red, green and blue) space, the user defines for each of ,, and a minimum and maximum value. The 3D box defined by these six constraints forms a color bounding box, and any pixel with values that are all within this bounding box is identified as atarget. Target pixels are merged into larger objects that are then reported to the user. The Cognachrome sensor achieves a position resolution of one pixel for the centroid of each object in a field that is 200 x 250 pixels in size.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 158,\n",
       "  'text_chunk': 'The key advantage of this sensor, justas with laser rangefinding and ultrasonics, is that there is no load on the mobile robot’s main processor due to the sensing modality. All processing is performed on sensor-specific hardware (i.e., a Motorola 68332 processor and a mated framegrabber). The Cognachromesystem costs several thousand dollars, but is being superseded by higher-performance hard- ware vision processors at Newton Labs, Inc. CMUcam robotic vision sensor.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 158,\n",
       "  'text_chunk': 'Recent advances in chip manufacturing, both in terms of CMOS imaging sensors and high-speed, readily available microprocessors at the 50+ MHz range, have made it possible to manufacture low-overhead intelligent vision sensorswith functionality similar to Cognachrome for a fraction of the cost. The CMUcam sensor is a recent system that mates a low-cost microprocessor with a consumer CMOS imaging chip to yield an intelligent, self-contained vision sensor for $100, as shown in figure 4.29.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 158,\n",
       "  'text_chunk': 'This sensor is designed to provide high-level information extracted from the camera image to an external processor that may, for example, control a mobile robot. An externalprocessor configures the sensor’s streaming data mode, for instance, specifying trackingmode for a bounded or value set. Then, the vision sensor processes the data in real time and outputs high-level information to the external consumer.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 158,\n",
       "  'text_chunk': 'At less than 150 mA of current draw, this sensor provides image color statistics and color-tracking services atapproximately twenty frames per second at a resolution of 80 x 143 [126]. Figure 4.29 demonstrates the color-based object tracking service as provided by CMUcam once the sensor is trained on a human hand. The approximate shape of the objectis extracted as well as its bounding box and approximate center of mass. CMVision color tracking software library.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 158,\n",
       "  'text_chunk': 'Because of the rapid speedup of processors in recent times, there has been a trend toward executing basic vision processing on a mainRGB RG B RGB RGB YU V'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 159,\n",
       "  'text_chunk': '144 Chapter 4 processor within the mobile robot. Intel Corporation’s computer vision library is an opti- mized library for just such processing [160]. In this spirit, the CMVision color-tracking software represents a state-of-the-art software solution for color tracking in dynamic envi-ronments [47]. CMVision can track up to thirty-two colors at 30 Hz on a standard 200 MHz Pentium computer.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 159,\n",
       "  'text_chunk': 'The basic algorithm this sensor uses is constant thresholding, as with Cognachrome, with the chief difference that the color space is used instead of the color spacewhen defining a six-constraint bounding box for each color. While ,, and values encode the intensity of each color, separates the color (or chrominance ) measure from the brightness (or luminosity ) measure.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 159,\n",
       "  'text_chunk': 'represents the image’s luminosity while Figure 4.28 The CMUcam sensor consists of three chips: a CMOS imaging chip, a SX28 microprocessor, and a Maxim RS232 level shifter [126]. Figure 4.29Color-based object extraction as applied to a human hand. YUV RGB RG B YUV Y U'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 160,\n",
       "  'text_chunk': 'Perception 145 and together capture its chrominance. Thus, a bounding box expressed in space can achieve greater stability with respect to changes in illumination than is possible in space. The CMVision color sensor achieves a resolution of 160 x 120 and returns, for each object detected, a bounding box and a centroid. The software for CMVision is available freely with a Gnu Public License at [161].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 160,\n",
       "  'text_chunk': 'Key performance bottlenecks for both the CMVision software, the CMUcam hardware system, and the Cognachrome hardware system continue to be the quality of imaging chips and available computational speed. As significant advances are made on these frontiers one can expect packaged vision systems to witness tremendous performance improvements. 4.2 Representing Uncertainty In section 4.1.2 we presented a terminology for describing the performance characteristics of a sensor. As mentioned there, sensors are imperfect devices with errors of both system- atic and random nature.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 160,\n",
       "  'text_chunk': 'Random errors, in particular, cannot be corrected, and so they rep-resent atomic levels of sensor uncertainty. But when you build a mobile robot, you combine information from many sensors, even using the same sensors repeatedly, over time, to possibly build a model of the environment.How can we scale up, from characterizing the uncertainty of a single sensor to the uncer- tainty of the resulting robot system? We begin by presenting a statistical representation for the random error associated with an individual sensor [12].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 160,\n",
       "  'text_chunk': 'With a quantitative tool in hand, the standard Gaussian uncer- tainty model can be presented and evaluated. Finally, we present a framework for comput- ing the uncertainty of conclusions drawn from a set of quantifiably uncertainmeasurements, known as the error propagation law . 4.2.1 Statistical representationWe have already defined error as the difference between a sensor measurement and the true value. From a statistical point of view, we wish to characterize the error of a sensor, not forone specific measurement but for any measurement.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 160,\n",
       "  'text_chunk': 'Let us formulate the problem of sens- ing as an estimation problem. The sensor has taken a set of measurements with values . The goal is to characterize the estimate of the true value given these measure- ments: (4.50)VY U V RGB n ρi EX[] EX[] gρ1ρ2…ρn ,,,()='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 161,\n",
       "  'text_chunk': '146 Chapter 4 From this perspective, the true value is represented by a random (and therefore unknown) variable . We use a probability density function to characterize the statistical properties of the value of . In figure 4.30, the density function identifies for each possible value of a probabil- ity density along the -axis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 161,\n",
       "  'text_chunk': 'The area under the curve is 1, indicating the complete chance of having some value: (4.51) The probability of the value of falling between two limits and is computed as the bounded integral: (4.52) The probability density function is a useful way to characterize the possible values of because it not only captures the range of but also the comparative probability of different values for . Using we can quantitatively define the mean, variance, and standard deviation as follows.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 161,\n",
       "  'text_chunk': 'Themean value is equivalent to the expected value if we were to measure an infinite number of times and average all of the resulting values.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 161,\n",
       "  'text_chunk': 'We can easily define :X XFigure 4.30 A sample probability density function, showing a single probability peak (i.e., unimodal) with asymp- totic drops in both directions.Probability Density f(x) Mean µArea = 1 x 0 x X fx() y X fx()xd ∞–∞∫1= X ab PaXb ≤<[] fx()xd ab∫= X X X fx() µ EX[] X EX[]'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 162,\n",
       "  'text_chunk': 'Perception 147 (4.53) Note in the above equation that calculation of is identical to the weighted average of all possible values of .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 162,\n",
       "  'text_chunk': 'In contrast, the mean square value is simply the weighted aver- age of the squares of all values of : (4.54) Characterization of the “width” of the possible values of is a key statistical measure, and this requires first defining the variance : (4.55) Finally, the standard deviation is simply the square root of variance , and will play important roles in our characterization of the error of a single sensor as well as the error of a model generated by combining multiple sensor readings.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 162,\n",
       "  'text_chunk': '4.2.1.1 Independence of random variables. With the tools presented above, we often evaluate systems with multiple random variables.For instance, a mobile robot’s laser rangefinder may be used to measure the position of a feature on the robot’s right and, later, another feature on the robot’s left. The position of each feature in the real world may be treated as random variables, and . Two random variables and are independent if the particular value of one has no bearing on the particular value of the other.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 162,\n",
       "  'text_chunk': 'In this case we can draw several important con-clusions about the statistical behavior of and . First, the expected value (or meanvalue) of the product of random variables is equal to the product of their mean values: (4.56) Second, the variance of their sums is equal to the sum of their variances: (4.57) In mobile robotics, we often assume the independence of random variables even when this assumption is not strictly true.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 162,\n",
       "  'text_chunk': 'The simplification that results makes a number of the existing mobile robot-mapping and navigation algorithms tenable, as described inµ EX[] xf x()xd ∞–∞ ∫== EX[] x x EX2[] x2fx()xd ∞–∞∫= X σ2 Var X() σ2xµ–()2fx()xd ∞–∞∫== σσ σ2 X1 X2 X1 X2 X1 X2 EX1X2 [] EX1[]EX2[] = Var X1X2+() Var X1() Var X2() + ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 163,\n",
       "  'text_chunk': '148 Chapter 4 chapter 5. A further simplification, described in section 4.2.1.2, revolves around one spe- cific probability density function used more often than any other when modeling error: the Gaussian distribution. 4.2.1.2 Gaussian distribution The Gaussian distribution, also called the normal distribution, is used across engineering disciplines when a well-behaved error model is required for a random variable for which no error model of greater felicity has been discovered. The Gaussian has many character- istics that make it mathematically advantageous to other ad hoc probability density func- tions.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 163,\n",
       "  'text_chunk': 'It is symmetric around the mean . There is no particular bias for being larger thanor smaller than , and this makes sense when there is no information to the contrary. The Gaussian distribution is also unimodal, with a single peak that reaches a maximum at (necessary for any symmetric, unimodal distribution). This distribution also has tails (thevalue of as approaches and ) that only approach zero asymptotically. This means that all amounts of error are possible, although very large errors may be highly improbable.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 163,\n",
       "  'text_chunk': 'In this sense, the Gaussian is conservative. Finally, as seen in the formula forthe Gaussian probability density function, the distribution depends only on two parameters: (4.58) The Gaussian’s basic shape is determined by the structure of this formula, and so the only two parameters required to fully specify a particular Gaussian are its mean, , and its Figure 4.31 The Gaussian function with and . We shall refer to this as the reference Gaussian.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 163,\n",
       "  'text_chunk': 'The value is often refereed to as the signal quality; 95.44% of the values are falling within .µ 0= σ 1= 2σ 2±σfx()1 σ2π--------------xµ–()2 2σ2------------------ - – \\uf8ed\\uf8f8\\uf8ec\\uf8f7\\uf8eb\\uf8f6exp = -σ -2σσ 2σ 3σ -3σ68.26% 95.44% 99.72% µ µ µ fx() x ∞– ∞ fx()1 σ2π--------------xµ–()2 2σ2------------------ - – \\uf8ed\\uf8f8\\uf8ec\\uf8f7\\uf8eb\\uf8f6exp = µ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 164,\n",
       "  'text_chunk': 'Perception 149 standard deviation, . Figure 4.31 shows the Gaussian function with and . Suppose that a random variable is modeled as a Gaussian. How does one identify the chance that the value of is within one standard deviation of ? In practice, this requires integration of , the Gaussian function to compute the area under a portion of the curve: (4.59) Unfortunately, there is no closed-form solution for the integral in equation (4.59), and so the common technique is to use a Gaussian cumulative probability table .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 164,\n",
       "  'text_chunk': 'Using such a table, one can compute the probability for various value ranges of : ; ; . For example, 95% of the values for fall within two standard deviations of its mean. This applies to any Gaussian distribution. As is clear from the above progression, under the Gaussian assumption, once bounds are relaxed to , the overwhelming proportion of values (and, therefore, probability) is subsumed.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 164,\n",
       "  'text_chunk': '4.2.2 Error propagation: combining uncertain measurements The probability mechanisms above may be used to describe the errors associated with asingle sensor’s attempts to measure a real-world value. But in mobile robotics, one often uses a series of measurements, all of them uncertain, to extract a single environmental mea- sure. For example, a series of uncertain measurements of single points can be fused toextract the position of a line (e.g., a hallway wall) in the environment (figure 4.36).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 164,\n",
       "  'text_chunk': 'Consider the system in figure 4.32, where are input signals with a known proba- bility distribution and are m outputs. The question of interest is: what can we say aboutσ µ 0= σ 1= X X µ fx() Area f x ()xd σ–σ ∫= X Pµσ– Xµσ+≤< [] 0.68= Pµ2σ– Xµ2σ+≤< [] 0.95 = Pµ3σ– Xµ3σ+≤< [] 0.997= X 3σFigure 4.32 Error propagation in a multiple-input multi-output system with n inputs and m outputs.X1 Xi XnSystem… …Y1 Yi Ym… … Xi n Yi'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 165,\n",
       "  'text_chunk': '150 Chapter 4 the probability distribution of the output signals if they depend with known functions upon the input signals? Figure 4.33 depicts the 1D version of this error propagation problem as an example. The general solution can be generated using the first order Taylor expansion of . The output covariance matrix is given by the error propagation law: (4.60) where = covariance matrix representing the input uncertainties; = covariance matrix representing the propagated uncertainties for the outputs; is the Jacobian matrix defined as .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 165,\n",
       "  'text_chunk': '(4.61) This is also the transpose of the gradient of .Yi fiFigure 4.33 One-dimensional case of a nonlinear error propagation problem.µxσx+ µxσx– µxµyσy+ µyσy–µy XY fx() fi CY CY FXCXFXT= CX CY Fx FX f∇ ∇XfX()T⋅Tf1 : fmX1∂∂…Xn∂∂f1∂ X1∂-------- …f1∂ Xn∂-------- :… : fm∂ X1∂-------- …fm∂ Xn∂--------== = = fX()'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 166,\n",
       "  'text_chunk': 'Perception 151 We will not present a detailed derivation here but will use equation (4.60) to solve an example problem in section 4.3.1.1. 4.3 Feature ExtractionAn autonomous mobile robot must be able to determine its relationship to the environment by making measurements with its sensors and then using those measured signals. A widevariety of sensing technologies are available, as shown in the previous section. But every sensor we have presented is imperfect: measurements always have error and, therefore, uncertainty associated with them.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 166,\n",
       "  'text_chunk': 'Therefore, sensor inputs must be used in a way thatenables the robot to interact with its environment successfully in spite of measurement uncertainty. There are two strategies for using uncertain sensor input to guide the robot’s behavior. One strategy is to use each sensor measurement as a raw and individual value. Such raw sensor values could, for example, be tied directly to robot behavior, whereby the robot’s actions are a function of its sensor inputs.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 166,\n",
       "  'text_chunk': 'Alternatively, the raw sensor values could beused to update an intermediate model, with the robot’s actions being triggered as a function of this model rather than the individual sensor measurements. The second strategy is to extract information from one or more sensor readings first, generating a higher-level percept that can then be used to inform the robot’s model and per- haps the robot’s actions directly.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 166,\n",
       "  'text_chunk': 'We call this process feature extraction , and it is this next, optional step in the perceptual interpretation pipeline (figure 4.34) that we will now discuss. In practical terms, mobile robots do not necessarily use feature extraction and scene interpretation for every activity. Instead, robots will interpret sensors to varying degrees depending on each specific functionality. For example, in order to guarantee emergencystops in the face of immediate obstacles, the robot may make direct use of raw forward- facing range readings to stop its drive motors.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 166,\n",
       "  'text_chunk': 'For local obstacle avoidance, raw ranging sensor strikes may be combined in an occupancy grid model, enabling smooth avoidanceof obstacles meters away. For map-building and precise navigation, the range sensor values and even vision sensor measurements may pass through the complete perceptual pipeline, being subjected to feature extraction followed by scene interpretation to minimize theimpact of individual sensor uncertainty on the robustness of the robot’s mapmaking and navigation skills.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 166,\n",
       "  'text_chunk': 'The pattern that thus emerges is that, as one moves into more sophisti- cated, long-term perceptual tasks, the feature extraction and scene interpretation aspects ofthe perceptual pipeline become essential. Feature definition. Features are recognizable structures of elements in the environment. They usually can be extracted from measurements and mathematically described. Good features are always perceivable and easily detectable from the environment. We distinguish'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 167,\n",
       "  'text_chunk': '152 Chapter 4 between low-level features (geometric primitives ) like lines, circles, or polygons, and high- level features (objects ) such as edges, doors, tables, or a trash can. At one extreme, raw sensor data provide a large volume of data, but with low distinctiveness of each individual quantum of data. Making use of raw data has the potential advantage that every bit of infor- mation is fully used, and thus there is a high conservation of information.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 167,\n",
       "  'text_chunk': 'Low-level fea- tures are abstractions of raw data, and as such provide a lower volume of data while increasing the distinctiveness of each feature. The hope, when one incorporates low-level features, is that the features are filtering out poor or useless data, but of course it is also likely that some valid information will be lost as a result of the feature extraction process.High-level features provide maximum abstraction from the raw data, thereby reducing the volume of data as much as possible while providing highly distinctive resulting features.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 167,\n",
       "  'text_chunk': 'Once again, the abstraction process has the risk of filtering away important information,potentially lowering data utilization. Although features must have some spatial locality, their geometric extent can range widely. For example, a corner feature inhabits a specific coordinate location in the geomet-ric world. In contrast, a visual “fingerprint” identifying a specific room in an office building applies to the entire room, but has a location that is spatially limited to the one particular room.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 167,\n",
       "  'text_chunk': 'In mobile robotics, features play an especially important role in the creation of environ- mental models. They enable more compact and robust descriptions of the environment,helping a mobile robot during both map-building and localization. When designing amobile robot, a critical decision revolves around choosing the appropriate features for the robot to use. A number of factors are essential to this decision: Target environment. For geometric features to be useful, the target geometries must be readily detected in the actual environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 167,\n",
       "  'text_chunk': 'For example, line features are extremely useful in office building environments due to the abundance of straight wall segments, while thesame features are virtually useless when navigating Mars.Figure 4.34 The perceptual pipeline: from sensor readings to knowledge models.sensingsignal treatmentfeature extractionscene pretationinter-Envi ronment'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 168,\n",
       "  'text_chunk': 'Perception 153 Available sensors. Obviously, the specific sensors and sensor uncertainty of the robot impacts the appropriateness of various features. Armed with a laser rangefinder, a robot is well qualified to use geometrically detailed features such as corner features owing to the high-quality angular and depth resolution of the laser scanner. In contrast, a sonar-equippedrobot may not have the appropriate tools for corner feature extraction. Computational power.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 168,\n",
       "  'text_chunk': 'Vision-based feature extraction can effect a significant computa- tional cost, particularly in robots where the vision sensor processing is performed by one of the robot’s main processors. Environment representation. Feature extraction is an important step toward scene inter- pretation, and by this token the features extracted must provide information that is conso- nant with the representation used for the environmental model. For example, nongeometricvision-based features are of little value in purely geometric environmental models but can be of great value in topological models of the environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 168,\n",
       "  'text_chunk': 'Figure 4.35 shows the applica- tion of two different representations to the task of modeling an office building hallway. Each approach has advantages and disadvantages, but extraction of line and corner features has much more relevance to the representation on the left. Refer to chapter 5, section 5.5 for a close look at map representations and their relative trade-offs. Figure 4.35 Environment representation and modeling: (a) feature based (continuous metric); (b) occupancy grid (discrete metric). Courtesy of Sjur Vestli.a) b)'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 169,\n",
       "  'text_chunk': '154 Chapter 4 In the following two sections, we present specific feature extraction techniques based on the two most popular sensing modalities of mobile robotics: range sensing and visual appearance-based sensing. 4.3.1 Feature extraction based on range data (laser, ultrasonic, vision-based ranging) Most of today’s features extracted from ranging sensors are geometric primitives such as line segments or circles. The main reason for this is that for most other geometric primitivesthe parametric description of the features becomes too complex and no closed-form solu- tion exists.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 169,\n",
       "  'text_chunk': 'Here we describe line extraction in detail, demonstrating how the uncertainty models presented above can be applied to the problem of combining multiple sensor mea-surements. Afterward, we briefly present another very successful feature of indoor mobile robots, the corner feature, and demonstrate how these features can be combined in a single representation. 4.3.1.1 Line extraction Geometric feature extraction is usually the process of comparing and matching measured sensor data against a predefined description, or template, of the expect feature.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 169,\n",
       "  'text_chunk': 'Usually, thesystem is overdetermined in that the number of sensor measurements exceeds the number of feature parameters to be estimated. Since the sensor measurements all have some error, there is no perfectly consistent solution and, instead, the problem is one of optimization.One can, for example, extract the feature that minimizes the discrepancy with all sensor measurements used (e.g,. least-squares estimation). In this section we present an optimization-based solution to the problem of extracting a line feature from a set of uncertain sensor measurements.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 169,\n",
       "  'text_chunk': 'For greater detail than is pre- sented below, refer to [14, pp. 15 and 221]. Probabilistic line extraction from uncertain range sensor data. Our goal is to extract a line feature based on a set of sensor measurements as shown in figure 4.36. There is uncer- tainty associated with each of the noisy range sensor measurements, and so there is nosingle line that passes through the set. Instead, we wish to select the best possible match, given some optimization criterion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 169,\n",
       "  'text_chunk': 'More formally, suppose ranging measurement points in polar coordinates are produced by the robot’s sensors. We know that there is uncertainty asso- ciated with each measurement, and so we can model each measurement using two random variables . In this analysis we assume that uncertainty with respect to the actual value of and is independent.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 169,\n",
       "  'text_chunk': 'Based on equation (4.56) we can state this for- mally: = (4.62)n x i ρiθi,()= Xi PiQi,()= P Q EPiPj⋅[] EPi[]EPj[] ∀ ij, 1…n ,,='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 170,\n",
       "  'text_chunk': 'Perception 155 = (4.63) = (4.64) Furthermore, we assume that each random variable is subject to a Gaussian probability density curve, with a mean at the true value and with some specified variance: ~ (4.65) ~ (4.66) Given some measurement point , we can calculate the corresponding Euclidean coordinates as and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 170,\n",
       "  'text_chunk': 'If there were no error, we would want to find a line for which all measurements lie on that line: (4.67) Of course there is measurement error, and so this quantity will not be zero. When it is nonzero, this is a measure of the error between the measurement point and the line, specifically in terms of the minimum orthogonal distance between the point and the line. Itis always important to understand how the error that shall be minimized is being measured.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 170,\n",
       "  'text_chunk': 'For example a number of line extraction techniques do not minimize this orthogonal point-αr Figure 4.36 Estimating a line in the least-squares sense.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 170,\n",
       "  'text_chunk': 'The model parameters y (length of the perpendicular) and α (its angle to the abscissa) uniquely describe a line.dixi= (ρi,θi) EQiQj⋅[] EQi[]EQj[] ∀ ij, 1…n ,,= EPiQj⋅[] EPi[]EQj[] ∀ ij, 1…n ,,= PiNρiσρi2,() QiNθiσθi2,() ρθ,() x ρθcos = y ρθsin= ρθcos αcos ρθsin αsin r– + ρ θα–()cos r– 0 == ρθ,()'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 171,\n",
       "  'text_chunk': '156 Chapter 4 line distance, but instead the distance parallel to the y-axis between the point and the line. A good illustration of the variety of optimization criteria is available in [17] where several algorithms for fitting circles and ellipses are presented which minimize algebraic and geo- metric distances. For each specific , we can write the orthogonal distance between and the line as .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 171,\n",
       "  'text_chunk': '(4.68) If we consider each measurement to be equally uncertain, we can sum the square of all errors together, for all measurement points, to quantify an overall fit between the line andall of the measurements: (4.69) Our goal is to minimize when selecting the line parameters . We can do so by solving the nonlinear equation system . (4.70) The above formalism is considered an unweighted least-squares solution because no distinction is made from among the measurements.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 171,\n",
       "  'text_chunk': 'In reality, each sensor measurementmay have its own, unique uncertainty based on the geometry of the robot and environment when the measurement was recorded. For example, we know with regard to vision-based stereo ranging that uncertainty and, therefore, variance increases as a square of the distancebetween the robot and the object. To make use of the variance that models the uncer- tainty regarding distance of a particular sensor measurement, we compute an individual weight for each measurement using the formula 2.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 171,\n",
       "  'text_chunk': '(4.71) Then, equation (4.69) becomes 2.The issue of determining an adequate weight when is given (and perhaps some additional information) is complex in general and beyond the scope of this text. See [9] for a careful treatment.ρiθi,() di ρiθi,() ρi θiα–()cos r– di= Sdi2 i∑ρi θiα–()cos r– ()2 i∑== S αr,() α∂∂S0=r∂∂S0= σi2 ρi wi wi1σi2⁄= σi'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 172,\n",
       "  'text_chunk': 'Perception 157 . (4.72) It can be shown that the solution to equation (4.70) in the weighted least-squares sense3 is (4.73) (4.74) In practice, equation (4.73) uses the four-quadrant arc tangent (atan2)4. Let us demonstrate equations (4.73) and (4.74) with a concrete example. The seventeen measurements in table 4.2 have been taken with a laser range sensor installed on a mobile robot.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 172,\n",
       "  'text_chunk': 'We assume that the uncertainties of all measurements are equal, uncorrelated, and that the robot was static during the measurement process. Direct application of the above solution equations yields the line defined by and . This line represents the best fit in a least-squares sense and is shown visuallyin figure 4.37. Propagation of uncertainty during line extraction. Returning to the subject of section 4.2.3, we would like to understand how the uncertainties of specific range sensor measure- ments propagate to govern the uncertainty of the extracted line.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 172,\n",
       "  'text_chunk': 'In other words, how does uncertainty in and propagate in equations (4.73) and (4.74) to affect the uncertainty of and ? 3.We follow here the notation of [14] and distinguish a weighted least-squares problem if is diagonal (input errors are mutually independent) and a generalized least-squares problem if is nondiagonal.4.Atan2 computes but uses the signs of both x and y to determine the quadrant in which the resulting angles lies. For example , whereas , a dis-tinction which would be lost with a single-argument arc tangent function.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 172,\n",
       "  'text_chunk': 'Swidi2∑wiρi θiα–()cos r– ()2∑== CX CXα1 2---atanwiρi22θi sin ∑2 Σwi-------- wiwjρiρjθi cos θjsin ∑∑– wiρi2 2θi cos ∑1 Σwi-------- wiwjρiρj θiθj+()cos ∑∑–-------------------------------------------------------------------------------------------------------------------- \\uf8ed\\uf8f8\\uf8ec\\uf8f7\\uf8ec\\uf8f7\\uf8ec\\uf8f7\\uf8eb\\uf8f6 = rwiρiθiα–()cos ∑ wi∑--------------------------------------------- - = xy⁄()tan1– 22–2–,() atan 135 ° –=2 2 2,() atan 45 °–=ρiθi,() α 37.36 = r 0.4= ρi θi α r'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 173,\n",
       "  'text_chunk': '158 Chapter 4 Table 4.2 Measured values pointing angle of sensor θi[deg]range ρi[m] 0 5 1015 20 25 30 35 40 4550 55 60 65 70 75 80 0.5197 0.4404 0.4850 0.4222 0.4132 0.4371 0.3912 0.3949 0.3919 0.4276 0.4075 0.3956 0.4053 0.4752 0.5032 0.5273 0.4879Figure 4.37 Extracted line from laser range measurements (+). The small lines at each measurement point repre- sent the measurement uncertainty σ that is proportional to the square of the measurement distance. αρ xy'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 174,\n",
       "  'text_chunk': 'Perception 159 This requires direct application of equation (4.60) with and representing the random output variables of and respectively. The goal is to derive the output covariance matrix , (4.75) given the input covariance matrix (4.76) and the system relationships [equations (4.73) and (4.74)].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 174,\n",
       "  'text_chunk': 'Then by calculating the Jaco- bian, (4.77) we can instantiate the uncertainty propagation equation (4.63) to yield : (4.78) Thus we have calculated the probability of the extracted line based on the probabilities of the measurement points. For more details about this method refer to [6, 37] 4.3.1.2 Segmentation for line extraction The previous section described how to extract a line feature given a set of range measure-ments. Unfortunately, the feature extraction process is significantly more complex than that.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 174,\n",
       "  'text_chunk': 'A mobile robot does indeed acquire a set of range measurements, but in general the range measurements are not all part of one line. Rather, only some of the range measure-ments should play a role in line extraction and, further, there may be more than one lineA R α r 22× CARσA2σAR σARσR2= 2n2n× CXCP0 0CQdiag σρi2() 0 0 diag σθi2()== FPQP1∂∂α P2∂∂α…Pn∂∂α Q1∂∂α Q2∂∂α…Qn∂∂α P1∂∂r P2∂∂r…Pn∂∂r Q1∂∂r Q2∂∂r…Qn∂∂r= CAR CAR FPQCXFPQT= CAR αr,()'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 175,\n",
       "  'text_chunk': '160 Chapter 4 feature represented in the measurement set. This more realistic scenario is shown in figure 4.38. The process of dividing up a set of measurements into subsets that can be interpreted one by one is termed segmentation and is an important aspect of both range-based and vision- based perception. A diverse set of techniques exist for segmentation of sensor input in gen- eral. This general problem is beyond the scope of this text and for details concerning seg-mentation algorithms, refer to [91, 131].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 175,\n",
       "  'text_chunk': 'For example, one segmentation technique is the merging, or bottom-up technique in which smaller features are identified and then merged together based on decision criteria toextract the goal features. Suppose that the problem of figure 4.38. is solved through merg- ing. First, one may generate a large number of line segments based on adjacent groups of range measurements. The second step would be to identify line segments that have a highprobability of belonging to the same extracted light feature.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 175,\n",
       "  'text_chunk': 'The simplest measure of the closeness of two line segments 5 and in the model space is given by Euclidean distance: (4.79) 5.Note: The lines are represented in polar coordinates.Figure 4.38 Clustering: finding neighboring segments of a common line [37].β1=r [m] A set of nf neighboring points of the image spaceEvidence accumulation in the model space → Clusters of normally distributed vectorsa) Image Space b) Model Space β0=α [rad] x1 α1r1,[]= x2 α2r2,[]= x1x2–()Tx1x2–() α1α2–()2r1r2–()2+ ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 176,\n",
       "  'text_chunk': 'Perception 161 The selection of all line segments that contribute to the same line can now be done in a threshold-based manner according to (4.80) where is a threshold value and is the representation of the reference line (from a model, average of a group of lines, etc.). But the approach of equation (4.80) does not take into account the fact that for each mea- surement and therefore for each line segment we have a measure of uncertainty.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 176,\n",
       "  'text_chunk': 'One canimprove upon this equation by selecting line segments that are weighted by their covariance matrix : (4.81) The distance measure of equation (4.81) discriminates the distance of uncertain points in model space considerably more effectively by taking uncertainty into account explicitly. 4.3.1.3 Range histogram features A histogram is a simple way to combine characteristic elements of an image. An angle his- togram, as presented in figure 4.39, plots the statistics of lines extracted by two adjacentrange measurements.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 176,\n",
       "  'text_chunk': 'First, a 360-degree scan of the room is taken with the range scanner, and the resulting “hits” are recorded in a map. Then the algorithm measures the relative angle between any two adjacent hits (see figure 4.39b). After compensating for noise in thereadings (caused by the inaccuracies in position between adjacent hits), the angle histogram shown in figure 4.39c can be built. The uniform direction of the main walls are clearly vis- ible as peaks in the angle histogram.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 176,\n",
       "  'text_chunk': 'Detection of peaks yields only two main peaks: onefor each pair of parallel walls. This algorithm is very robust with regard to openings in the walls, such as doors and windows, or even cabinets lining the walls. 4.3.1.4 Extracting other geometric features Line features are of particular value for mobile robots operating in man-made environ- ments, where, for example, building walls and hallway walls are usually straight.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 176,\n",
       "  'text_chunk': 'In gen- eral, a mobile robot makes use of multiple features simultaneously, comprising a feature set that is most appropriate for its operating environment. For indoor mobile robots, the line feature is certainly a member of the optimal feature set. In addition, other geometric kernels consistently appear throughout the indoor man- made environment. Corner features are defined as a point feature with an orientation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 176,\n",
       "  'text_chunk': 'Step discontinuities , defined as a step change perpendicular to the direction of hallway travel,x j xjx–()Txjx–() dm≤ dm x Cj xjx–()TCjC+()1–xjx–() dm≤'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 177,\n",
       "  'text_chunk': '162 Chapter 4 are characterized by their form (convex or concave) and step size. Doorways , defined as openings of the appropriate dimensions in walls, are characterized by their width. Thus, the standard segmentation problem is not so simple as deciding on a mapping from sensor readings to line segments, but rather it is a process in which features of different types are extracted based on the available sensor measurements.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 177,\n",
       "  'text_chunk': 'Figure 4.40 shows a model of an indoor hallway environment along with both indentation features (i.e., step disconti- nuities) and doorways. Note that different feature types can provide quantitatively different information for mobile robot localization. The line feature, for example, provides two degrees of informa- tion, angle and distance. But the step feature provides 2D relative position information as well as angle. The set of useful geometric features is essentially unbounded, and as sensor perfor- mance improves we can only expect greater success at the feature extraction level.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 177,\n",
       "  'text_chunk': 'For example, an interesting improvement upon the line feature described above relates to the Figure 4.39 Angle histogram [155]. δ 20°AB C DE F δ [°]na) b) c)'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 178,\n",
       "  'text_chunk': 'Perception 163 advent of successful vision-based ranging systems. Because stereo vision provides a full 3D set of range measurements, one can extract plane features in addition to line features from the resulting data set. Plane features are valuable in man-made environments due tothe flat walls, floors, and ceilings of our indoor environments. Thus they are promising as another highly informative feature for mobile robots to use for mapping and localization.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 178,\n",
       "  'text_chunk': '4.3.2 Visual appearance based feature extraction Visual interpretation is, as we have mentioned before, an extremely challenging problem to fully solve. Significant research effort has been dedicated over the past several decades, to inventing algorithms for understanding a scene based on 2D images and the research efforts have slowly produced fruitful results. Covering the field of computer vision andimage processing is, of course, beyond the scope of this book. To explore these disciplines, refer to [18, 29, 159].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 178,\n",
       "  'text_chunk': 'An overview on some of the most popular approaches can be seen in figure 4.41. In section 4.1.8 we have already seen vision-based ranging and color-tracking sensors that are commercially available for mobile robots. These specific vision applications havewitnessed commercial solutions primarily because the challenges are in both cases rela-tively well focused and the resulting, problem-specific algorithms are straightforward. But images contain much more than implicit depth information and color blobs.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 178,\n",
       "  'text_chunk': 'We would like to solve the more general problem of extracting a large number of feature types fromimages.Figure 4.40 Multiple geometric features in a single hallway, including doorways and discontinuities in the width of the hallway.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 179,\n",
       "  'text_chunk': '164 Chapter 4 This section presents some appearance-based feature extraction techniques that are rel- evant to mobile robotics along these lines. Two key requirements must be met for a vision- based feature extraction technique to have mobile robotic relevance. First, the method mustoperate in real time. Mobile robots move through their environment, and so the processing simply cannot be an off-line operation. Second, the method must be robust to the real-world conditions outside of a laboratory. This means that carefully controlled illuminationassumptions and carefully painted objects are unacceptable requirements.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 179,\n",
       "  'text_chunk': 'Throughout the following descriptions, keep in mind that vision-based interpretation is primarily about the challenge of reducing information . A sonar unit produces perhaps fifty bits of information per second. By contrast, a CCD camera can output 240 million bits per second! The sonar produces a tiny amount of information from which we hope to drawbroader conclusions. But the CCD chip produces too much information, and this overabun-dance of information mixes together relevant and irrelevant information haphazardly. For example, we may intend to measure the color of a landmark.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 179,\n",
       "  'text_chunk': 'The CCD camera does not simply report its color, but also measures the general illumination of the environment, theFigure 4.41 Scheme and tools in computer vision. See also [18].conditioning labeling/grouping extracting matching modelenvironment imagesimplified im age propertiesgroups of pixelcognition / actionthreshold- ing connected compo- nent labelingedge detection Hough transfor- mationfiltering disparitycorrelationImage Processing Scheme Computer VisionTools in Computer Vision'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 180,\n",
       "  'text_chunk': 'Perception 165 direction of illumination, the defocusing caused by optics, the side effects imposed by nearby objects with different colors, and so on. Therefore the problem of visual feature extraction is largely one of removing the majority of irrelevant information in an image so that the remaining information unambiguously describes specific features in the environ-ment. We divide vision-based feature extraction methods into two classes based on their spa- tial extent. Spatially localized features are those features found in subregions of one or more images, corresponding to specific locations in the physical world.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 180,\n",
       "  'text_chunk': 'Whole-image fea- tures are those features that are functions of the entire image or set of images, correspond- ing to a large visually connected area in the physical world. Before continuing it is important to note that all vision-based sensors supply images with such a significant amount of noise that a first step usually consists of “cleaning” theimage before launching any feature extraction algorithm. Therefore, we first describe theprocess of initial image filtering, or preprocessing. Image preprocessing.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 180,\n",
       "  'text_chunk': 'Many image-processing algorithms make use of the second deriv- ative of the image intensity. Indeed, the Laplacian of Gaussian method we studied in sec- tion 4.1.8.2 for stereo ranging is such an example. Because of the susceptibility of such high-order derivative algorithms to changes in illumination in the basic signal, it is impor-tant to smooth the signal so that changes in intensity are due to real changes in the luminos- ity of objects in the scene rather than random variations due to imaging noise.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 180,\n",
       "  'text_chunk': 'A standard approach is convolution with a Gaussian distribution function, as we described earlier insection 4.1.8.2: (4.82) Of course, when approximated by a discrete kernel, such as a 3 x 3 table, the result is essentially local, weighted averaging: (4.83) Such a low-pass filter effectively removes high-frequency noise, and this in turn causes the first derivative and especially the second derivative of intensity to be far more stable.Because of the importance of gradients and derivatives to image processing, such Gaussiansmoothing preprocessing is a popular first step of virtually all computer vision algorithms.I ˆGI⊗ = G1 16------121 242 121='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 181,\n",
       "  'text_chunk': '166 Chapter 4 4.3.2.1 Spatially localized features In the computer vision community many algorithms assume that the object of interest occu- pies only a sub-region of the image, and therefore the features being sought are localized spatially within images of the scene. Local image-processing techniques find features thatare local to a subset of pixels, and such local features map to specific locations in the phys- ical world. This makes them particularly applicable to geometric models of the robot’s environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 181,\n",
       "  'text_chunk': 'The single most popular local feature extractor used by the mobile robotics community is the edge detector, and so we begin with a discussion of this classic topic in computer vision. However, mobile robots face the specific mobility challenges of obstacle avoidanceand localization. In view of obstacle avoidance, we present vision-based extraction of the floor plane, enabling a robot to detect all areas that can be safely traversed. Finally, in view of the need for localization we discuss the role of vision-based feature extraction in thedetection of robot navigation landmarks.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 181,\n",
       "  'text_chunk': 'Edge detection. Figure 4.42 shows an image of a scene containing a part of a ceiling lamp as well as the edges extracted from this image. Edges define regions in the image plane where a significant change in the image brightness takes place. As shown in this example, edge detection significantly reduces the amount of information in an image, and is therefore a useful potential feature during image interpretation. The hypothesis is that edge contours in an image correspond to important scene contours.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 181,\n",
       "  'text_chunk': 'As figure 4.42b shows, this is not entirely true. There is a difference between the output of an edge detector and an ideal linedrawing. Typically, there are missing contours, as well as noise contours, that do not cor- respond to anything of significance in the scene.Figure 4.42 (a) Photo of a ceiling lamp. (b) Edges computed from (a).a) b)'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 182,\n",
       "  'text_chunk': 'Perception 167 The basic challenge of edge detection is visualized in figure 4.23. Figure 4.23 (top left) shows the 1D section of an ideal edge. But the signal produced by a camera will look more like figure 4.23 (top right). The location of the edge is still at the same xvalue, but a signif- icant level of high-frequency noise affects the signal quality. A naive edge detector would simply differentiate, since an edge by definition is located where there are large transitions in intensity.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 182,\n",
       "  'text_chunk': 'As shown in figure 4.23 (bottom right), dif- ferentiation of the noisy camera signal results in subsidiary peaks that can make edge detec-tion very challenging. A far more stable derivative signal can be generated simply by preprocessing the camera signal using the Gaussian smoothing function described above. Below, we present several popular edge detection algorithms, all of which operate on thissame basic principle, that the derivative(s) of intensity, following some form of smoothing, comprises the basic signal from which to extract edge features.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 182,\n",
       "  'text_chunk': 'Optimal edge detection Canny. The current reference edge detector throughout the vision community was invented by John Canny in 1983 [30]. This edge detector was born out of a formal approach in which Canny treated edge detection as a signal-processingproblem in which there are three explicit goals: •Maximizing the signal-to-noise ratio; •Achieving the highest precision possible on the location of edges; •Minimizing the number of edge responses associated with each edge.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 182,\n",
       "  'text_chunk': 'The Canny edge extractor smooths the image I via Gaussian convolution and then looks for maxima in the (rectified) derivative. In practice the smoothing and differentiation are combined into one operation because (4.84) Thus, smoothing the image by convolving with a Gaussian and then differentiating is equivalent to convolving the image with , the first derivative of a Gaussian (figure4.43b). We wish to detect edges in any direction.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 182,\n",
       "  'text_chunk': 'Since is directional, this requires applica- tion of two perpendicular filters, just as we did for the Laplacian in equation (4.35). Wedefine the two filters as and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 182,\n",
       "  'text_chunk': \"The result is a basic algorithm for detecting edges at arbitrary orientations: The algorithm for detecting edge pixels at an arbitrary orientation is as follows: 1.Convolve the image with and to obtain the gradient compo-nents and , respectively.G I⊗() 'G'I⊗ = Gσ G'σ G' fVxy,() G'σx()Gσy() = fHxy,() G'σy()Gσx() = Ixy,() fVxy,() fHxy,() RVxy,() RHxy,()\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 183,\n",
       "  'text_chunk': '168 Chapter 4 2.Define the square of the gradient magnitude . 3.Mark those peaks in that are above some predefined threshold . Once edge pixels are extracted, the next step is to construct complete edges. A popular next step in this process is nonmaxima suppression . Using edge direction information, the process involves revisiting the gradient value and determining whether or not it is at a local Figure 4.43 (a) A Gaussian function.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 183,\n",
       "  'text_chunk': \"(b) The first derivative of a Gaussian function.Gσx()1 2πσ-------------- ex2 2σ2---------– =a)b) Gσ'x()x– 2πσ3---------------- -ex2 2σ2---------– = Figure 4.44(a) Two-dimensional Gaussian function. (b) Vertical filter. (c) Horizontal filter.\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 183,\n",
       "  'text_chunk': \"abc Gσxy,() Gσx()Gσy() = fVxy,() G'σx()Gσy() = fHxy,() G'σy()Gσx() = Rxy,() RV2xy,() RH2xy,() + = Rxy,() T\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 184,\n",
       "  'text_chunk': 'Perception 169 maximum. If not, then the value is set to zero. This causes only the maxima to be preserved, and thus reduces the thickness of all edges to a single pixel (figure 4.45). Finally, we are ready to go from edge pixels to complete edges. First, find adjacent (or connected) sets of edges and group them into ordered lists. Second, use thresholding to eliminate the weakest edges. Gradient edge detectors.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 184,\n",
       "  'text_chunk': 'On a mobile robot, computation time must be minimized to retain the real-time behavior of the robot. Therefore simpler, discrete kernel operators are commonly used to approximate the behavior of the Canny edge detector. One such early operator was developed by Roberts in 1965 [29]. He used two 2 x 2 masks to calculate the gradient across the edge in two diagonal directions. Let be the value calculated from the first mask and from the second mask.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 184,\n",
       "  'text_chunk': 'Roberts obtained the gradient magnitude with the equation ; ; (4.85) Prewitt (1970) [29] used two 3 x 3 masks oriented in the row and column directions. Let be the value calculated from the first mask and the value calculated from the second mask. Prewitt obtained the gradient magnitude and the gradient direction taken in aclockwise angle with respect to the column axis shown in the following equation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 184,\n",
       "  'text_chunk': ';Figure 4.45 (a) Example of an edge image; (b) Nonmaxima suppression of (a).ab r1 r2 G Gr12r22+ ≅ r11–0 01= r201– 10= p1 p2 G θ Gp12p22+ ≅'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 185,\n",
       "  'text_chunk': '170 Chapter 4 ; ; (4.86) In the same year Sobel [29] used, like Prewitt, two 3 x 3 masks oriented in the row and column direction. Let be the value calculated from the first mask and the value cal- culated from the second mask. Sobel obtained the same results as Prewitt for the gradient magnitude and the gradient direction taken in a clockwise angle with respect to the column axis.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 185,\n",
       "  'text_chunk': 'Figure 4.46 shows application of the Sobel filter to a visual scene.θp1 p2-----\\uf8ed\\uf8f8\\uf8eb\\uf8f6atan≅ p11–1–1– 000 111= p21–0 1 1–0 1 1–0 1= s1 s2 G θFigure 4.46 Example of vision-based feature extraction with the different processing steps: (a) raw image data; (b) filtered image using a Sobel filter; (c) thresholding, selection of edge pixels (d) nonmaxima sup-pression. a b c d'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 186,\n",
       "  'text_chunk': 'Perception 171 ; ; ; (4.87) Dynamic thresholding. Many image-processing algorithms have generally been tested in laboratory conditions or by using static image databases. Mobile robots, however, operate in dynamic real-world settings where there is no guarantee regarding optimal or even stable illumination. A vision system for mobile robots has to adapt to the changing illumination. Therefore a constant threshold level for edge detection is not suitable. The same scene withdifferent illumination results in edge images with considerable differences.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 186,\n",
       "  'text_chunk': 'To dynamically adapt the edge detector to the ambient light, a more adaptive threshold is required, and one approach involves calculating that threshold based on a statistical analysis of the imageabout to be processed. To do this, a histogram of the gradient magnitudes of the processed image is calculated (figure 4.47). With this simple histogram it is easy to consider only the pixels with thehighest gradient magnitude for further calculation steps. The pixels are counted backward starting at the highest magnitude.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 186,\n",
       "  'text_chunk': 'The gradient magnitude of the point where is reached will be used as the temporary threshold value. The motivation for this technique is that the pixels with the highest gradient are expected to be the most relevant ones for the processed image. Furthermore, for eachimage, the same number of relevant edge pixels is considered, independent of illumination. It is important to pay attention to the fact that the number of pixels in the edge image deliv- ered by the edge detector is not .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 186,\n",
       "  'text_chunk': 'Because most detectors use nonmaxima suppression, the number of edge pixels will be further reduced. Straight edge extraction: Hough transforms. In mobile robotics the straight edge is often extracted as a specific feature. Straight vertical edges, for example, can be used as clues to the location of doorways and hallway intersections. The Hough transform is a simple tool for extracting edges of a particular shape[16, 18]. Here we explain its applica- tion to the problem of extracting straight edges.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 186,\n",
       "  'text_chunk': 'Suppose a pixel in the image is part of an edge. Any straight-line edge includ- ing point must satisfy the equation: . This equation can only be satisfied with a constrained set of possible values for and . In other words, this equa-tion is satisfied only by lines through I that pass through .Gs 12s22+ ≅ θs1 s2----\\uf8ed\\uf8f8\\uf8eb\\uf8f6atan≅ s11–2–1– 000 121= s21–0 1 2–0 2 1–0 1= n n n n xpyp,() I xpyp,() yp m1xpb1+ = m1 b1 xpyp,()'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 187,\n",
       "  'text_chunk': '172 Chapter 4 Now consider a second pixel, in . Any line passing through this second pixel must satisfy the equation: . What if and ? Then the line defined by both equations is one and the same: it is the line that passes through both and . More generally, for all pixels that are part of a single straight line through , they must all lie on a line defined by the same values for and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 187,\n",
       "  'text_chunk': 'The general definition of this line is, of course, .The Hough transform uses this basic property, creating a mech- anism so that each edge pixel can “vote” for various values of the parameters. Thelines with the most votes at the end are straight edge features: •Create a 2D array A with axes that tessellate the values of m and b. •Initialize the array to zero: for all values of . •For each edge pixel in , loop over all values of and : if then .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 187,\n",
       "  'text_chunk': '•Search the cells in A to identify those with the largest value. Each such cell’s indices correspond to an extracted straight-line edge in .Figure 4.47 (a) Number of pixels with a specific gradient magnitude in the image of figure 4.46(b).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 187,\n",
       "  'text_chunk': '(b) Same as (a), but with logarithmic scale a b xqyq,() I yq m2xqb2+ = m1 m2= b1 b2= xpyp,() xqyq,() I mb ym x b + = mb,() Amb,[] 0= mb, xpyp,() I mb yp mxpb+ = Amb,[] +=1 mb,() I'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 188,\n",
       "  'text_chunk': 'Perception 173 Floor plane extraction. Obstacle avoidance is one of the basic tasks required of most mobile robots. Range-based sensors provide effective means for identifying most types of obstacles facing a mobile robot. In fact, because they directly measure range to objects in the world, range-based sensors such as ultrasonic and laser rangefinders are inherently wellsuited for the task of obstacle detection. However, each ranging sensor has limitations. Ultrasonics have poor angular resolution and suffer from coherent reflection at shallow angles.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 188,\n",
       "  'text_chunk': 'Most laser rangefinders are 2D, only detecting obstacles penetrating a specificsensed plane. Stereo vision and depth from focus require the obstacles and floor plane to have texture in order to enable correspondence and blurring respectively. In addition to each individual shortcoming, range-based obstacle detection systems will have difficulty detecting small or flat objects that are on the ground. For example, a vacuumcleaner may need to avoid large, flat objects, such as paper or money left on the floor.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 188,\n",
       "  'text_chunk': 'In addition, different types of floor surfaces cannot easily be discriminated by ranging. Forexample, a sidewalk-following robot will have difficulty discriminating grass from pave- ment using range sensing alone. Floor plane extraction is a vision-based approach for identifying the traversable portions of the ground. Because it makes use of edges and color in a variety of implementations,such obstacle detection systems can easily detect obstacles in cases that are difficult for tra- ditional ranging devices.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 188,\n",
       "  'text_chunk': 'As is the case with all vision-based algorithms, floor plane extraction succeeds only in environments that satisfy several important assumptions: •Obstacles differ in appearance from the ground. •The ground is flat and its angle to the camera is known. •There are no overhanging obstacles. The first assumption is a requirement in order to discriminate the ground from obstacles using its appearance. A stronger version of this assumption, sometimes invoked, states that the ground is uniform in appearance and different from all obstacles.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 188,\n",
       "  'text_chunk': 'The second and third assumptions allow floor plane extraction algorithms to estimate the robot’s distance to obstacles detected. Floor plane extraction in artificial environments. In a controlled environment, the floor, walls and obstacles can be designed so that the walls and obstacles appear signifi- cantly different from the floor in a camera image. Shakey, the first autonomous robot devel- oped from 1966 through 1972 at SRI, used vision-based floor plane extraction in amanufactured environment for obstacle detection [115].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 188,\n",
       "  'text_chunk': 'Shakey’s artificial environment used textureless, homogeneously white floor tiles. Furthermore, the base of each wall was painted with a high-contrast strip of black paint and the edges of all simple polygonal obsta-cles were also painted black.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 189,\n",
       "  'text_chunk': '174 Chapter 4 In Shakey’s environment, edges corresponded to nonfloor objects, and so the floor plane extraction algorithm simply consisted of the application of an edge detector to the mono- chrome camera image. The lowest edges detected in an image corresponded to the closest obstacles, and the direction of straight-line edges extracted from the image provided cluesregarding not only the position but also the orientation of walls and polygonal obstacles.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 189,\n",
       "  'text_chunk': 'Although this very simple appearance-based obstacle detection system was successful, it should be noted that special care had to be taken at the time to create indirect lighting inthe laboratory such that shadows were not cast, as the system would falsely interpret the edges of shadows as obstacles. Adaptive floor plane extraction.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 189,\n",
       "  'text_chunk': 'Floor plane extraction has succeeded not only in artifi- cial environments but in real-world mobile robot demonstrations in which a robot avoids both static obstacles such as walls and dynamic obstacles such as passersby, based on seg-mentation of the floor plane at a rate of several hertz. Such floor plane extraction algorithms tend to use edge detection and color detection jointly while making certain assumptions regarding the floor, for example, the floor’s maximum texture or approximate color range[78].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 189,\n",
       "  'text_chunk': 'Each system based on fixed assumptions regarding the floor’s appearance is limited to only those environments satisfying its constraints. A more recent approach is that of adap-tive floor plane extraction, whereby the parameters defining the expected appearance of the floor are allowed to vary over time. In the simplest instance, one can assume that the pixels at the bottom of the image (i.e., closest to the robot) are part of the floor and contain noobstacles.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 189,\n",
       "  'text_chunk': 'Then, statistics computed on these “floor sample” pixels can be used to classify the remaining image pixels. The key challenge in adaptive systems is the choice of what statistics to compute using the “floor sample” pixels. The most popular solution is to construct one or more histograms based on the floor sample pixel values. Under “edge detection” above, we found histograms to be useful in determining the best cut point in edge detection thresholding algorithms.Histograms are also useful as discrete representations of distributions.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 189,\n",
       "  'text_chunk': 'Unlike the Gaussian representation, a histogram can capture multi-modal distributions. Histograms can also be updated very quickly and use very little processor memory. An intensity histogram of the“floor sample” subregion of image is constructed as follows: •As preprocessing, smooth , using a Gaussian smoothing operator. •Initialize a histogram array H with n intensity values: for . •For every pixel in increment the histogram: += 1.The histogram array serves as a characterization of the appearance of the floor plane.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 189,\n",
       "  'text_chunk': 'Often, several 1D histograms are constructed, corresponding to intensity, hue, and satura- tion, for example. Classification of each pixel in as floor plane or obstacle is performed If I If Hi[] 0= i1…n ,,= xy,() If HIfxy,()[] H I'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 190,\n",
       "  'text_chunk': 'Perception 175 by looking at the appropriate histogram counts for the qualities of the target pixel. For example, if the target pixel has a hue that never occurred in the “floor sample,” then the corresponding hue histogram will have a count of zero. When a pixel references a histo-gram value below a predefined threshold, that pixel is classified as an obstacle. Figure 4.48 shows an appearance-based floor plane extraction algorithm operating on both indoor and outdoor images [151].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 190,\n",
       "  'text_chunk': 'Note that, unlike the static floor extraction algo- rithm, the adaptive algorithm is able to successfully classify a human shadow due to the adaptive histogram representation. An interesting extension of the work has been to not use the static floor sample assumption, but rather to record visual history and to use, as the floorsample, only the portion of prior visual images that has successfully rolled under the robot during mobile robot motion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 190,\n",
       "  'text_chunk': 'Appearance-based extraction of the floor plane has been demonstrated on both indoor and outdoor robots for real-time obstacle avoidance with a bandwidth of up to 10 Hz.Applications include robotics lawn mowing, social indoor robots, and automated electric wheelchairs. 4.3.2.2 Whole-image features A single visual image provides so much information regarding a robot’s immediate sur- roundings that an alternative to searching the image for spatially localized features is tomake use of the information captured by the entire image to extract a whole-image feature.Figure 4.48 Examples of adaptive floor plane extraction.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 190,\n",
       "  'text_chunk': 'The trapezoidal polygon identifies the floor sampling region.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 191,\n",
       "  'text_chunk': '176 Chapter 4 Whole-image features are not designed to identify specific spatial structures such as obsta- cles or the position of specific landmarks. Rather, they serve as compact representations of the entire local region. From the perspective of robot localization, the goal is to extract oneor more features from the image that are correlated well with the robot’s position. In other words, small changes in robot position should cause only small changes to whole-image features, while large changes in robot position should cause correspondingly large changesto whole-image features.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 191,\n",
       "  'text_chunk': 'We present two techniques for whole-image feature extraction below. The first tech- nique is another popular application of the image histogramming approach. The resultingimage histogram comprises a set of whole-image features derived directly from the pixel information of an image. The second technique, tiered extraction, covers approaches in which a whole-image feature is built by first extracting spatially localized features, thencomposing these features together to form a single metafeature. Direct extraction: image histograms.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 191,\n",
       "  'text_chunk': 'Recall that we wish to design whole-image fea- tures that are insensitive to a small amount of robot motion while registering significant changes for large-scale robot motion. A logical first step in designing a vision-based sensor for this purpose is to maximize the field of view of the camera. As the field of viewincreases, a small-scale structure in the robot’s environment occupies a smaller proportion of the image, thereby mitigating the impact of individual scene objects on image character- istics.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 191,\n",
       "  'text_chunk': 'The catadioptric camera system, now very popular in mobile robotics, offers anextremely wide field of view [114]. This imaging system consists of a high-quality CCD camera mounted, together with customized optics, toward a parabolic mirror. The image provides a 360-degree view of the robot’s environment, as shown in figure 4.49.Figure 4.49 Two typical images acquired by the OmniCam catadioptric camera system.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 192,\n",
       "  'text_chunk': 'Perception 177 The catadioptric image is a 360-degree image warped onto a 2D image surface. Because of this, it offers another critical advantage in terms of sensitivity to small-scale robot motion. If the camera is mounted vertically on the robot so that the image represents the environment surrounding the robot (i.e., its horizon), then rotation of the camera and robotsimply results in image rotation. In short, the catadioptric camera can be rotationally invari- ant to field of view.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 192,\n",
       "  'text_chunk': 'Of course, mobile robot rotation will still change the image; that is, pixel positions will change, although the new image will simply be a rotation of the original image. But we intend to extract image features via histogramming. Because histogramming is a function of the set of pixel values and not the position of each pixel, the process is pixel positioninvariant.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 192,\n",
       "  'text_chunk': 'When combined with the catadioptric camera’s field of view invariance, we can create a system that is invariant to robot rotation and insensitive to small-scale robot trans- lation. A color camera’s output image generally contains useful information along multiple bands : , , and values as well as hue, saturation, and luminance values. The simplest histogram-based extraction strategy is to build separate 1D histograms characterizing eachband. Given a color camera image, , the first step is to create mappings from to each of the available bands.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 192,\n",
       "  'text_chunk': 'We use to refer to an array storing the values in band for all pixels in . Each band-specific histogram is calculated as before: •As preprocessing, smooth using a Gaussian smoothing operator. •Initialize with n levels: for . •For every pixel ( x,y) in , increment the histogram: . Given the image shown in figure 4.49, the image histogram technique extracts six his- tograms (for each of , , , hue, saturation, and luminance) as shown in figure 4.50.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 192,\n",
       "  'text_chunk': 'In order to make use of such histograms as whole-image features, we need ways to compareto histograms to quantify the likelihood that the histograms map to nearby robot positions. The problem of defining useful histogram distance metrics is itself an important subfield within the image retrieval field. For an overview refer to [127]. One of the most successful distance metrics encountered in mobile robot localization is the Jeffrey divergence .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 192,\n",
       "  'text_chunk': 'Given two histograms and , with and denoting the histogram entries, the Jeffrey diver-gence is defined as (4.88) Using measures such as the Jeffrey divergence, mobile robots have used whole-image histogram features to identify their position in real time against a database of previouslyrg b G G n Gi i G Hi Gi Hi Hj[] 0= j1…n ,,= Gi HiGixy,[][] +=1 rgb H K hi ki dH K,() dHK,() hi2hi hiki+--------------log ki2ki hiki+--------------log+\\uf8ed\\uf8f8\\uf8eb\\uf8f6 i∑='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 193,\n",
       "  'text_chunk': '178 Chapter 4 recorded images of locations in their environment. Using this whole-image extraction approach, a robot can readily recover the particular hallway or particular room in which itis located [152]. Tiered extraction: image fingerprint extraction. An alternative to extracting a whole- image feature directly from pixel values is to use a tiered approach: first identify spatially localized features in the image, then translate from this set of local features to a single metafeature for the whole image.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 193,\n",
       "  'text_chunk': 'We describe one particular implementation of this approach, in which the resulting whole-image feature is called the image fingerprint [95]. As with other whole-image extraction techniques, because low sensitivity to small robotmotions is desired, the system makes use of a 360-degree panoramic image, here con-structed as a mosaic of images captured with a standard CMOS chip camera. The first extraction tier searches the panoramic image for spatially localized features: vertical edges and sixteen discrete hues of color.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 193,\n",
       "  'text_chunk': 'The vertical edge detector is a straightfor-ward gradient approach implementing a horizontal difference operator. Vertical edges are “voted upon” by each edge pixel just as in a vertical edge Hough transform. As describedFigure 4.50 Six 1D histograms of the image above. A 5 x 5 smoothing filter was convolved with each band before histogramming.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 194,\n",
       "  'text_chunk': 'Perception 179 in section 4.3.2.1, an adaptive threshold is used to reduce the number of edges. Suppose the Hough table’s tallies for each candidate vertical line have a mean and a standard devia-tion . The chosen threshold is simply . Vertical color bands are identified in largely the same way, identifying statistics over the occurrence of each color, then filtering out all candidate color patches except those withtallies greater than . Figure 4.51 shows two sample panoramic images and their asso- ciated fingerprints.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 194,\n",
       "  'text_chunk': 'Note that each fingerprint is converted to an ASCII string representa- tion. Just as with histogram distance metrics in the case of image histogramming, we need a quantifiable measure of the distance between two fingerprint strings. String-matching algo-rithms are yet another large field of study, with particularly interesting applications todayin the areas of genetics [34]. Note that we may have strings that differ not just in a single element value, but even in their overall length.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 194,\n",
       "  'text_chunk': 'For example, figure 4.52 depicts three actualµ σ µσ+ µσ+Figure 4.51 Two panoramic images and their associated fingerprint sequences [95]. Figure 4.52Three actual string sequences. The top two are strings extracted by the robot at the same position [95].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 195,\n",
       "  'text_chunk': '180 Chapter 4 sequences generated using the above algorithm. The top string should match Place 1 , but note that there are deletions and insertions between the two strings. The technique used in the fingerprinting approach for string differencing is known as a minimum energy algorithm . Taken from the stereo vision community, this optimization- based algorithm will find the minimum energy required to “transform” one sequence into another sequence.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 195,\n",
       "  'text_chunk': 'The result is a distance metric that is relatively insensitive to the addition or subtraction of individual local features while still able to robustly identify the correctmatching string in a variety of circumstances. It should be clear from the previous two sections that whole-image feature extraction is straightforward with vision-based perception and can be applicable to mobile robot local-ization. But it is spatially localized features that continue to play a dominant role because of their immediate application to the more urgent need for real-time obstacle avoidance.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 196,\n",
       "  'text_chunk': '5 Mobile Robot Localization 5.1 Introduction Navigation is one of the most challenging competences required of a mobile robot. Success in navigation requires success at the four building blocks of navigation: perception, the robot must interpret its sensors to extract meaningful data; localization, the robot must determine its position in the environment (figure 5.1); cognition, the robot must decide how to act to achieve its goals; and motion control, the robot must modulate its motor outputs to achieve the desired trajectory.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 196,\n",
       "  'text_chunk': 'Of these four components (figure 5.2), localization has received the greatest research attention in the past decade and, as a result, significant advances have been made on thisfront. In this chapter, we explore the successful localization methodologies of recent years. First, section 5.2 describes how sensor and effector uncertainty is responsible for the diffi- culties of localization. Then, section 5.3 describes two extreme approaches to dealing withthe challenge of robot localization: avoiding localization altogether, and performing explicit map-based localization.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 196,\n",
       "  'text_chunk': 'The remainder of the chapter discusses the question of rep- Figure 5.1 Where am I? ?'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 197,\n",
       "  'text_chunk': '182 Chapter 5 resentation, then presents case studies of successful localization systems using a variety of representations and techniques to achieve mobile robot localization competence. 5.2 The Challenge of Localization: Noise and Aliasing If one could attach an accurate GPS (global positioning system) sensor to a mobile robot, much of the localization problem would be obviated. The GPS would inform the robot ofits exact position, indoors and outdoors, so that the answer to the question, “Where am I?”, would always be immediately available.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 197,\n",
       "  'text_chunk': 'Unfortunately, such a sensor is not currently prac- tical. The existing GPS network provides accuracy to within several meters, which is unac-ceptable for localizing human-scale mobile robots as well as miniature mobile robots such as desk robots and the body-navigating nanorobots of the future. Furthermore, GPS tech- nologies cannot function indoors or in obstructed areas and are thus limited in their work-space. But, looking beyond the limitations of GPS , localization implies more than knowing one’s absolute position in the Earth’s reference frame.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 197,\n",
       "  'text_chunk': 'Consider a robot that is interacting with humans. This robot may need to identify its absolute position, but its relative positionObservationMap data basePrediction of Position (e.g. odometry) Figure 5.2 General schematic for mobile robot localization. PerceptionMatchingPosition Update (Estimation?) raw sensor data or extracted featurespredicted positionposition matched observations YESEncoder'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 198,\n",
       "  'text_chunk': 'Mobile Robot Localization 183 with respect to target humans is equally important. Its localization task can include identi- fying humans using its sensor array, then computing its relative position to the humans. Furthermore, during the cognition step a robot will select a strategy for achieving its goals. If it intends to reach a particular location, then localization may not be enough. The robot may need to acquire or build an environmental model, a map, that aids it in planning a path to the goal.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 198,\n",
       "  'text_chunk': 'Once again, localization means more than simply determining an absolute posein space; it means building a map, then identifying the robot’s position relative to that map. Clearly, the robot’s sensors and effectors play an integral role in all the above forms of localization. It is because of the inaccuracy and incompleteness of these sensors and effec-tors that localization poses difficult challenges. This section identifies important aspects ofthis sensor and effector suboptimality.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 198,\n",
       "  'text_chunk': '5.2.1 Sensor noise Sensors are the fundamental robot input for the process of perception , and therefore the degree to which sensors can discriminate the world state is critical. Sensor noise induces a limitation on the consistency of sensor readings in the same environmental state and, there-fore, on the number of useful bits available from each sensor reading. Often, the source of sensor noise problems is that some environmental features are not captured by the robot’s representation and are thus overlooked.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 198,\n",
       "  'text_chunk': 'For example, a vision system used for indoor navigation in an office building may use the color values detected by its color CCD camera. When the sun is hidden by clouds, theillumination of the building’s interior changes because of the windows throughout thebuilding. As a result, hue values are not constant.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 198,\n",
       "  'text_chunk': 'The color CCD appears noisy from the robot’s perspective as if subject to random error, and the hue values obtained from the CCD camera will be unusable, unless the robot is able to note the position of the sun and cloudsin its representation. Illumination dependence is only one example of the apparent noise in a vision-based sensor system. Picture jitter, signal gain, blooming, and blurring are all additional sourcesof noise, potentially reducing the useful content of a color video image.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 198,\n",
       "  'text_chunk': 'Consider the noise level (i.e., apparent random error) of ultrasonic range-measuring sen- sors (e.g., sonars) as discussed in section 4.1.2.3. When a sonar transducer emits soundtoward a relatively smooth and angled surface, much of the signal will coherently reflect away, failing to generate a return echo. Depending on the material characteristics, a small amount of energy may return nonetheless.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 198,\n",
       "  'text_chunk': 'When this level is close to the gain threshold of the sonar sensor, then the sonar will, at times, succeed and, at other times, fail to detect the object. From the robot’s perspective, a virtually unchanged environmental state will result in two different possible sonar readings: one short and one long. The poor signal-to-noise ratio of a sonar sensor is further confounded by interference between multiple sonar emitters. Often, research robots have between twelve and forty-'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 199,\n",
       "  'text_chunk': '184 Chapter 5 eight sonars on a single platform. In acoustically reflective environments, multipath inter- ference is possible between the sonar emissions of one transducer and the echo detection circuitry of another transducer. The result can be dramatically large errors (i.e., underesti- mation) in ranging values due to a set of coincidental angles. Such errors occur rarely, lessthan 1% of the time, and are virtually random from the robot’s perspective. In conclusion, sensor noise reduces the useful information content of sensor readings.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 199,\n",
       "  'text_chunk': 'Clearly, the solution is to take multiple readings into account, employing temporal fusionor multisensor fusion to increase the overall information content of the robot’s inputs. 5.2.2 Sensor aliasing A second shortcoming of mobile robot sensors causes them to yield little information con- tent, further exacerbating the problem of perception and, thus, localization. The problem, known as sensor aliasing , is a phenomenon that humans rarely encounter. The human sen- sory system, particularly the visual system, tends to receive unique inputs in each unique local state.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 199,\n",
       "  'text_chunk': 'In other words, every different place looks different. The power of this unique mapping is only apparent when one considers situations where this fails to hold. Considermoving through an unfamiliar building that is completely dark. When the visual system sees only black, one’s localization system quickly degrades. Another useful example is that of a human-sized maze made from tall hedges.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 199,\n",
       "  'text_chunk': 'Such mazes have been created for centuries,and humans find them extremely difficult to solve without landmarks or clues because, without visual uniqueness, human localization competence degrades rapidly. In robots, the nonuniqueness of sensor readings, or sensor aliasing , is the norm and not the exception. Consider a narrow-beam rangefinder such as an ultrasonic or infrared rangefinder. This sensor provides range information in a single direction without any addi- tional data regarding material composition such as color, texture, and hardness.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 199,\n",
       "  'text_chunk': 'Even for arobot with several such sensors in an array, there are a variety of environmental states that would trigger the same sensor values across the array. Formally, there is a many-to-one mapping from environmental states to the robot’s perceptual inputs. Thus, the robot’s per-cepts cannot distinguish from among these many states. A classic problem with sonar- based robots involves distinguishing between humans and inanimate objects in an indoor setting.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 199,\n",
       "  'text_chunk': 'When facing an apparent obstacle in front of itself, should the robot say “Excuseme” because the obstacle may be a moving human, or should the robot plan a path around the object because it may be a cardboard box? With sonar alone, these states are aliased and differentiation is impossible. The problem posed to navigation because of sensor aliasing is that, even with noise-free sensors, the amount of information is generally insufficient to identify the robot’s position from a single-percept reading.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 199,\n",
       "  'text_chunk': 'Thus techniques must be employed by the robot programmerthat base the robot’s localization on a series of readings and, thus, sufficient information to recover the robot’s position over time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 200,\n",
       "  'text_chunk': 'Mobile Robot Localization 185 5.2.3 Effector noise The challenges of localization do not lie with sensor technologies alone. Just as robot sen- sors are noisy, limiting the information content of the signal, so robot effectors are also noisy. In particular, a single action taken by a mobile robot may have several different pos-sible results, even though from the robot’s point of view the initial state before the action was taken is well known. In short, mobile robot effectors introduce uncertainty about future state.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 200,\n",
       "  'text_chunk': 'Therefore the simple act of moving tends to increase the uncertainty of a mobile robot. There are, of course, exceptions. Using cognition, the motion can be carefully planned so as to minimize this effect, and indeed sometimes to actually result in more certainty. Furthermore, when the robot’s actions are taken in concert with careful interpretation of sensory feedback, it can compensate for the uncertainty introduced by noisy actions using the information pro- vided by the sensors.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 200,\n",
       "  'text_chunk': 'First, however, it is important to understand the precise nature of the effector noise that impacts mobile robots. It is important to note that, from the robot’s point of view, this errorin motion is viewed as an error in odometry, or the robot’s inability to estimate its own posi-tion over time using knowledge of its kinematics and dynamics. The true source of error generally lies in an incomplete model of the environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 200,\n",
       "  'text_chunk': 'For instance, the robot does not model the fact that the floor may be sloped, the wheels may slip, and a human may pushthe robot. All of these unmodeled sources of error result in inaccuracy between the physical motion of the robot, the intended motion of the robot, and the proprioceptive sensor esti- mates of motion. In odometry (wheel sensors only) and dead reckoning (also heading sensors) the posi- tion update is based on proprioceptive sensors.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 200,\n",
       "  'text_chunk': 'The movement of the robot, sensed with wheel encoders or heading sensors or both, is integrated to compute position. Because the sensor measurement errors are integrated, the position error accumulates over time. Thus the position has to be updated from time to time by other localization mechanisms. Other- wise the robot is not able to maintain a meaningful position estimate in the long run.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 200,\n",
       "  'text_chunk': 'In the following we concentrate on odometry based on the wheel sensor readings of a differential-drive robot only (see also [4, 57, 58]). Using additional heading sensors (e.g.,gyroscope) can help to reduce the cumulative errors, but the main problems remain thesame. There are many sources of odometric error, from environmental factors to resolution: •Limited resolution during integration (time increments, measurement resolution, etc.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 200,\n",
       "  'text_chunk': '); •Misalignment of the wheels (deterministic); •Uncertainty in the wheel diameter and in particular unequal wheel diameter (determin-istic); •Variation in the contact point of the wheel;'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 201,\n",
       "  'text_chunk': '186 Chapter 5 •Unequal floor contact (slipping, nonplanar surface, etc.). Some of the errors might be deterministic (systematic) , thus they can be eliminated by proper calibration of the system. However, there are still a number of nondeterministic (random) errors which remain, leading to uncertainties in position estimation over time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 201,\n",
       "  'text_chunk': 'From a geometric point of view one can classify the errors into three types: 1.Range error: integrated path length (distance) of the robot’s movement → sum of the wheel movements 2.Turn error: similar to range error, but for turns → difference of the wheel motions 3.Drift error: difference in the error of the wheels leads to an error in the robot’s angular orientation Over long periods of time, turn and drift errors far outweigh range errors, since their con- tribution to the overall position error is nonlinear.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 201,\n",
       "  'text_chunk': 'Consider a robot whose position is ini- tially perfectly well-known, moving forward in a straight line along the -axis. The error in the -position introduced by a move of meters will have a component of , which can be quite large as the angular error grows. Over time, as a mobile robot movesabout the environment, the rotational error between its internal reference frame and its orig- inal reference frame grows quickly.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 201,\n",
       "  'text_chunk': 'As the robot moves away from the origin of these ref- erence frames, the resulting linear error in position grows quite large. It is instructive toestablish an error model for odometric accuracy and see how the errors propagate over time. 5.2.4 An error model for odometric position estimation Generally the pose (position) of a robot is represented by the vector (5.1) For a differential-drive robot the position can be estimated starting from a known posi- tion by integrating the movement (summing the incremental travel distances).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 201,\n",
       "  'text_chunk': 'For a dis-crete system with a fixed sampling interval the incremental travel distances are (5.2)x yd d ∆ θ sin ∆θ px y θ= ∆t ∆x∆y∆θ;;() ∆x ∆s θ∆ θ 2⁄ +()cos ='},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 202,\n",
       "  'text_chunk': 'Mobile Robot Localization 187 (5.3) (5.4) (5.5) where = path traveled in the last sampling interval; = traveled distances for the right and left wheel respectively; = distance between the two wheels of differential-drive robot.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 202,\n",
       "  'text_chunk': \"Thus we get the updated position : (5.6) By using the relation for of equations (5.4) and (5.5) we further obtain the basic equation for odometric position update (for differential drive robots):∆y ∆s θ∆ θ 2⁄ +()sin = ∆θ∆sr∆–sl b-------------------= ∆s∆sr∆sl + 2--------------------- - = ∆x∆y∆θ;;() ∆sr∆sl; bFigure 5.3 Movement of a differential-drive robot.v(t) ω(t)θXI XI p' p'x' y' θ'p∆s θ∆ θ 2⁄ +()cos ∆s θ∆ θ 2⁄ +()sin ∆θ+x y θ∆s θ∆ θ 2⁄ +()cos ∆s θ∆ θ 2⁄ +()sin ∆θ+ == = ∆s∆θ;()\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 203,\n",
       "  'text_chunk': '188 Chapter 5 (5.7) As we discussed earlier, odometric position updates can give only a very rough estimate of the actual position. Owing to integration errors of the uncertainties of and the motion errors during the incremental motion the position error based on odometry inte- gration grows with time. In the next step we will establish an error model for the integrated position to obtain the covariance matrix of the odometric position estimate. To do so, we assume that at the starting point the initial covariance matrix is known.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 203,\n",
       "  'text_chunk': 'For the motion increment we assume the following covariance matrix : (5.8) where and are the distances traveled by each wheel, and , are error con- stants representing the nondeterministic parameters of the motor drive and the wheel-floor interaction. As you can see, in equation (5.8) we made the following assumptions: •The two errors of the individually driven wheels are independent5; •The variance of the errors (left and right wheels) are proportional to the absolute value of the traveled distances .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 203,\n",
       "  'text_chunk': 'These assumptions, while not perfect, are suitable and will thus be used for the further development of the error model. The motion errors are due to imprecise movement because of deformation of wheel, slippage, unequal floor, errors in encoders, and so on. The values for the error constants and depend on the robot and the environment and should beexperimentally established by performing and analyzing representative movements.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 203,\n",
       "  'text_chunk': \"If we assume that and are uncorrelated and the derivation of f [equa- tion (5.7)] is reasonably approximated by the first-order Taylor expansion (linearization),we conclude, using the error propagation law (see section 4.2.2), 5.If there is more knowledge regarding the actual robot kinematics, the correlation terms of the covariance matrix could also be used.p'fxy θ∆sr∆sl ,,, ,()x y θ∆sr∆sl + 2--------------------- - θ∆sr∆–sl 2b-------------------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6cos ∆sr∆sl + 2--------------------- - θ∆sr∆–sl 2b-------------------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6sin ∆sr∆–sl b-------------------+ == p ∆sr∆sl;() p' Σp' Σp ∆sr∆sl;() Σ∆ Σ∆ covar ∆sr∆sl,()kr∆sr 0 0 kl∆sl== ∆sr ∆sl krkl ∆sr∆sl;() kr kl p ∆rl ∆sr∆sl;()=\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 204,\n",
       "  'text_chunk': 'Mobile Robot Localization 189 (5.9) The covariance matrix is, of course, always given by the of the previous step, and can thus be calculated after specifying an initial value (e.g., 0).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 204,\n",
       "  'text_chunk': \"Using equation (5.7) we can develop the two Jacobians , and : (5.10) (5.11) The details for arriving at equation (5.11) are (5.12) (5.13) and with ; (5.14)Σp'∇pfΣp∇⋅pfT⋅∇∆rlfΣ∆∇∆rlf ⋅T⋅ + = Σp Σp' Fp ∇pf = F∆rl∇∆rlf = Fp ∇pf∇pfT()f∂ x∂-----f∂ y∂-----f∂ θ∂------10 ∆s θ∆ θ 2⁄ +()sin– 01 ∆s θ∆ θ 2⁄ +()cos 00 1== = = F∆rl1 2--- θ∆θ 2-------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6 ∆s 2b------ θ∆θ 2-------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6sin – cos1 2--- θ∆θ 2-------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6 ∆s 2b------ θ∆θ 2-------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6sin + cos 1 2--- θ∆θ 2-------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6 ∆s 2b------ θ∆θ 2-------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6cos + sin1 2--- θ∆θ 2-------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6 ∆s 2b------– θ∆θ 2-------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6cos sin 1 b---1 b---–= F∆rl∇∆rlff∂ ∆sr∂-----------f∂ ∆sl∂---------- … == = ∆s∂ ∆sr∂------------- θ∆θ 2------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6 ∆s 2------ θ∆θ 2------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6sin–∆θ∂ ∆sr∂------------- + cos∆s∂ ∆sl∂------------ θ∆θ 2------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6∆s 2------ θ∆θ 2------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6sin–∆θ∂ ∆sl∂------------ + cos ∆s∂ ∆sr∂------------- θ∆θ 2------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6∆s 2------ θ∆θ 2------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6cos∆θ∂ ∆sr∂------------- + sin∆s∂ ∆sl∂------------ θ∆θ 2------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6∆s 2------ θ∆θ 2------+\\uf8ed\\uf8f8\\uf8eb\\uf8f6cos∆θ∂ ∆sl∂------------ + sin ∆θ∂ ∆sr∂-------------∆θ∂ ∆sl∂------------ ∆s∆sr∆sl + 2--------------------- - = ∆θ∆sr∆–sl b-------------------=\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 205,\n",
       "  'text_chunk': '190 Chapter 5 ; ; ; (5.15) we obtain equation (5.11). Figures 5.4 and 5.5 show typical examples of how the position errors grow with time. The results have been computed using the error model presented above. Once the error model has been established, the error parameters must be specified. One can compensate for deterministic errors properly calibrating the robot. However the error parameters specifying the nondeterministic errors can only be quantified by statistical(repetitive) measurements.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 205,\n",
       "  'text_chunk': 'A detailed discussion of odometric errors and a method for cal- ibration and quantification of deterministic and nondeterministic errors can be found in [5]. A method for on-the-fly odometry error estimation is presented in [105].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 205,\n",
       "  'text_chunk': '∆ s∂ ∆sr∂-----------1 2---=∆s∂ ∆sl∂----------1 2---=∆θ∂ ∆sr∂-----------1 b---=∆θ∂ ∆sl∂----------1 b---–=Figure 5.4 Growth of the pose uncertainty for straight-line movement: Note that the uncertainty in y grows much faster than in the direction of movement. This results from the integration of the uncertainty about the robot’s orientation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 205,\n",
       "  'text_chunk': 'The ellipses drawn around the robot positions represent the uncertainties in the x,y direction (e.g. ). The uncertainty of the orientation is not represented in the picture although its effect can be indirectly observed.3σ θ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 206,\n",
       "  'text_chunk': 'Mobile Robot Localization 191 5.3 To Localize or Not to Localize: Localization-Based Navigation versus Programmed Solutions Figure 5.6 depicts a standard indoor environment that a mobile robot navigates. Suppose that the mobile robot in question must deliver messages between two specific rooms in thisenvironment: rooms A and B. In creating a navigation system, it is clear that the mobile robot will need sensors and a motion control system.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 206,\n",
       "  'text_chunk': 'Sensors are absolutely required toavoid hitting moving obstacles such as humans, and some motion control system is required so that the robot can deliberately move. It is less evident, however, whether or not this mobile robot will require a localization system . Localization may seem mandatory in order to successfully navigate between the two rooms. It is through localizing on a map, after all, that the robot can hope to recover its position and detect when it has arrived at the goal location.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 206,\n",
       "  'text_chunk': 'It is true that, at the least, the robot must have a way of detecting the goal location. However, explicit localization withreference to a map is not the only strategy that qualifies as a goal detector. An alternative, espoused by the behavior-based community, suggests that, since sensors and effectors are noisy and information-limited, one should avoid creating a geometric mapfor localization. Instead, this community suggests designing sets of behaviors that together result in the desired robot motion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 206,\n",
       "  'text_chunk': 'Fundamentally, this approach avoids explicit reasoning about localization and position, and thus generally avoids explicit path planning as well.Figure 5.5 Growth of the pose uncertainty for circular movement ( r= const): Again, the uncertainty perpendic- ular to the movement grows much faster than that in the direction of movement. Note that the main axis of the uncertainty ellipse does not remain perpendicular to the direction of movement.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 207,\n",
       "  'text_chunk': '192 Chapter 5 This technique is based on a belief that there exists a procedural solution to the particular navigation problem at hand. For example, in figure 5.6, the behavioralist approach to nav- igating from room A to room B might be to design a left-wall following behavior and a detector for room B that is triggered by some unique queue in room B, such as the color of the carpet.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 207,\n",
       "  'text_chunk': 'Then the robot can reach room B by engaging the left-wall follower with the room B detector as the termination condition for the program. The architecture of this solution to a specific navigation problem is shown in figure 5.7. The key advantage of this method is that, when possible, it may be implemented veryquickly for a single environment with a small number of goal positions. It suffers from some disadvantages, however. First, the method does not directly scale to other environ-ments or to larger environments.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 207,\n",
       "  'text_chunk': 'Often, the navigation code is location-specific, and the same degree of coding and debugging is required to move the robot to a new environment. Second, the underlying procedures, such as left-wall-follow, must be carefully designed to produce the desired behavior. This task may be time-consuming and is heavily dependenton the specific robot hardware and environmental characteristics. Third, a behavior-based system may have multiple active behaviors at any one time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 207,\n",
       "  'text_chunk': 'Even when individual behaviors are tuned to optimize performance, this fusion and rapidswitching between multiple behaviors can negate that fine-tuning. Often, the addition of each new incremental behavior forces the robot designer to retune all of the existing behav-iors again to ensure that the new interactions with the freshly introduced behavior are all stable.Figure 5.6 A sample environment. AB'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 208,\n",
       "  'text_chunk': 'Mobile Robot Localization 193 In contrast to the behavior-based approach, the map-based approach includes both local- ization and cognition modules (see figure 5.8). In map-based navigation, the robot explic- itly attempts to localize by collecting sensor data, then updating some belief about its position with respect to a map of the environment. The key advantages of the map-based approach for navigation are as follows: •The explicit, map-based concept of position makes the system’s belief about position transparently available to the human operators.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 208,\n",
       "  'text_chunk': '•The existence of the map itself represents a medium for communication between human and robot: the human can simply give the robot a new map if the robot goes to a newenvironment.Figure 5.7 An architecture for behavior-based navigation.sensors detect goal positiondiscover new area avoid obstacles follow right / left wallcommunicate data actuators coordination / fusion e.g. fusion via vector summationΣ Figure 5.8 An architecture for map-based (or model-based) navigation.sensors cognition / planninglocalization / map-building motion controlperception actuators'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 209,\n",
       "  'text_chunk': '194 Chapter 5 •The map, if created by the robot, can be used by humans as well, achieving two uses. The map-based approach will require more up-front development effort to create a nav- igating mobile robot. The hope is that the development effort results in an architecture that can successfully map and navigate a variety of environments, thereby amortizing the up-front design cost over time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 209,\n",
       "  'text_chunk': 'Of course the key risk of the map-based approach is that an internal representation, rather than the real world itself, is being constructed and trusted by the robot. If that model diverges from reality (i.e. , if the map is wrong), then the robot’s behavior may be undesir- able, even if the raw sensor values of the robot are only transiently incorrect. In the remainder of this chapter, we focus on a discussion of map-based approaches and, specifically, the localization component of these techniques.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 209,\n",
       "  'text_chunk': 'These approaches are partic- ularly appropriate for study given their significant recent successes in enabling mobile robots to navigate a variety of environments, from academic research buildings, to factoryfloors, and to museums around the world. 5.4 Belief Representation The fundamental issue that differentiates various map-based localization systems is the issue of representation . There are two specific concepts that the robot must represent, and each has its own unique possible solutions. The robot must have a representation (a model) of the environment, or a map.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 209,\n",
       "  'text_chunk': 'What aspects of the environment are contained in this map? At what level of fidelity does the map represent the environment? These are the designquestions for map representation . The robot must also have a representation of its belief regarding its position on the map. Does the robot identify a single unique position as its current position, or does it describeits position in terms of a set of possible positions? If multiple possible positions are expressed in a single belief, how are those multiple positions ranked, if at all?'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 209,\n",
       "  'text_chunk': 'These are the design questions for belief representation . Decisions along these two design axes can result in varying levels of architectural com- plexity, computational complexity, and overall localization accuracy. We begin by discuss-ing belief representation. The first major branch in a taxonomy of belief representationsystems differentiates between single-hypothesis and multiple-hypothesis belief systems. The former covers solutions in which the robot postulates its unique position, whereas the latter enables a mobile robot to describe the degree to which it is uncertain about its posi- tion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 209,\n",
       "  'text_chunk': 'A sampling of different belief and map representations is shown in figure 5.9. 5.4.1 Single-hypothesis belief The single-hypothesis belief representation is the most direct possible postulation of mobile robot position. Given some environmental map, the robot’s belief about position is'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 210,\n",
       "  'text_chunk': 'Mobile Robot Localization 195 Figure 5.9 Belief representation regarding the robot position (1D) in continuous and discretized (tessellated) maps. (a) Continuous map with single-hypothesis belief, e.g., single Gaussian centered at a single continuous value. (b) Continuous map with multiple-hypothesis belief, e.g;. multiple Gaussians cen- tered at multiple continuous values. (c) Discretized (decomposed) grid map with probability values for all possible robot positions, e.g. ; Markov approach.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 210,\n",
       "  'text_chunk': '(d) Discretized topological map with proba- bility value for all possible nodes (topological robot positions), e.g. ; Markov approach.position xprobability P position xprobability P position xprobability Pa) b) c) nodeprobability Pd) ABCDE F Gof topological map'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 211,\n",
       "  'text_chunk': '196 Chapter 5 expressed as a single unique point on the map. In figure 5.10, three examples of a single- hypothesis belief are shown using three different map representations of the same actual environment (figure 5.10a). In figure 5.10b, a single point is geometrically annotated as the robot’s position in a continuous 2D geometric map. In figure 5.10c, the map is a discrete,tessellated map, and the position is noted at the same level of fidelity as the map cell size.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 211,\n",
       "  'text_chunk': 'In figure 5.10d, the map is not geometric at all but abstract and topological. In this case, the single hypothesis of position involves identifying a single node iin the topological graph as the robot’s position. The principal advantage of the single-hypothesis representation of position stems from the fact that, given a unique belief, there is no position ambiguity. The unambiguous natureof this representation facilitates decision-making at the robot’s cognitive level (e.g., path planning).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 211,\n",
       "  'text_chunk': 'The robot can simply assume that its belief is correct, and can then select its future actions based on its unique position. Just as decision-making is facilitated by a single-position hypothesis, so updating the robot’s belief regarding position is also facilitated, since the single position must beupdated by definition to a new, single position. The challenge with this position updateapproach, which ultimately is the principal disadvantage of single-hypothesis representa- tion, is that robot motion often induces uncertainty due to effector and sensor noise.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 211,\n",
       "  'text_chunk': 'There- fore, forcing the position update process to always generate a single hypothesis of position is challenging and, often, impossible. 5.4.2 Multiple-hypothesis belief In the case of multiple-hypothesis beliefs regarding position, the robot tracks not just a single possible position but a possibly infinite set of positions. In one simple example originating in the work of Jean-Claude Latombe [21, 99], the robot’s position is described in terms of a convex polygon positioned in a 2D map of the environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 211,\n",
       "  'text_chunk': 'This multiple-hypothesis representation communicates the set of possible robot positions geometrically, with no preference ordering over the positions. Each pointin the map is simply either contained by the polygon and, therefore, in the robot’s belief set, or outside the polygon and thereby excluded. Mathematically, the position polygon serves to partition the space of possible robot positions.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 211,\n",
       "  'text_chunk': 'Such a polygonal representation of themultiple-hypothesis belief can apply to a continuous, geometric map of the environment [35] or, alternatively, to a tessellated, discrete approximation to the continuous environ- ment. It may be useful, however, to incorporate some ordering on the possible robot positions, capturing the fact that some robot positions are likelier than others. A strategy for repre-senting a continuous multiple-hypothesis belief state along with a preference ordering overpossible positions is to model the belief as a mathematical distribution.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 211,\n",
       "  'text_chunk': 'For example, [50, 142] notate the robot’s position belief using an point in the 2D environment as the X Y,{}'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 212,\n",
       "  'text_chunk': 'Mobile Robot Localization 197 Figure 5.10 Three examples of single hypotheses of position using different map representations: (a) real map with walls, doors and furniture; (b) line-based map → around 100 lines with two parameters; (c) occupancy grid-based map → around 3000 grid cells size 50 x 50 cm; (d) topological map using line features (Z/S lines) and doors → around 50 features and 18 nodes. node ia) c)b) d) (x,y,θ)robot position'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 213,\n",
       "  'text_chunk': '198 Chapter 5 mean plus a standard deviation parameter , thereby defining a Gaussian distribution. The intended interpretation is that the distribution at each position represents the probabil- ity assigned to the robot being at that location. This representation is particularly amenable to mathematically defined tracking functions, such as the Kalman filter, that are designedto operate efficiently on Gaussian distributions. An alternative is to represent the set of possible robot positions, not using a single Gaus- sian probability density function, but using discrete markers for each possible position.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 213,\n",
       "  'text_chunk': 'In this case, each possible robot position is individually noted along with a confidence or probability parameter (see figure 5.11). In the case of a highly tessellated map this can result in thousands or even tens of thousands of possible robot positions in a single-beliefstate. The key advantage of the multiple-hypothesis representation is that the robot can explic- itly maintain uncertainty regarding its position. If the robot only acquires partial informa-tion regarding position from its sensors and effectors, that information can conceptually be incorporated in an updated belief.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 213,\n",
       "  'text_chunk': 'A more subtle advantage of this approach revolves around the robot’s ability to explic- itly measure its own degree of uncertainty regarding position. This advantage is the key toa class of localization and navigation solutions in which the robot not only reasons about reaching a particular goal but reasons about the future trajectory of its own belief state. Forinstance, a robot may choose paths that minimize its future position uncertainty.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 213,\n",
       "  'text_chunk': 'An exam- ple of this approach is [141], in which the robot plans a path from point to point that takes it near a series of landmarks in order to mitigate localization difficulties. This type ofµ σFigure 5.11 Example of multiple-hypothesis tracking (courtesy of W. Burgard [49]). The belief state that is largely distributed becomes very certain after moving to position 4. Note that darker coloring repre- sents higher probability. Belief states at positions 2, 3, and 4 Path of the robot A B'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 214,\n",
       "  'text_chunk': 'Mobile Robot Localization 199 explicit reasoning about the effect that trajectories will have on the quality of localization requires a multiple-hypothesis representation. One of the fundamental disadvantages of multiple-hypothesis approaches involves deci- sion-making. If the robot represents its position as a region or set of possible positions, thenhow shall it decide what to do next? Figure 5.11 provides an example. At position 3, the robot’s belief state is distributed among five hallways separately.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 214,\n",
       "  'text_chunk': 'If the goal of the robot is to travel down one particular hallway, then given this belief state, what action should therobot choose? The challenge occurs because some of the robot’s possible positions imply a motion tra- jectory that is inconsistent with some of its other possible positions. One approach that wewill see in the case studies below is to assume, for decision-making purposes, that the robot is physically at the most probable location in its belief state, then to choose a path based on that current position.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 214,\n",
       "  'text_chunk': 'But this approach demands that each possible position have an asso-ciated probability. In general, the right approach to such decision-making problems would be to decide on trajectories that eliminate the ambiguity explicitly. But this leads us to the second majordisadvantage of multiple-hypothesis approaches. In the most general case, they can be computationally very expensive. When one reasons in a 3D space of discrete possible posi- tions, the number of possible belief states in the single-hypothesis case is limited to thenumber of possible positions in the 3D world.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 214,\n",
       "  'text_chunk': 'Consider this number to be . When one moves to an arbitrary multiple-hypothesis representation, then the number of possible belief states is the power set of , which is far larger: . Thus explicit reasoning about the possible trajectory of the belief state over time quickly becomes computationally unten- able as the size of the environment grows. There are, however, specific forms of multiple-hypothesis representations that are some- what more constrained, thereby avoiding the computational explosion while allowing a limited type of multiple-hypothesis belief.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 214,\n",
       "  'text_chunk': 'For example, if one assumes a Gaussian distri- bution of probability centered at a single position, then the problem of representation andtracking of belief becomes equivalent to Kalman filtering, a straightforward mathematical process described below. Alternatively, a highly tessellated map representation combined with a limit of ten possible positions in the belief state, results in a discrete update cycle thatis, at worst, only ten times more computationally expensive than a single-hypothesis belief update.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 214,\n",
       "  'text_chunk': 'And other ways to cope with the complexity problem, still being precise and com- putationally cheap, are hybrid metric-topological approaches [145, 147] or multi-Gaussianposition estimation [35, 60, 81]. In conclusion, the most critical benefit of the multiple-hypothesis belief state is the abil- ity to maintain a sense of position while explicitly annotating the robot’s uncertainty aboutits own position. This powerful representation has enabled robots with limited sensoryN N 2N'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 215,\n",
       "  'text_chunk': '200 Chapter 5 information to navigate robustly in an array of environments, as we shall see in the case studies below. 5.5 Map RepresentationThe problem of representing the environment in which the robot moves is a dual of the problem of representing the robot’s possible position or positions. Decisions made regard-ing the environmental representation can have impact on the choices available for robot position representation. Often the fidelity of the position representation is bounded by the fidelity of the map.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 215,\n",
       "  'text_chunk': 'Three fundamental relationships must be understood when choosing a particular map representation: 1.The precision of the map must appropriately match the precision with which the robot needs to achieve its goals. 2.The precision of the map and the type of features represented must match the precisionand data types returned by the robot’s sensors. 3.The complexity of the map representation has direct impact on the computational com- plexity of reasoning about mapping, localization, and navigation. In the following sections, we identify and discuss critical design choices in creating a map representation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 215,\n",
       "  'text_chunk': 'Each such choice has great impact on the relationships listed above and on the resulting robot localization architecture. As we shall see, the choice of possible map representations is broad. Selecting an appropriate representation requires understanding all of the trade-offs inherent in that choice as well as understanding the specific context in which a particular mobile robot implementation must perform localization. In general, the environmental representation and model can be roughly classified as presented in chapter4, section 4.3.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 215,\n",
       "  'text_chunk': '5.5.1 Continuous representations A continuous-valued map is one method for exact decomposition of the environment. The position of environmental features can be annotated precisely in continuous space. Mobilerobot implementations to date use continuous maps only in 2D representations, as furtherdimensionality can result in computational explosion. A common approach is to combine the exactness of a continuous representation with the compactness of the closed-world assumption .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 215,\n",
       "  'text_chunk': 'This means that one assumes that the repre- sentation will specify all environmental objects in the map, and that any area in the map that is devoid of objects has no objects in the corresponding portion of the environment. Thus, the total storage needed in the map is proportional to the density of objects in theenvironment, and a sparse environment can be represented by a low-memory map.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 216,\n",
       "  'text_chunk': 'Mobile Robot Localization 201 One example of such a representation, shown in figure 5.12, is a 2D representation in which polygons represent all obstacles in a continuous-valued coordinate space. This is similar to the method used by Latombe [21, 98] and others to represent environments for mobile robot path-planning techniques. In the case of [21, 98], most of the experiments are in fact simulations run exclusively within the computer’s memory.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 216,\n",
       "  'text_chunk': 'Therefore, no real effort would have been expended to attempt to use sets of polygons to describe a real-world environment, such as a park oroffice building. In other work in which real environments must be captured by the maps, one sees a trend toward selectivity and abstraction. The human map maker tends to capture on the map, for localization purposes, only objects that can be detected by the robot’s sensors and, further- more, only a subset of the features of real-world objects.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 216,\n",
       "  'text_chunk': 'It should be immediately apparent that geometric maps can capably represent the phys- ical locations of objects without referring to their texture, color, elasticity, or any other suchsecondary features that do not relate directly to position and space. In addition to this level of simplification, a mobile robot map can further reduce memory usage by capturing onlyaspects of object geometry that are immediately relevant to localization. For example, all objects may be approximated using very simple convex polygons, sacrificing map felicity for the sake of computational speed.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 216,\n",
       "  'text_chunk': 'Figure 5.12 A continuous representation using polygons as environmental obstacles.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 217,\n",
       "  'text_chunk': '202 Chapter 5 One excellent example involves line extraction. Many indoor mobile robots rely upon laser rangefinding devices to recover distance readings to nearby objects. Such robots can automatically extract best-fit lines from the dense range data provided by thousands ofpoints of laser strikes. Given such a line extraction sensor, an appropriate continuous map- ping approach is to populate the map with a set of infinite lines. The continuous nature of the map guarantees that lines can be positioned at arbitrary positions in the plane and atarbitrary angles.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 217,\n",
       "  'text_chunk': 'The abstraction of real environmental objects such as walls and intersec- tions captures only the information in the map representation that matches the type of infor- mation recovered by the mobile robot’s rangefinding sensor. Figure 5.13 shows a map of an indoor environment at EPFL using a continuous line rep- resentation. Note that the only environmental features captured by the map are straightlines, such as those found at corners and along walls.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 217,\n",
       "  'text_chunk': 'This represents not only a sampling of the real world of richer features but also a simplification, for an actual wall may have texture and relief that is not captured by the mapped line. The impact of continuous map representations on position representation is primarily positive. In the case of single-hypothesis position representation, that position may be spec-ified as any continuous-valued point in the coordinate space, and therefore extremely highaccuracy is possible. In the case of multiple-hypothesis position representation, the contin- uous map enables two types of multiple position representation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 217,\n",
       "  'text_chunk': 'In one case, the possible robot position may be depicted as a geometric shape in the hyperplane, such that the robot is known to be within the bounds of that shape. This isshown in figure 5.29, in which the position of the robot is depicted by an oval bounding area.Figure 5.13 Example of a continuous-valued line representation of EPFL. (a) Real map. (b) Representation with a set of infinite lines.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 218,\n",
       "  'text_chunk': 'Mobile Robot Localization 203 Yet, the continuous representation does not disallow representation of position in the form of a discrete set of possible positions. For instance, in [62] the robot position belief state is captured by sampling nine continuous-valued positions from within a region near the robot’s best-known position. This algorithm captures, within a continuous space, a dis-crete sampling of possible robot positions.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 218,\n",
       "  'text_chunk': 'In summary, the key advantage of a continuous map representation is the potential for high accuracy and expressiveness with respect to the environmental configuration as wellas the robot position within that environment. The danger of a continuous representation is that the map may be computationally costly. But this danger can be tempered by employing abstraction and capturing only the most relevant environmental features. Together with theuse of the closed-world assumption , these techniques can enable a continuous-valued map to be no more costly, and sometimes even less costly, than a standard discrete representa-tion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 218,\n",
       "  'text_chunk': '5.5.2 Decomposition strategies In the section above, we discussed one method of simplification, in which the continuousmap representation contains a set of infinite lines that approximate real-world environmen- tal lines based on a 2D slice of the world. Basically this transformation from the real world to the map representation is a filter that removes all nonstraight data and furthermoreextends line segment data into infinite lines that require fewer parameters. A more dramatic form of simplification is abstraction : a general decomposition and selection of environmental features.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 218,\n",
       "  'text_chunk': 'In this section, we explore decomposition as appliedin its more extreme forms to the question of map representation. Why would one radically decompose the real environment during the design of a map representation? The immediate disadvantage of decomposition and abstraction is the lossof fidelity between the map and the real world. Both qualitatively, in terms of overall struc- ture, and quantitatively, in terms of geometric precision, a highly abstract map does not compare favorably to a high-fidelity map.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 218,\n",
       "  'text_chunk': 'Despite this disadvantage, decomposition and abstraction may be useful if the abstrac- tion can be planned carefully so as to capture the relevant, useful features of the world while discarding all other features. The advantage of this approach is that the map representationcan potentially be minimized. Furthermore, if the decomposition is hierarchical, such as in a pyramid of recursive abstraction, then reasoning and planning with respect to the map representation may be computationally far superior to planning in a fully detailed worldmodel.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 218,\n",
       "  'text_chunk': 'A standard, lossless form of opportunistic decomposition is termed exact cell decompo- sition . This method, introduced by Latombe [21], achieves decomposition by selecting boundaries between discrete cells based on geometric criticality.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 219,\n",
       "  'text_chunk': '204 Chapter 5 Figure 5.14 depicts an exact decomposition of a planar workspace populated by polyg- onal obstacles. The map representation tessellates the space into areas of free space. The representation can be extremely compact because each such area is actually stored as a single node, resulting in a total of only eighteen nodes in this example. The underlying assumption behind this decomposition is that the particular position of a robot within each area of free space does not matter.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 219,\n",
       "  'text_chunk': 'What matters is the robot’s ability to traverse from each area of free space to the adjacent areas. Therefore, as with other rep- resentations we will see, the resulting graph captures the adjacency of map locales. If indeed the assumptions are valid and the robot does not care about its precise position within a single area, then this can be an effective representation that nonetheless captures the connectivity of the environment. Such an exact decomposition is not always appropriate.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 219,\n",
       "  'text_chunk': 'Exact decomposition is a func- tion of the particular environment obstacles and free space. If this information is expensiveto collect or even unknown, then such an approach is not feasible. An alternative is fixed decomposition , in which the world is tessellated, transforming the continuous real environment into a discrete approximation for the map. Such a transforma-tion is demonstrated in figure 5.15, which depicts what happens to obstacle-filled and freeareas during this transformation. The key disadvantage of this approach stems from its inex- act nature.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 219,\n",
       "  'text_chunk': 'It is possible for narrow passageways to be lost during such a transformation, as shown in figure 5.15. Formally, this means that fixed decomposition is sound but not com-plete. Yet another approach is adaptive cell decomposition, as presented in figure 5.16.Figure 5.14 Example of exact cell decomposition.goalstart 17 2 345 68 9 10 11 12 1314 1517 1618'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 220,\n",
       "  'text_chunk': 'Mobile Robot Localization 205 The concept of fixed decomposition is extremely popular in mobile robotics; it is per- haps the single most common map representation technique currently utilized. One very popular version of fixed decomposition is known as the occupancy grid representation [112]. In an occupancy grid, the environment is represented by a discrete grid, where each cell is either filled (part of an obstacle) or empty (part of free space).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 220,\n",
       "  'text_chunk': 'This method is of par- ticular value when a robot is equipped with range-based sensors because the range valuesFigure 5.15 Fixed decomposition of the same space (narrow passage disappears).goalstartgoalstart goalstart'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 221,\n",
       "  'text_chunk': '206 Chapter 5 of each sensor, combined with the absolute position of the robot, can be used directly to update the filled or empty value of each cell. In the occupancy grid, each cell may have a counter, whereby the value 0 indicates that the cell has not been “hit” by any ranging measurements and, therefore, it is likely freespace.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 221,\n",
       "  'text_chunk': 'As the number of ranging strikes increases, the cell’s value is incremented and, above a certain threshold, the cell is deemed to be an obstacle. The values of cells are com- monly discounted when a ranging strike travels through the cell, striking a further cell. By also discounting the values of cells over time, both hysteresis and the possibility of transient obstacles can be represented using this occupancy grid approach.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 221,\n",
       "  'text_chunk': 'Figure 5.17 depicts an occupancy grid representation in which the darkness of each cell is proportional to the valueof its counter. One commercial robot that uses a standard occupancy grid for mapping and navigation is the Cye robot [163]. There remain two main disadvantages of the occupancy grid approach. First, the size of the map in robot memory grows with the size of the environment and if a small cell size is used, this size can quickly become untenable.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 221,\n",
       "  'text_chunk': 'This occupancy grid approach is not compat- ible with the closed-world assumption , which enabled continuous representations to have potentially very small memory requirements in large, sparse environments. In contrast, theFigure 5.16 Example of adaptive (approximate variable-cell) decomposition of an environment [21]. The rectan- gle, bounding the free space, is decomposed into four identical rectangles. If the interior of a rectangle lies completely in free space or in the configuration space obstacle, it is not decomposed further.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 221,\n",
       "  'text_chunk': 'Oth- erwise, it is recursively decomposed into four rectangles until some predefined resolution is attained. The white cells lie outside the obstacles, the black inside, and the gray are part of both regions.goalstart'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 222,\n",
       "  'text_chunk': 'Mobile Robot Localization 207 occupancy grid must have memory set aside for every cell in the matrix. Furthermore, any fixed decomposition method such as this imposes a geometric grid on the world a priori , regardless of the environmental details. This can be inappropriate in cases where geometry is not the most salient feature of the environment. For these reasons, an alternative, called topological decomposition, has been the subject of some exploration in mobile robotics.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 222,\n",
       "  'text_chunk': 'Topological approaches avoid direct measurementof geometric environmental qualities, instead concentrating on characteristics of the envi- ronment that are most relevant to the robot for localization. Formally, a topological representation is a graph that specifies two things: nodes and the connectivity between those nodes. Insofar as a topological representation is intended for the use of a mobile robot, nodes are used to denote areas in the world and arcs are used todenote adjacency of pairs of nodes.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 222,\n",
       "  'text_chunk': 'When an arc connects two nodes, then the robot can traverse from one node to the other without requiring traversal of any other intermediary node. Adjacency is clearly at the heart of the topological approach, just as adjacency in a cell decomposition representation maps to geometric adjacency in the real world. However, thetopological approach diverges in that the nodes are not of fixed size or even specificationsof free space. Instead, nodes document an area based on any sensor discriminant such that the robot can recognize entry and exit of the node.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 222,\n",
       "  'text_chunk': 'Figure 5.18 depicts a topological representation of a set of hallways and offices in an indoor environment. In this case, the robot is assumed to have an intersection detector, per- haps using sonar and vision to find intersections between halls and between halls andFigure 5.17 Example of an occupancy grid map representation (courtesy of S. Thrun [145]).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 223,\n",
       "  'text_chunk': '208 Chapter 5 rooms. Note that nodes capture geometric space, and arcs in this representation simply rep- resent connectivity. Another example of topological representation is the work of Simhon and Dudek [134], in which the goal is to create a mobile robot that can capture the most interesting aspects ofan area for human consumption. The nodes in their representation are visually striking locales rather than route intersections. In order to navigate using a topological map robustly, a robot must satisfy two con- straints.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 223,\n",
       "  'text_chunk': 'First, it must have a means for detecting its current position in terms of the nodesof the topological graph. Second, it must have a means for traveling between nodes using robot motion. The node sizes and particular dimensions must be optimized to match thesensory discrimination of the mobile robot hardware. This ability to “tune” the representa- tion to the robot’s particular sensors can be an important advantage of the topological approach.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 223,\n",
       "  'text_chunk': 'However, as the map representation drifts further away from true geometry, theexpressiveness of the representation for accurately and precisely describing a robot position is lost. Therein lies the compromise between the discrete cell-based map representations and the topological representations. Interestingly, the continuous map representation hasFigure 5.18 A topological representation of an indoor office area.1 2 3 4 567 8 910 111213 14 151618 17'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 224,\n",
       "  'text_chunk': 'Mobile Robot Localization 209 the potential to be both compact like a topological representation and precise as with all direct geometric representations. Yet, a chief motivation of the topological approach is that the environment may contain important nongeometric features – features that have no ranging relevance but are usefulfor localization. In chapter 4 we described such whole-image vision-based features. In contrast to these whole-image feature extractors, often spatially localized landmarks are artificially placed in an environment to impose a particular visual-topological connec- tivity upon the environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 224,\n",
       "  'text_chunk': 'In effect, the artificial landmark can impose artificial struc-ture. Examples of working systems operating with this landmark-based strategy have also demonstrated success. Latombe’s landmark-based navigation research [99] has been implemented on real-world indoor mobile robots that employ paper landmarks attached tothe ceiling as the locally observable features. Chips, the museum robot, is another robot that uses man-made landmarks to obviate the localization problem.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 224,\n",
       "  'text_chunk': 'In this case, a bright pink square serves as a landmark with dimensions and color signature that would be hard to acci-dentally reproduce in a museum environment [118]. One such museum landmark is shown in figure 5.19. In summary, range is clearly not the only measurable and useful environmental value for a mobile robot. This is particularly true with the advent of color vision, as well as laserFigure 5.19 An artificial landmark used by Chips during autonomous docking.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 225,\n",
       "  'text_chunk': '210 Chapter 5 rangefinding, which provides reflectance information in addition to range information. Choosing a map representation for a particular mobile robot requires, first, understanding the sensors available on the mobile robot, and, second, understanding the mobile robot’s functional requirements (e.g .,required goal precision and accuracy). 5.5.3 State of the art: current challenges in map representation The sections above describe major design decisions in regard to map representationchoices.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 225,\n",
       "  'text_chunk': 'There are, however, fundamental real-world features that mobile robot map repre- sentations do not yet represent well. These continue to be the subject of open research, and several such challenges are described below. The real world is dynamic. As mobile robots come to inhabit the same spaces as humans, they will encounter moving people, cars, strollers, and the transient obstacles placed andmoved by humans as they go about their activities.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 225,\n",
       "  'text_chunk': 'This is particularly true when one con-siders the home environment with which domestic robots will someday need to contend. The map representations described above do not, in general, have explicit facilities for identifying and distinguishing between permanent obstacles (e.g., walls, doorways, etc. )and transient obstacles (e.g., humans, shipping packages, etc.). The current state of the art in terms of mobile robot sensors is partly to blame for this shortcoming.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 225,\n",
       "  'text_chunk': 'Although vision research is rapidly advancing, robust sensors that discriminate between moving animalsand static structures from a moving reference frame are not yet available. Furthermore, esti- mating the motion vector of transient objects remains a research problem. Usually, the assumption behind the above map representations is that all objects on the map are effectively static. Partial success can be achieved by discounting mapped objects over time. For example, occupancy grid techniques can be more robust to dynamic settings by introducing temporal discounting, effectively treating transient obstacles as noise.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 225,\n",
       "  'text_chunk': 'Themore challenging process of map creation is particularly fragile to environmental dynam- ics; most mapping techniques generally require that the environment be free of moving objects during the mapping process. One exception to this limitation involves topologicalrepresentations. Because precise geometry is not important, transient objects have little effect on the mapping or localization process, subject to the critical constraint that the tran- sient objects must not change the topological connectivity of the environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 225,\n",
       "  'text_chunk': 'Still, neitherthe occupancy grid representation nor a topological approach is actively recognizing and representing transient objects as distinct from both sensor error and permanent map fea- tures. As vision sensing provides more robust and more informative content regarding the transience and motion details of objects in the world, mobile roboticists will in time pro-pose representations that make use of that information. A classic example involves occlu-sion by human crowds. Museum tour guide robots generally suffer from an extreme amount of occlusion.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 225,\n",
       "  'text_chunk': 'If the robot’s sensing suite is located along the robot’s body, then the robot is'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 226,\n",
       "  'text_chunk': 'Mobile Robot Localization 211 effectively blind when a group of human visitors completely surround the robot. This is because its map contains only environmental features that are, at that point, fully hidden from the robot’s sensors by the wall of people. In the best case, the robot should recognize its occlusion and make no effort to localize using these invalid sensor readings. In the worstcase, the robot will localize with the fully occluded data, and will update its location incor- rectly.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 226,\n",
       "  'text_chunk': 'A vision sensor that can discriminate the local conditions of the robot (e.g,. we are surrounded by people) can help eliminate this error mode. A second open challenge in mobile robot localization involves the traversal of open spaces. Existing localization techniques generally depend on local measures such as range,thereby demanding environments that are somewhat densely filled with objects that thesensors can detect and measure.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 226,\n",
       "  'text_chunk': 'Wide-open spaces such as parking lots, fields of grass, and indoor atriums such as those found in convention centers pose a difficulty for such systems because of their relative sparseness. Indeed, when populated with humans, the challenge isexacerbated because any mapped objects are almost certain to be occluded from view by the people. Once again, more recent technologies provide some hope of overcoming these limita- tions. Both vision and state-of-the-art laser rangefinding devices offer outdoor performancewith ranges of up to a hundred meters and more.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 226,\n",
       "  'text_chunk': 'Of course, GPS performs even better. Such long-range sensing may be required for robots to localize using distant features. This trend teases out a hidden assumption underlying most topological map representa- tions. Usually, topological representations make assumptions regarding spatial locality: a node contains objects and features that are themselves within that node. The process of map creation thus involves making nodes that are, in their own self-contained way, recognizable by virtue of the objects contained within the node.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 226,\n",
       "  'text_chunk': 'Therefore, in an indoor environment, each room can be a separate node, and this is reasonable because each room will have alayout and a set of belongings that are unique to that room. However, consider the outdoor world of a wide-open park. Where should a single node end and the next node begin? The answer is unclear because objects that are far away fromthe current node, or position, can yield information for the localization process.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 226,\n",
       "  'text_chunk': 'For exam- ple, the hump of a hill at the horizon, the position of a river in the valley, and the trajectory of the sun all are nonlocal features that have great bearing on one’s ability to infer currentposition. The spatial locality assumption is violated and, instead, replaced by a visibility criterion: the node or cell may need a mechanism for representing objects that are measur- able and visible from that cell.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 226,\n",
       "  'text_chunk': 'Once again, as sensors improve and, in this case, as outdoorlocomotion mechanisms improve, there will be greater urgency to solve problems associ- ated with localization in wide-open settings, with and without GPS-type global localization sensors. We end this section with one final open challenge that represents one of the fundamental academic research questions of robotics: sensor fusion. A variety of measurement types are'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 227,\n",
       "  'text_chunk': '212 Chapter 5 possible using off-the-shelf robot sensors, including heat, range, acoustic and light-based reflectivity, color, texture, friction, and so on. Sensor fusion is a research topic closely related to map representation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 227,\n",
       "  'text_chunk': 'Just as a map must embody an environment in sufficient detail for a robot to perform localization and reasoning, sensor fusion demands a represen-tation of the world that is sufficiently general and expressive that a variety of sensor types can have their data correlated appropriately, strengthening the resulting percepts well beyond that of any individual sensor’s readings. Perhaps the only general implementation of sensor fusion to date is that of neural net- work classifier.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 227,\n",
       "  'text_chunk': 'Using this technique, any number and any type of sensor values may bejointly combined in a network that will use whatever means necessary to optimize its clas-sification accuracy. For the mobile robot that must use a human-readable internal map rep- resentation, no equally general sensor fusion scheme has yet been born.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 227,\n",
       "  'text_chunk': 'It is reasonable to expect that, when the sensor fusion problem is solved, integration of a large number of dis-parate sensor types may easily result in sufficient discriminatory power for robots to achieve real-world navigation, even in wide-open and dynamic circumstances such as a public square filled with people.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 227,\n",
       "  'text_chunk': '5.6 Probabilistic Map-Based Localization 5.6.1 Introduction As stated earlier, multiple-hypothesis position representation is advantageous because the robot can explicitly track its own beliefs regarding its possible positions in the environment.Ideally, the robot’s belief state will change, over time, as is consistent with its motor outputs and perceptual inputs. One geometric approach to multiple-hypothesis representation, men-tioned earlier, involves identifying the possible positions of the robot by specifying a poly-gon in the environmental representation [98].'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 227,\n",
       "  'text_chunk': 'This method does not provide any indication of the relative chances between various possible robot positions. Probabilistic techniques differ from this because they explicitly identify probabilities with the possible robot positions, and for this reason these methods have been the focus ofrecent research. In the following sections we present two classes of probabilistic localiza- tion. The first class, Markov localization , uses an explicitly specified probability distribu- tion across all possible robot positions.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 227,\n",
       "  'text_chunk': 'The second method, Kalman filter localization , uses a Gaussian probability density representation of robot position and scan matching for local-ization. Unlike Markov localization, Kalman filter localization does not independently con- sider each possible pose in the robot’s configuration space. Interestingly, the Kalman filter localization process results from the Markov localization axioms if the robot’s position uncertainty is assumed to have a Gaussian form [3, pp. 43-44]. Before discussing each method in detail, we present the general robot localization prob- lem and solution strategy.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 227,\n",
       "  'text_chunk': 'Consider a mobile robot moving in a known environment. As it'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 228,\n",
       "  'text_chunk': 'Mobile Robot Localization 213 starts to move, say from a precisely known location, it can keep track of its motion using odometry. Due to odometry uncertainty, after some movement the robot will become very uncertain about its position (see section 5.2.4). To keep position uncertainty from growing unbounded, the robot must localize itself in relation to its environment map. To localize,the robot might use its on-board sensors (ultrasonic, range sensor, vision) to make observa- tions of its environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 228,\n",
       "  'text_chunk': 'The information provided by the robot’s odometry, plus the infor- mation provided by such exteroceptive observations, can be combined to enable the robotto localize as well as possible with respect to its map. The processes of updating based on proprioceptive sensor values and exteroceptive sensor values are often separated logically, leading to a general two-step process for robot position update.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 228,\n",
       "  'text_chunk': 'Action update represents the application of some action model to the mobile robot’s proprioceptive encoder measurements and prior belief state to yield a new beliefstate representing the robot’s belief about its current position. Note that throughout thischapter we assume that the robot’s proprioceptive encoder measurements are used as the best possible measure of its actions over time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 228,\n",
       "  'text_chunk': 'If, for instance, a differential-drive robot had motors without encoders connected to its wheels and employed open-loop control, theninstead of encoder measurements the robot’s highly uncertain estimates of wheel spin would need to be incorporated. We ignore such cases and therefore have a simple formula: .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 228,\n",
       "  'text_chunk': '(5.16) Perception update represents the application of some perception model to the mobile robot’s exteroceptive sensor inputs and updated belief state to yield a refined belief state representing the robot’s current position: (5.17) The perception model See and sometimes the action model are abstract functions of both the map and the robot’s physical configuration (e.g., sensors and their positions, kinematics, etc.).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 228,\n",
       "  'text_chunk': 'In general, the action update process contributes uncertainty to the robot’s belief about position: encoders have error and therefore motion is somewhat nondeterministic. By con- trast, perception update generally refines the belief state. Sensor measurements, when com- pared to the robot’s environmental model, tend to provide clues regarding the robot’spossible position. In the case of Markov localization, the robot’s belief state is usually represented as sep- arate probability assignments for every possible robot pose in its map.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 228,\n",
       "  'text_chunk': \"The action updateand perception update processes must update the probability of every cell in this case. Kalman filter localization represents the robot’s belief state using a single, well-definedAct o t st1– s'tAct otst1–,() = See it s't stSee its't,() = Act\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 229,\n",
       "  'text_chunk': '214 Chapter 5 Gaussian probability density function, and thus retains just a and parameterization of the robot’s belief about position with respect to the map. Updating the parameters of the Gaussian distribution is all that is required. This fundamental difference in the representa- tion of belief state leads to the following advantages and disadvantages of the two methods,as presented in [73]: •Markov localization allows for localization starting from any unknown position and can thus recover from ambiguous situations because the robot can track multiple, completelydisparate possible positions.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 229,\n",
       "  'text_chunk': 'However, to update the probability of all positions within the whole state space at any time requires a discrete representation of the space, such as a geometric grid or a topological graph (see section 5.5.2). The required memory andcomputational power can thus limit precision and map size. •Kalman filter localization tracks the robot from an initially known position and is inher-ently both precise and efficient. In particular, Kalman filter localization can be used incontinuous world representations.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 229,\n",
       "  'text_chunk': 'However, if the uncertainty of the robot becomes too large (e.g., due to a robot collision with an object) and thus not truly unimodal, the Kalman filter can fail to capture the multitude of possible robot positions and canbecome irrevocably lost.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 229,\n",
       "  'text_chunk': 'In recent research projects improvements are achieved or proposed by either only updat- ing the state space of interest within the Markov approach [49] or by tracking multiple hypotheses with Kalman filters [35], or by combining both methods to create a hybrid localization system [73, 147]. In the next two sections we will each approach in detail.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 229,\n",
       "  'text_chunk': '5.6.2 Markov localization Markov localization tracks the robot’s belief state using an arbitrary probability density function to represent the robot’s position (see also [50, 88, 116, 119]). In practice, allknown Markov localization systems implement this generic belief representation by first tessellating the robot configuration space into a finite, discrete number of possible robot poses in the map. In actual applications, the number of possible poses can range from sev-eral hundred to millions of positions.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 229,\n",
       "  'text_chunk': 'Given such a generic conception of robot position, a powerful update mechanism is required that can compute the belief state that results when new information (e.g., encodervalues and sensor values) is incorporated into a prior belief state with arbitrary probability density.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 229,\n",
       "  'text_chunk': 'The solution is born out of probability theory, and so the next section describes the foundations of probability theory that apply to this problem, notably the Bayes formula.Then, two subsequent sections provide case studies, one robot implementing a simple fea- ture-driven topological representation of the environment [88, 116, 119] and the other using a geometric grid-based map [49, 50].µ σ'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 230,\n",
       "  'text_chunk': 'Mobile Robot Localization 215 5.6.2.1 Introduction: applying probability theory to robot localization Given a discrete representation of robot positions, in order to express a belief state we wish to assign to each possible robot position a probability that the robot is indeed at that posi- tion. From probability theory we use the term to denote the probability that is true.This is also called the prior probability of because it measures the probability that is true independent of any additional knowledge we may have.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 230,\n",
       "  'text_chunk': 'For example we can use to denote the prior probability that the robot r is at position at time . In practice, we wish to compute the probability of each individual robot position given the encoder and sensor evidence the robot has collected. In probability theory, we use theterm to denote the conditional probability of given that we know . For exam- ple, we use to denote the probability that the robot is at position given that the robot’s sensor inputs .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 230,\n",
       "  'text_chunk': 'The question is, how can a term such as be simplified to its constituent parts so that it can be computed? The answer lies in the product rule, which states (5.18) Equation (5.18) is intuitively straightforward, as the probability of both and being true is being related to being true and the other being conditionally true.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 230,\n",
       "  'text_chunk': 'But you should be able to convince yourself that the alternate equation is equally correct: (5.19) Using equations (5.18) and (5.19) together, we can derive the Bayes formula for com- puting : (5.20) We use the Bayes rule to compute the robot’s new belief state as a function of its sensory inputs and its former belief state. But to do this properly, we must recall the basic goal of the Markov localization approach: a discrete set of possible robot positions are repre- sented.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 230,\n",
       "  'text_chunk': 'The belief state of the robot must assign a probability for each location in . The function described in equation (5.17) expresses a mapping from a belief state and sensor input to a refined belief state. To do this, we must update the probability asso-ciated with each position in , and we can do this by directly applying the Bayes formulato every such .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 230,\n",
       "  'text_chunk': 'In denoting this, we will stop representing the temporal index for sim- plicity and will further use to mean :pA() A AA prtl=() lt pAB() A B prtlit =() l i prtlit =() pAB∧() pAB() pB() = A B B pAB∧() pBA() pA() = pAB() pAB()pBA() pA() pB()----------------------------- - = L prtl=() l L See lL lt pl() prl=()'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 231,\n",
       "  'text_chunk': '216 Chapter 5 (5.21) The value of is key to equation (5.21), and this probability of a sensor input at each robot position must be computed using some model. An obvious strategy would be to consult the robot’s map, identifying the probability of particular sensor readings with each possible map position, given knowledge about the robot’s sensor geometry and the mapped environment. The value of is easy to recover in this case.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 231,\n",
       "  'text_chunk': 'It is simply the probability associated with the belief state before the perceptual update process. Finally, note that the denominator does not depend upon ; that is, as we apply equation (5.21) toall positions in , the denominator never varies. Because it is effectively constant, in practice this denominator is usually dropped and, at the end of the perception update step,all probabilities in the belief state are re-normalized to sum at 1.0. Now consider the Act function of equation (5.16).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 231,\n",
       "  'text_chunk': 'Act maps a former belief state and encoder measurement (i.e., robot action) to a new belief state. In order to compute the prob-ability of position in the new belief state, one must integrate over all the possible ways in which the robot may have reached according to the potential positions expressed in the former belief state. This is subtle but fundamentally important. The same location can bereached from multiple source locations with the same encoder measurement o because the encoder measurement is uncertain.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 231,\n",
       "  'text_chunk': 'Temporal indices are required in this update equation: (5.22) Thus, the total probability for a specific position is built up from the individual con- tributions from every location in the former belief state given encoder measurement . Equations (5.21) and (5.22) form the basis of Markov localization, and they incorporate theMarkov assumption . Formally, this means that their output is a function only of the robot’s previous state and its most recent actions (odometry) and perception.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 231,\n",
       "  'text_chunk': 'In a general, non-Markovian situation, the state of a system depends upon all of its history. After all, the values of a robot’s sensors at time t do not really depend only on its position at time . They depend to some degree on the trajectory of the robot over time; indeed, on the entire historyof the robot. For example, the robot could have experienced a serious collision recently that has biased the sensor’s behavior.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 231,\n",
       "  'text_chunk': 'By the same token, the position of the robot at time doesnot really depend only on its position at time and its odometric measurements. Due to its history of motion, one wheel may have worn more than the other, causing a left-turn- ing bias over time that affects its current position. So the Markov assumption is, of course, not a valid assumption.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 231,\n",
       "  'text_chunk': \"However the Markov assumption greatly simplifies tracking, reasoning, and planning and so it is an approxima- tion that continues to be extremely popular in mobile robotics.pli()pil()pl() pi()----------------------- - = pil() pl() prl=() pi() l lL l l l pltot() pltl't1–ot, () pl't1–() l't1–d∫= l l' o t t t1–\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 232,\n",
       "  'text_chunk': 'Mobile Robot Localization 217 5.6.2.2 Case study 1: Markov localization using a topological map A straightforward application of Markov localization is possible when the robot’s environ-ment representation already provides an appropriate decomposition. This is the case when the environmental representation is purely topological. Consider a contest in which each robot is to receive a topological description of the envi- ronment. The description would include only the connectivity of hallways and rooms, with no mention of geometric distance.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 232,\n",
       "  'text_chunk': 'In addition, this supplied map would be imperfect, con- taining several false arcs (e.g., a closed door). Such was the case for the 1994 American Association for Artificial Intelligence (AAAI) National Robot Contest, at which each robot’s mission was to use the supplied map and its own sensors to navigate from a chosen starting position to a target room. Dervish, the winner of this contest, employed probabilistic Markov localization and used a multiple-hypothesis belief state over a topological environmental representation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 232,\n",
       "  'text_chunk': 'Wenow describe Dervish as an example of a robot with a discrete, topological representationand a probabilistic localization algorithm. Dervish, shown in figure 5.20, includes a sonar arrangement custom-designed for the 1994 AAAI National Robot Contest. The environment in this contest consisted of a recti-Figure 5.20 Dervish exploring its environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 233,\n",
       "  'text_chunk': '218 Chapter 5 linear indoor office space filled with real office furniture as obstacles. Traditional sonars were arranged radially around the robot in a ring. Robots with such sensor configurations are subject to both tripping over short objects below the ring and to decapitation by tall objects (such as ledges, shelves, and tables) that are above the ring. Dervish’s answer to this challenge was to arrange one pair of sonars diagonally upward to detect ledges and other overhangs.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 233,\n",
       "  'text_chunk': 'In addition, the diagonal sonar pair also proved toably detect tables, enabling the robot to avoid wandering underneath tall tables. Theremaining sonars were clustered in sets of sonars, such that each individual transducer in the set would be at a slightly varied angle to minimize specularity. Finally, two sonars near the robot’s base were positioned to detect low obstacles, such as paper cups, on the floor.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 233,\n",
       "  'text_chunk': 'We have already noted that the representation provided by the contest organizers was purely topological, noting the connectivity of hallways and rooms in the office environ-ment. Thus, it would be appropriate to design Dervish’s perceptual system to detect match-ing perceptual events: the detection and passage of connections between hallways and offices. This abstract perceptual system was implemented by viewing the trajectory of sonar strikes to the left and right sides of Dervish over time.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 233,\n",
       "  'text_chunk': 'Interestingly, this perceptual systemwould use time alone and no concept of encoder value to trigger perceptual events. Thus, for instance, when the robot detects a 7 to 17 cm indentation in the width of the hallway formore than 1 second continuously, a closed door sensory event is triggered. If the sonar strikes jump well beyond 17 cm for more than 1 second, an open door sensory event trig- gers.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 233,\n",
       "  'text_chunk': 'To reduce coherent reflection sensor noise (see section 4.1.6) associated with Dervish’s sonars, the robot would track its angle relative to the hallway centerline and completelysuppress sensor events when its angle to the hallway exceeded 9 degrees. Interestingly, thiswould result in a conservative perceptual system that frequently misses features, particu- larly when the hallway is crowded with obstacles that Dervish must negotiate.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 233,\n",
       "  'text_chunk': 'Once again, the conservative nature of the perceptual system, and in particular its tendency to issue falsenegatives, would point to a probabilistic solution to the localization problem so that a com- plete trajectory of perceptual inputs could be considered. Dervish’s environmental representation was a discrete topological map, identical in abstraction and information to the map provided by the contest organizers. Figure 5.21depicts a geometric representation of a typical office environment overlaid with the topo- logical map for the same office environment.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 233,\n",
       "  'text_chunk': 'Recall that for a topological representationthe key decision involves assignment of nodes and connectivity between nodes (see section 5.5.2). As shown on the left in figure 5.21 Dervish uses a topology in which node bound- aries are marked primarily by doorways (and hallways and foyers). The topological graphshown on the right depicts the information captured in the example shown.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 234,\n",
       "  'text_chunk': 'Mobile Robot Localization 219 Note that in this particular topological model arcs are zero-length while nodes have spa- tial expansiveness and together cover the entire space. This particular topological represen- tation is particularly apt for Dervish given its task of navigating through hallways into a specific room and its perceptual capability of recognizing discontinuities in hallway walls. In order to represent a specific belief state, Dervish associated with each topological node n a probability that the robot is at a physical position within the boundaries of : .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 234,\n",
       "  'text_chunk': 'As will become clear below, the probabilistic update used by Dervish was approximate, therefore technically one should refer to the resulting values as likelihoods rather than probabilities. The perception update process for Dervish functions precisely as in equation (5.21). Per- ceptual events are generated asynchronously, each time the feature extractor is able to rec- ognize a large scale feature (e.g., doorway, intersection) based on recent ultrasonic values.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 234,\n",
       "  'text_chunk': 'Each perceptual event consists of a percept-pair (a feature on one side of the robot or two features on both sides). Given a specific percept-pair , equation (5.21) enables the likelihood of each possible position to be updated using the formula: Table 5.1 Dervish’s certainty matrix.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 234,\n",
       "  'text_chunk': 'Wall Closed doorOpen doorOpen hallwayFoyer Nothing detected 0.70 0.40 0.05 0.001 0.30 Closed door detected 0.30 0.60 0 0 0.05 Open door detected 0 0 0.90 0.10 0.15 Open hallway detected 0 0 0.001 0.90 0.50Figure 5.21 A geometric office environment (left) and its topological analog (right).R1 H1H1 H1-2 H2 H2-3 H3R1R2 H1-2 H2 H2-3R2 H3 n prtn=() i n'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 235,\n",
       "  'text_chunk': '220 Chapter 5 (5.23) The value of is already available from the current belief state of Dervish, and so the challenge lies in computing . The key simplification for Dervish is based upon the realization that, because the feature extraction system only extracts four total featuresand because a node contains (on a single side) one of five total features, every possible com- bination of node type and extracted feature can be represented in a 4 x 5 table.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 235,\n",
       "  'text_chunk': 'Dervish’s certainty matrix (show in table 5.1) is just this lookup table. Dervish makes the simplifying assumption that the performance of the feature detector (i.e., the probability that it is correct) is only a function of the feature extracted and the actual feature in the node. With this assumption in hand, we can populate the certainty matrix with confidence esti- mates for each possible pairing of perception and node type.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 235,\n",
       "  'text_chunk': 'For each of the five world fea- tures that the robot can encounter (wall, closed door, open door, open hallway-and foyer) this matrix assigns a likelihood for each of the three one-sided percepts that the sensorysystem can issue. In addition, this matrix assigns a likelihood that the sensory system will fail to issue a perceptual event altogether ( nothing detected ).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 235,\n",
       "  'text_chunk': 'For example, using the specific values in table 5.1, if Dervish is next to an open hallway, the likelihood of mistakenly recognizing it as an open door is 0.10. This means that for anynode n that is of type open hallway and for the sensor value = open door, . Together with a specific topological map, the certainty matrix enables straightforward computation of during the perception update process.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 235,\n",
       "  'text_chunk': 'For Dervish’s particular sensory suite and for any specific environment it intends to nav- igate, humans generate a specific certainty matrix that loosely represents its perceptual con-fidence, along with a global measure for the probability that any given door will be closed versus opened in the real world. Recall that Dervish has no encoders and that perceptual events are triggered asynchro- nously by the feature extraction processes. Therefore, Dervish has no action update step as depicted by equation (5.22).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 235,\n",
       "  'text_chunk': 'When the robot does detect a perceptual event, multiple per- ception update steps will need to be performed to update the likelihood of every possiblerobot position given Dervish’s former belief state. This is because there is a chance that the robot has traveled multiple topological nodes since its previous perceptual event (i.e., false- negative errors). Formally, the perception update formula for Dervish is in reality a combi- nation of the general form of action update and perception update.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 235,\n",
       "  'text_chunk': \"The likelihood of posi- tion given perceptual event i is calculated as in equation (5.22): (5.24) The value of denotes the likelihood of Dervish being at position as repre- sented by Dervish’s former belief state. The temporal subscript is used in lieu of pni() pin() pn() = pn() pin() i pin() 0.10 = pin() n pntit() pntn'ti–it, () pn'ti–() n'ti–d∫= pn'ti–() n' ti– t1–\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 236,\n",
       "  'text_chunk': 'Mobile Robot Localization 221 because for each possible position the discrete topological distance from to can vary depending on the specific topological map. The calculation of is per- formed by multiplying the probability of generating perceptual event at position by the probability of having failed to generate perceptual events at all nodes between and : (5.25) For example (figure 5.22), suppose that the robot has only two nonzero nodes in its belief state, {1-2, 2-3}, with likelihoods associated with each possible position: and .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 236,\n",
       "  'text_chunk': 'For simplicity assume the robot is facing east with certainty. Note that the likelihoods for nodes 1-2 and 2-3 do not sum to 1.0. These valuesare not formal probabilities, and so computational effort is minimized in Dervish by avoid- ing normalization altogether. Now suppose that a perceptual event is generated: the robot detects an open hallway on its left and an open door on its right simultaneously. State 2-3 will progress potentially to states 3, 3-4, and 4.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 236,\n",
       "  'text_chunk': 'But states 3 and 3-4 can be eliminated because the likelihood of detecting an open door when there is only wall is zero.The likelihood of reaching state 4 is the product of the initial likelihood for state 2-3, 0.2,the likelihood of not detecting anything at node 3, (a), and the likelihood of detecting a hall- way on the left and a door on the right at node 4, (b). Note that we assume the likelihood of detecting nothing at node 3-4 is 1.0 (a simplifying approximation).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 236,\n",
       "  'text_chunk': '(a) occurs only if Dervish fails to detect the door on its left at node 3 (either closed or open), , and correctly detects nothing on its right, 0.7. (b) occurs if Dervish correctly identifies the open hallway on its left at node 4, 0.90, and mistakes the right hallway for an open door, 0.10. The final formula, , yields a likelihood of 0.003 for state 4.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 236,\n",
       "  'text_chunk': \"This is a partial result for following from the prior belief state node2-3.n' n' n pntn'ti–it, () in n' n pntn'ti–it, () pitnt,() p∅nt1–,() p∅nt2–,() … p∅nti–1+ ,() ⋅⋅⋅ ⋅ =Figure 5.22 A realistic indoor topological environment.1 1-2 2 2-3 3 3-4 4N p12–() 1.0= p23–() 0.2= 0.6 0.4 1 0.\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 236,\n",
       "  'text_chunk': '6–() 0.05⋅ + ⋅ [] 0.2 0.6 0.4 0.4 0.05 ⋅ + ⋅ [] 0.7 0.9 0.1 ⋅ []⋅ ⋅⋅ p4()'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 237,\n",
       "  'text_chunk': '222 Chapter 5 Turning to the other node in Dervish’s prior belief state, 1-2 will potentially progress to states 2, 2-3, 3, 3-4, and 4. Again, states 2-3, 3, and 3-4 can all be eliminated since the like- lihood of detecting an open door when a wall is present is zero.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 237,\n",
       "  'text_chunk': 'The likelihood of state 2 is the product of the prior likelihood for state 1-2, (1.0), the likelihood of detecting the dooron the right as an open door, , and the likelihood of correctly detecting an open hallway to the left, 0.9. The likelihood for being at state 2 is then . In addition, 1-2 progresses to state 4 with a certainty factor of , which is added to the certainty factor above to bring the total for state 4 to 0.00328.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 237,\n",
       "  'text_chunk': 'Dervish would therefore track the new belief state to be {2, 4}, assigning a very high likelihood to position 2 and a low likelihood to position 4. Empirically, Dervish’s map representation and localization system have proved to be sufficient for navigation of four indoor office environments: the artificial office environ-ment created explicitly for the 1994 National Conference on Artificial Intelligence; and thepsychology, history, and computer science departments at Stanford University.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 237,\n",
       "  'text_chunk': 'All of these experiments were run while providing Dervish with no notion of the distance between adja- cent nodes in its topological map. It is a demonstration of the power of probabilistic local-ization that, in spite of the tremendous lack of action and encoder information, the robot is able to navigate several real-world office buildings successfully. One open question remains with respect to Dervish’s localization system. Dervish was not just a localizer but also a navigator.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 237,\n",
       "  'text_chunk': 'As with all multiple hypothesis systems, one must ask the question, how does the robot decide how to move, given that it has multiple possible robot positions in its representation? The technique employed by Dervish is a common technique in the mobile robotics field: plan the robot’s actions by assuming that the robot’s actual position is its most likely node in the belief state. Generally, the most likely position is a good measure of the robot’s actual world position.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 237,\n",
       "  'text_chunk': 'However, this technique has short-comings when the highest and second highest most likely positions have similar values. In the case of Dervish, it nonetheless goes with the highest-likelihood position at all times, save at one critical juncture. The robot’s goal is to enter a target room and remain there.Therefore, from the point of view of its goal, it is critical that Dervish finish navigating only when the robot has strong confidence in being at the correct final location.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 237,\n",
       "  'text_chunk': 'In this particular case, Dervish’s execution module refuses to enter a room if the gap between the most likelyposition and the second likeliest position is below a preset threshold. In such a case, Der- vish will actively plan a path that causes it to move further down the hallway in an attempt to collect more sensor data and thereby increase the relative likelihood of one position inthe multiple-hypothesis belief state.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 237,\n",
       "  'text_chunk': 'Although computationally unattractive, one can go further, imagining a planning system for robots such as Dervish for which one specifies a goal belief state rather than a goal posi- tion. The robot can then reason and plan in order to achieve a goal confidence level, thus explicitly taking into account not only robot position but also the measured likelihood of0.6 0 0.4 0. 9⋅ +⋅ [] 1.0 0.4 0.9 0. 9 ⋅⋅⋅ 0.3= 4.3 106–⋅'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 238,\n",
       "  'text_chunk': 'Mobile Robot Localization 223 each position. An example of just such a procedure is the sensory uncertainty field of Latombe [141], in which the robot must find a trajectory that reaches its goal while maxi- mizing its localization confidence on-line. 5.6.2.3 Case study 2: Markov localization using a grid map The major weakness of a purely topological decomposition of the environment is the reso- lution limitation imposed by such a granular representation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 238,\n",
       "  'text_chunk': 'The position of the robot is usually limited to the resolution of a single node in such cases, and this may be undesirablefor certain applications. In this case study, we examine the work of Burgard and colleagues [49, 50] in which far more precise navigation is made possible using a grid-based representation while still employing the Markov localization technique. The robot used by this research, Rhino, is an RWI B24 robot with twenty-four sonars and two Sick laser rangefinders.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 238,\n",
       "  'text_chunk': 'Clearly, at the sensory level this robot accumulates greater and more accurate range data than is possible with the handful of sonar sensors mounted on Dervish. In order to make maximal use of these fine-grained sensory data, Rhino uses a 2D geometric environmental representation of free and occupied space. This metric map is tes-sellated regularly into a fixed decomposition grid with each cell occupying 4 to 64 cm in various instantiations. Like Dervish, Rhino uses multiple-hypothesis belief representation.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 238,\n",
       "  'text_chunk': 'In line with the far improved resolution of the environmental representation, the belief state representation of Rhino consists of a 3D array representing the probability of possible robot positions (see figure 5.23). The resolution of the array is . Notethat unlike Dervish, which assumes its orientation is approximate and known, Rhino explicitly represents fine-grained alternative orientations, and so its belief state formally represents three degrees of freedom.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 238,\n",
       "  'text_chunk': 'As we have stated before, the resolution of the belief state representation must match the environmental representation in order for the overall system to function well. Whereas Dervish made use of only perceptual events, ignoring encoder inputs and there- fore metric distance altogether, Rhino uses the complete Markov probabilistic localizationapproach summarized in section 5.6.2.1, including both an explicit action update phase and a perception update phase at every cycle. The discrete Markov chain version of action update is performed because of the tessel- lated representation of position.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 238,\n",
       "  'text_chunk': \"Given encoder measurements o at time , each updated position probability in the belief state is expressed as a sum over previous possible positionsand the motion model: (5.26)15 15 1 5× × 153 15 cm 15 cm 1 °× × t Pltot() Pltl't1–ot, () pl't1–()⋅ l'∑=\"},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 239,\n",
       "  'text_chunk': '224 Chapter 5 Note that equation (5.26) is simply a discrete version of equation (5.22). The specific motion model used by Rhino represents the result of motion as a Gaussian that is bounded (i.e., the tails of the distribution are finite). Rhino’s kinematic configuration is a three- wheel synchro-drive rather than a differential-drive robot. Nevertheless, the error ellipsesdepicted in figures 5.4 and 5.5 are similar to the Gaussian bounds that result from Rhino’s motion model.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 239,\n",
       "  'text_chunk': 'The perception model follows the Bayes formula precisely, as in equation (5.21). Given a range perception the probability of the robot being at each location is updated as fol- lows: (5.27) Note that a denominator is used by Rhino, although the denominator is constant for vary- ing values of . This denominator acts as a normalizer to ensure that the probability mea- sures in the belief state continue to sum to 1. The critical challenge is, of course, the calculation of .'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 239,\n",
       "  'text_chunk': 'In the case of Dervish, the number of possible values for and were so small that a simple table could suffice. How-ever, with the fine-grained metric representation of Rhino, the number of possible sensor readings and environmental geometric contexts is extremely large. Thus, Rhino computesFigure 5.23 The belief state representation 3D array used by Rhino (courtesy of W. Burgard and S. Thrun).'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 239,\n",
       "  'text_chunk': 'il pli()pil()pl() pi()----------------------- - = l pil() il'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 240,\n",
       "  'text_chunk': 'Mobile Robot Localization 225 directly using a model of the robot’s sensor behavior, its position , and the local environmental metric map around . The sensor model must calculate the probability of a specific perceptual measurement given that its likelihood is justified by known errors of the sonar or laser rangefinder sen- sors. Three key assumptions are used to construct this sensor model: 1.If an object in the metric map is detected by a range sensor, the measurement error can be described with a distribution that has a mean at the correct reading.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 240,\n",
       "  'text_chunk': '2.There should always be a nonzero chance that a range sensor will read any measurement value, even if this measurement disagrees sharply with the environmental geometry. 3.In contrast to the generic error described in (2), there is a specific failure mode in ranging sensors whereby the signal is absorbed or coherently reflected, causing the sensor’srange measurement to be maximal. Therefore, there is a local peak in the probability density distribution at the maximal reading of a range sensor.'},\n",
       " {'book_name': 'Introduction to Autonomous Mobile Robots book',\n",
       "  'page_number': 240,\n",
       "  'text_chunk': 'By validating these assumptions using empirical sonar trials in multiple environments, the research group has delivered to Rhino a conservative and powerful sensor model for its particular sensors. Figure 5.24 provides a simple 1D example of the grid-based Markov localization algo- rithm. The robot begins with a flat probability density function for its possible location. Inother words, it initially has no bias regarding position. As the robot encounters first onedoor and then a second door, the probability density function over possible positions becomes first multimodal and finally unimodal and sharply defined.'},\n",
       " ...]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_textbook_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_chunk_to_pickle(combined_textbook_chunk,'combined_textbook_chunk_metadata.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
