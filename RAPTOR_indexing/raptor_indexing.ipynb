{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the textbook embeddings and instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chunks_to_pickle(file_path):\n",
    "    with open(file_path,'rb') as f:\n",
    "        embedding_with_metadata = pickle.load(f)\n",
    "    return embedding_with_metadata\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_textbook = load_chunks_to_pickle('/Users/haridevaraj/Documents/Projects/steps_ai/Content_extraction_and_chunking_embed/combined_textbook_chunk_metadata.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings using SBERT for vector representations with helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "\n",
    "\n",
    "#helper function to embed text chunks with metadata\n",
    "def embed_text_chunks(textbook_chunk_metadata,e_model):\n",
    "    for chunk in textbook_chunk_metadata:\n",
    "        text_chunk = chunk['text_chunk']\n",
    "        embedding = e_model.encode(text_chunk)\n",
    "        chunk['embedding'] = embedding\n",
    "    return textbook_chunk_metadata\n",
    "\n",
    "\n",
    "def get_embeddings(textbook_chunk,embed_model=embedding_model):\n",
    "    text_embed = embedding_model.encode(textbook_chunk)\n",
    "    return np.array(text_embed)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textbook1_embed = load_embeddings_to_pickle('/Users/haridevaraj/Documents/Projects/steps_ai/Content_extraction_and_chunking_embed/textbook1_embedding_metadata.pkl')\n",
    "# textbook2_embed =load_embeddings_to_pickle('/Users/haridevaraj/Documents/Projects/steps_ai/Content_extraction_and_chunking_embed/textbook2_embedding_metadata.pkl')\n",
    "# textbook3_embed =load_embeddings_to_pickle('/Users/haridevaraj/Documents/Projects/steps_ai/Content_extraction_and_chunking_embed/textbook3_embedding_metadata.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.chains import StuffDocumentsChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "import umap\n",
    "from config import SUM_API_KEY \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering  helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_cluster_embeddings_prob_threshold(textbook_embeddings,prob_threshold):\n",
    "    num_clusters = get_bayesian_optimal_num_clusters(textbook_embeddings)\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=num_clusters,random_state=42)\n",
    "    gmm.fit(textbook_embeddings)\n",
    "    probas = gmm.predict_proba(textbook_embeddings)\n",
    "    cluster_labels = [np.where(prob > prob_threshold)[0] for prob in probas]\n",
    "    return cluster_labels, num_clusters\n",
    "\n",
    "#get the optimal number using the Bayesian Information criteria with a GMM\n",
    "def get_bayesian_optimal_num_clusters(textbook_embeddings,max_clusters=50):\n",
    "    max_clusters = min(max_clusters,len(textbook_embeddings))\n",
    "    n_clusters = np.arange(1,max_clusters)\n",
    "    \n",
    "    bayesian_info_criteria=[]\n",
    "    \n",
    "    for num in n_clusters:\n",
    "        gmm = GaussianMixture(n_components=num,random_state=42)\n",
    "        gmm.fit(textbook_embeddings)\n",
    "        bayesian_info_criteria.append(gmm.bic(textbook_embeddings))\n",
    "    optim_clusters = n_clusters[np.argmin(bayesian_info_criteria)]\n",
    "    \n",
    "    return optim_clusters\n",
    "    \n",
    "#reducing the cluster using UMAP dimensionality reduction in global and local dimensionality\n",
    "def umap_reduce_global_cluster_embed(textbook_embeddings,dim,num_neighbors=None,metric=\"cosine\"):\n",
    "    if num_neighbors is None:\n",
    "        num_neighbors = int((len(textbook_embeddings)-1)**0.5)\n",
    "    global_embed= umap.UMAP(n_neighbors=num_neighbors,n_components=dim,metric=metric).fit_transform(textbook_embeddings)\n",
    "    return global_embed\n",
    "\n",
    "    \n",
    "\n",
    "def umap_local_cluster_embed(textbook_embeddings,dim,num_neighbhors=10,metric=\"cosine\"):\n",
    "    local_embed = umap.UMAP(n_neighbors=num_neighbhors,n_components=dim,metric=metric).fit_transform(textbook_embeddings)\n",
    "    return local_embed\n",
    "\n",
    " \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = np.array([chunk['embedding']]for chunk in combined_textbook_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<generator object <genexpr> at 0x12a0c5b10>, dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster the embeddings in two steps:\n",
    "#  1) reduce their dimensionality globally and the cluster using GMM\n",
    "# 2) Perfom local clustering for each global cluster\n",
    "def clustering(textbook_embeddings,dim,prob_cluster_threshold):\n",
    "    \n",
    "    #avoid clustering for less data---checking if the number of embeddings is too small to perform meaningful clustering. \n",
    "    if len(textbook_embeddings) <=dim +1:\n",
    "        # all data points belong to a single cluster\n",
    "        return [np.array([0]) for _ in range(len(textbook_embeddings))]\n",
    "    \n",
    "    #global dimensionality reduction\n",
    "    global_reduct_embed = umap_reduce_global_cluster_embed(textbook_embeddings,dim)\n",
    "    \n",
    "    #global GMM clustering\n",
    "    global_clusters, num_global_clusters = gmm_cluster_embeddings_prob_threshold(global_reduct_embed,prob_cluster_threshold)\n",
    "    \n",
    "    total_local_clusters = [np.array([]) for _ in range(len(textbook_embeddings))]\n",
    "    #keep track of cumulative count of local clusters\n",
    "    total_clusters = 0\n",
    "    \n",
    "    #perform local clustering for each global cluster\n",
    "    for num_g in range(num_global_clusters):\n",
    "        #extract current global cluster embeddings\n",
    "        gc_embeddings = textbook_embeddings[np.array([num_g in gc for gc in global_clusters])]\n",
    "        \n",
    "        if len(gc_embeddings) == 0:\n",
    "            continue\n",
    "        \n",
    "        # case for small clusters\n",
    "        if len(gc_embeddings) <=dim +1:\n",
    "            local_clusters = [np.array([0])for _ in gc_embeddings]\n",
    "            num_local_clusters = 1\n",
    "        else:\n",
    "            # reduce the local dimensionality and cluster\n",
    "            local_reduct_embed = umap_local_cluster_embed(gc_embeddings,dim)\n",
    "            local_clusters , num_local_clusters = gmm_cluster_embeddings_prob_threshold(local_reduct_embed,prob_cluster_threshold)\n",
    "            \n",
    "        #specify ids for local clusters\n",
    "        for num_l in range(num_local_clusters):\n",
    "            lc_embeddings = gc_embeddings[np.array([num_l in lc for lc in local_clusters])]\n",
    "            \n",
    "            local_idx = np.where(textbook_embeddings==lc_embeddings[:,None].all(-1))[1]\n",
    "            for idx in local_idx:\n",
    "                total_local_clusters[idx] = np.append(total_local_clusters[idx],num_l + total_clusters)\n",
    "        \n",
    "        \n",
    "        total_clusters += num_local_clusters\n",
    "        \n",
    "    return total_local_clusters\n",
    "            \n",
    "        \n",
    "def get_cluster_embed_texts(df_textbook):\n",
    "    \n",
    "    text_embed = get_embeddings(df_textbook['text_chunk'].tolist())\n",
    "    \n",
    "    # cluster the embeddings\n",
    "    cluster_labels = clustering(text_embed,10,0.1)\n",
    "    \n",
    "    df_textbook['embedding'] = list(text_embed)\n",
    "    df_textbook['cluster']   = cluster_labels\n",
    "    \n",
    "    return df_textbook\n",
    "    \n",
    "         \n",
    "        \n",
    "        \n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(df_textbooks,depth,summarize_llm):\n",
    "    \n",
    "    df_cluster_text_metadata = get_cluster_embed_texts(df_textbooks)\n",
    "    \n",
    "    \n",
    "    #use a new dataframe, representing the document-cluster pair in a straighforward way\n",
    "    new_cluster_list = []\n",
    "    \n",
    "    for idx, row in df_cluster_text_metadata.iterrows():\n",
    "        for cluster in row['cluster']:\n",
    "            new_cluster_list.append({\"text\":row['text_chunk'],\"embedding\":row[\"embedding\"],\"cluster\":cluster,\"book_name\":row[\"book_name\"],\"page_number\":row[\"page_number\"]})\n",
    "    \n",
    "    new_cluster_df = pd.DataFrame(new_cluster_list)\n",
    "    \n",
    "    #process all the unique clusters idx \n",
    "    all_unique_clusters = new_cluster_df[\"cluster\"].unique()\n",
    "    print(\"Unique clusters: \",all_unique_clusters)\n",
    "    \n",
    "    \n",
    "    #summarization prompt and chain\n",
    "    template= \"\"\"Here is a text from a collection of three textbooks about robotics. Please write a concise and informative summary of the following text. Include the main points, key details, and any important terminology or concepts. Ensure that the summary is clear and captures the essence of the provided text:\n",
    "                {text} \n",
    "                Summary:\n",
    "                \"\"\"\n",
    "    output_parser = StrOutputParser()\n",
    "    prompt_temp = ChatPromptTemplate.from_template(template)\n",
    "    summarization_chain = prompt_temp | summarize_llm | output_parser\n",
    "    \n",
    "    #get text summaries\n",
    "    \n",
    "    textbook_summaries = []\n",
    "    for c in all_unique_clusters:\n",
    "        \n",
    "        df_unq_cluster = new_cluster_df[new_cluster_df[\"cluster\"]==c]\n",
    "        \n",
    "        #to maintain clear chunck boundaries between different chunks\n",
    "        formatted_input_text = \"--- --- \\n --- --- \".join(df_unq_cluster['text'].to_list())\n",
    "        \n",
    "        #invove the summarization chain\n",
    "        textbook_summary = summarization_chain.invoke({\"text\": formatted_input_text})\n",
    "        textbook_summaries.append(textbook_summary)\n",
    "    #create a new dataframe for summaries\n",
    "    df_textbook_summary = pd.DataFrame({\"textbook-summaries\":textbook_summaries,\"depth\":[depth]*len(textbook_summaries),\"cluster\":list(all_unique_clusters)})\n",
    "    \n",
    "    return df_cluster_text_metadata, df_textbook_summary\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursion function for forming hierarchical tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_summarize_recursion(df_textbook,summarize_llm,depth,num_recursion_level):\n",
    "    \n",
    "    # store results for each depth of recursion as a dict\n",
    "    rec_depth_results= {}\n",
    "    \n",
    "    # for the current depth do the embedding, clustering and summarization\n",
    "    df_cluster_metadata, df_textbook_summaries = summarization(df_textbook,depth,summarize_llm)\n",
    "    \n",
    "    #store the current depth results in the dict\n",
    "    rec_depth_results[depth] = (df_cluster_metadata,df_textbook_summaries) \n",
    "    \n",
    "    #check if there is a need to do recursion again\n",
    "    num_uniq_clusters = df_textbook_summaries[\"cluster\"].nunique()\n",
    "    \n",
    "    if depth < num_recursion_level and num_uniq_clusters > 1:\n",
    "        #for the next depth of recursion, use the summaries as the input text\n",
    "        new_textbook_texts = df_textbook_summaries[\"textbook-summaries\"].tolist()\n",
    "        new_rec_depth_results = embed_cluster_summarize_recursion(new_textbook_texts,depth +1 ,num_recursion_level)\n",
    "        \n",
    "        \n",
    "        #store the results in the dict\n",
    "        rec_depth_results.update(new_rec_depth_results)\n",
    "    return rec_depth_results\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'n_neighbhours' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# build the tree and define the llm model for summarization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m sum_llm \u001b[38;5;241m=\u001b[39m ChatGoogleGenerativeAI(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-pro\u001b[39m\u001b[38;5;124m\"\u001b[39m,google_api_key\u001b[38;5;241m=\u001b[39mSUM_API_KEY)\n\u001b[0;32m----> 4\u001b[0m rec_results \u001b[38;5;241m=\u001b[39m \u001b[43membed_cluster_summarize_recursion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_textbook\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf_combined_textbook\u001b[49m\u001b[43m,\u001b[49m\u001b[43msummarize_llm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msum_llm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_recursion_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[76], line 7\u001b[0m, in \u001b[0;36membed_cluster_summarize_recursion\u001b[0;34m(df_textbook, summarize_llm, depth, num_recursion_level)\u001b[0m\n\u001b[1;32m      4\u001b[0m rec_depth_results\u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# for the current depth do the embedding, clustering and summarization\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df_cluster_metadata, df_textbook_summaries \u001b[38;5;241m=\u001b[39m \u001b[43msummarization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_textbook\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m,\u001b[49m\u001b[43msummarize_llm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#store the current depth results in the dict\u001b[39;00m\n\u001b[1;32m     10\u001b[0m rec_depth_results[depth] \u001b[38;5;241m=\u001b[39m (df_cluster_metadata,df_textbook_summaries) \n",
      "Cell \u001b[0;32mIn[75], line 3\u001b[0m, in \u001b[0;36msummarization\u001b[0;34m(df_textbooks, depth, summarize_llm)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msummarization\u001b[39m(df_textbooks,depth,summarize_llm):\n\u001b[0;32m----> 3\u001b[0m     df_cluster_text_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_cluster_embed_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_textbooks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m#use a new dataframe, representing the document-cluster pair in a straighforward way\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     new_cluster_list \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[74], line 57\u001b[0m, in \u001b[0;36mget_cluster_embed_texts\u001b[0;34m(df_textbook)\u001b[0m\n\u001b[1;32m     54\u001b[0m text_embed \u001b[38;5;241m=\u001b[39m get_embeddings(df_textbook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_chunk\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# cluster the embeddings\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m cluster_labels \u001b[38;5;241m=\u001b[39m \u001b[43mclustering\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m df_textbook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(text_embed)\n\u001b[1;32m     60\u001b[0m df_textbook[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m]   \u001b[38;5;241m=\u001b[39m cluster_labels\n",
      "Cell \u001b[0;32mIn[74], line 12\u001b[0m, in \u001b[0;36mclustering\u001b[0;34m(textbook_embeddings, dim, prob_cluster_threshold)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(textbook_embeddings))]\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#global dimensionality reduction\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m global_reduct_embed \u001b[38;5;241m=\u001b[39m \u001b[43mumap_reduce_global_cluster_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtextbook_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#global GMM clustering\u001b[39;00m\n\u001b[1;32m     15\u001b[0m global_clusters, num_global_clusters \u001b[38;5;241m=\u001b[39m gmm_cluster_embeddings_prob_threshold(global_reduct_embed,prob_cluster_threshold)\n",
      "Cell \u001b[0;32mIn[73], line 26\u001b[0m, in \u001b[0;36mumap_reduce_global_cluster_embed\u001b[0;34m(textbook_embeddings, dim, n_neighbours, metric)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mumap_reduce_global_cluster_embed\u001b[39m(textbook_embeddings,dim,n_neighbours\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcosine\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mn_neighbhours\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m         n_neighbhours \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((\u001b[38;5;28mlen\u001b[39m(textbook_embeddings)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m     28\u001b[0m     global_embed\u001b[38;5;241m=\u001b[39m umap\u001b[38;5;241m.\u001b[39mUMAP(n_neighbours\u001b[38;5;241m=\u001b[39mn_neighbhours,n_components\u001b[38;5;241m=\u001b[39mdim,metric\u001b[38;5;241m=\u001b[39mmetric)\u001b[38;5;241m.\u001b[39mfit_transform(texbook_embeddings)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'n_neighbhours' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "#convert the list into a dataframe for easier processing\n",
    "df_combined_textbook = pd.DataFrame(combined_textbook)\n",
    "\n",
    "# build the tree and define the llm model for summarization\n",
    "sum_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\",google_api_key=SUM_API_KEY)\n",
    "\n",
    "rec_results = embed_cluster_summarize_recursion(df_textbook=df_combined_textbook,summarize_llm=sum_llm,depth=1,num_recursion_level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapsed Tree retrieval(best performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus.vectorstores import Milvus\n",
    "\n",
    "#store the metadata in the metdata field for milvus maybe a df_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
