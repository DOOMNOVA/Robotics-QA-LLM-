{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the textbook embeddings and instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_to_pickle(file_path):\n",
    "    with open(file_path,'rb') as f:\n",
    "        embedding_with_metadata = pickle.load(f)\n",
    "    return embedding_with_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "\n",
    "\n",
    "\n",
    "#helper function to embed text chunks with metadata\n",
    "def embed_text_chunks(textbook_chunk_metadata,model):\n",
    "    for chunk in textbook_chunk_metadata:\n",
    "        text_chunk = chunk['text_chunk']\n",
    "        embedding = model.encode(text_chunk)\n",
    "        chunk['embedding'] = embedding\n",
    "    return textbook_chunk_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_textbook_embed = load_embeddings_to_pickle('/Users/haridevaraj/Documents/Projects/steps_ai/Content_extraction_and_chunking_embed/combined_textbook_embedding_metadata.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# textbook1_embed = load_embeddings_to_pickle('/Users/haridevaraj/Documents/Projects/steps_ai/Content_extraction_and_chunking_embed/textbook1_embedding_metadata.pkl')\n",
    "# textbook2_embed =load_embeddings_to_pickle('/Users/haridevaraj/Documents/Projects/steps_ai/Content_extraction_and_chunking_embed/textbook2_embedding_metadata.pkl')\n",
    "# textbook3_embed =load_embeddings_to_pickle('/Users/haridevaraj/Documents/Projects/steps_ai/Content_extraction_and_chunking_embed/textbook3_embedding_metadata.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPTOR indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import Optional\n",
    "import umap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering  helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmm_cluster_embeddings_prob_threshold(textbook_embeddings,random_state=42):\n",
    "    num_clusters = get_bayesian_optimal_num_clusters(textbook_embeddings)\n",
    "    \n",
    "    gmm = GaussianMixture(n_components=num_clusters,random_state=random_state)\n",
    "    gmm.fit(textbook_embeddings)\n",
    "    clusters = gmm.predict_proba(textbook_embeddings)\n",
    "    return clusters, gmm\n",
    "\n",
    "#get the optimal number using the Bayesian Information criteria with a GMM\n",
    "def get_bayesian_optimal_num_clusters(textbook_embeddings,max_clusters=30,random_state=42):\n",
    "    max_clusters = min(max_clusters,len(textbook_embeddings))\n",
    "    n_clusters = np.arange(1,max_clusters)\n",
    "    \n",
    "    bayesian_info_criteria=[]\n",
    "    \n",
    "    for num in n_clusters:\n",
    "        gmm = GaussianMixture(n_components=num,random_state=random_state)\n",
    "        gmm.fit(textbook_embeddings)\n",
    "        bayesian_info_criteria.append(gmm.bic())\n",
    "    optim_clusters = n_clusters[np.argmin(bayesian_info_criteria)]\n",
    "    \n",
    "    return optim_clusters\n",
    "    \n",
    "#reducing the cluster using UMAP dimensionality reduction in global and local dimensionality\n",
    "def umap_reduce_global_cluster_embed(textbook_embeddings,dim,n_neighbours=None,metric='cosine'):\n",
    "    if n_neighbhours is None:\n",
    "        n_neighbhours = int((len(textbook_embeddings)-1)**0.5)\n",
    "    global_embed= umap.UMAP(n_neighbours=n_neighbhours,n_components=dim,metric=metric).fit_transform(texbook_embeddings)\n",
    "    return global_embed\n",
    "\n",
    "    \n",
    "\n",
    "def umap_local_cluster_embed(textbook_embeddings,dim,num_neighbhours=10,metric='cosine'):\n",
    "    local_embed = umap.UMAP(n_neighbours=num_neighbhours,n_components=dim,metric=metric).fit_transform(textbook_embeddings)\n",
    "    return local_embed\n",
    "\n",
    " \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.array([chunk['embedding']]for chunk in combined_textbook_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<generator object <genexpr> at 0x12a0c5b10>, dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster the embeddings in two steps:\n",
    "#  1) reduce their dimensionality globally and the cluster using GMM\n",
    "# 2) Perfom local clustering for each global cluster\n",
    "def clustering(textbook_embeddings,dim,prob_cluster_threshold):\n",
    "    \n",
    "    #avoid clustering for less data---checking if the number of embeddings is too small to perform meaningful clustering. \n",
    "    if len(textbook_embeddings) <=dim +1:\n",
    "        # all data points belong to a single cluster\n",
    "        return [np.array([0]) for _ in range(len(textbook_embeddings))]\n",
    "    \n",
    "    #global dimensionality reduction\n",
    "    global_reduct_embed = umap_reduce_global_cluster_embed(textbook_embeddings,dim)\n",
    "    \n",
    "    #global GMM clustering\n",
    "    global_clusters, num_global_clusters = gmm_cluster_embeddings_prob_threshold(global_reduct_embed,prob_cluster_threshold)\n",
    "    \n",
    "    total_local_clusters = [np.array([]) for _ in range(len(textbook_embeddings))]\n",
    "    #keep track of cumulative count of local clusters\n",
    "    total_clusters = 0\n",
    "    \n",
    "    #perform local clustering for each global cluster\n",
    "    for num_g in range(num_global_clusters):\n",
    "        #extract current global cluster embeddings\n",
    "        gc_embeddings = textbook_embeddings[np.array([num_g in gc for gc in global_clusters])]\n",
    "        \n",
    "        if len(gc_embeddings) == 0:\n",
    "            continue\n",
    "        \n",
    "        # case for small clusters\n",
    "        if len(gc_embeddings) <=dim +1:\n",
    "            local_clusters = [np.array([0])for _ in gc_embeddings]\n",
    "            num_local_clusters = 1\n",
    "        else:\n",
    "            # reduce the local dimensionality and cluster\n",
    "            local_reduct_embed = umap_local_cluster_embed(gc_embeddings,dim)\n",
    "            local_clusters , num_local_clusters = gmm_cluster_embeddings_prob_threshold(local_reduct_embed,prob_cluster_threshold)\n",
    "            \n",
    "        #specify ids for local clusters\n",
    "        for num_l in range(num_local_clusters):\n",
    "            lc_embeddings = gc_embeddings[np.array([num_l in lc for lc in local_clusters])]\n",
    "            \n",
    "            local_idx = np.where(textbook_embeddings==lc_embeddings[:,None].all(-1))[1]\n",
    "            for idx in local_idx:\n",
    "                total_local_clusters[idx] = np.append(total_local_clusters[idx],num_l + total_clusters)\n",
    "        \n",
    "        \n",
    "        total_clusters += num_local_clusters\n",
    "        \n",
    "    return total_local_clusters\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarization(textbook_text,depth):\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursion function for forming hierarchical tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_cluster_summarize_recursion(textbook_text,current_recursion_level,num_recurion_level):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collapsed Tree retrieval(best performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_milvus.vectorstores import Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "steps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
